<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>启蛰海</title>
  
  <subtitle>挽弦暮笙</subtitle>
  <link href="http://wwffyy.life/atom.xml" rel="self"/>
  
  <link href="http://wwffyy.life/"/>
  <updated>2024-01-07T06:59:00.230Z</updated>
  <id>http://wwffyy.life/</id>
  
  <author>
    <name>whisper</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>sam入门</title>
    <link href="http://wwffyy.life/posts/11203c58.html"/>
    <id>http://wwffyy.life/posts/11203c58.html</id>
    <published>2024-01-07T03:38:31.000Z</published>
    <updated>2024-01-07T06:59:00.230Z</updated>
    
    <content type="html"><![CDATA[<h2 id="工作介绍"><a href="#工作介绍" class="headerlink" title="工作介绍"></a>工作介绍</h2><p>基于分割大模型sam，通过微调等方法迁移到分割相关领域辅助模型训练。</p><h2 id="相关文章"><a href="#相关文章" class="headerlink" title="相关文章"></a>相关文章</h2><h3 id="Segment-Anything"><a href="#Segment-Anything" class="headerlink" title="Segment Anything"></a>Segment Anything</h3><p>原始SAM文章，给出了模型架构，训练方法。<br>价值：学习到SAM基本输入输出形式，方便调用。<br>代码：<a href="https://github.com/facebookresearch/segment-anything">facebookresearch/segment-anything: The repository provides code for running inference with the SegmentAnything Model (SAM), links for downloading the trained model checkpoints, and example notebooks that show how to use the model. (github.com)</a></p><h3 id="SAM-Assisted-Remote-Sensing-Imagery-Semantic-Segmentation-with-Object-and-Boundary-Constraints"><a href="#SAM-Assisted-Remote-Sensing-Imagery-Semantic-Segmentation-with-Object-and-Boundary-Constraints" class="headerlink" title="SAM-Assisted Remote Sensing Imagery Semantic Segmentation with Object and Boundary Constraints"></a>SAM-Assisted Remote Sensing Imagery Semantic Segmentation with Object and Boundary Constraints</h3><p>一个基于sam辅助训练的语义分割模型。SAM 仅限于生成没有类别信息的分割结果，提出了一种辅助训练方法。通过调用sam大模型的接口，辅助遥感变换检测类任务训练。<br>数据集：ISPRS Vaihingen 和 LoveDA Urban遥感语义分割<br>代码：<a href="https://github.com/sstary/SSRS">sstary/SSRS (github.com)</a></p><h4 id="价值："><a href="#价值：" class="headerlink" title="价值："></a>价值：</h4><p>通过sam模型获取到图片的对象（SGO）和边界（SGB），提出了提出了一种新颖的对象一致性损失，并在这项工作中进一步引入了边界保留损失。</p><h4 id="方法："><a href="#方法：" class="headerlink" title="方法："></a>方法：</h4><p><img src="https://pic.imgdb.cn/item/659a23f9871b83018a3eed30.png" alt=""><br>主干网络使用Unetformer等主流分割方法，外加从sam导出的SGO与SGD进行损失计算。<br>冻结SAM的使用方法：<br><img src="https://pic.imgdb.cn/item/659a23fa871b83018a3eee1e.png" alt=""></p><h4 id="SAM使用方法："><a href="#SAM使用方法：" class="headerlink" title="SAM使用方法："></a>SAM使用方法：</h4><p>SAM 提供了一组应用程序编程接口 (API)，只需几行代码即可获得分段掩码。 API 中的不同提示支持不同的分割模式选项，例如全自动、边界框、点模式。</p><h4 id="损失计算方法："><a href="#损失计算方法：" class="headerlink" title="损失计算方法："></a>损失计算方法：</h4><ul><li>字母含义介绍：<br>$X$：输入图像<br>$P$：模型预测分割输出<br>$C$：需要分割的总类别数<br>$Y$：真实分割标签图<br>$Y<em>{o}$:sam生成对象图（SGO）<br>$Y</em>{b}$:sam生成边界图（SGB）<br>$M^{i}$:从SGO中对每一个对象提取得到的掩码</li></ul><ol><li>$L_{seg}$分割损失计算方法：<script type="math/tex; mode=display">L_{seg}=-\sum_{H,W}^{}\sum_{c\in C}^{}Y^{(H,W,c)}log (P^{(H,W,c)})</script></li><li>$L_{obj}$对象一致性损失计算方法：<br><img src="https://pic.imgdb.cn/item/659a23fa871b83018a3eee9c.png" alt=""><br>对象特征：<script type="math/tex; mode=display">F_{o}^{i}=P\odot M^{i}</script>对象平均特征：<script type="math/tex; mode=display">F_{avg}^{i}=\frac{G(F_i^o)}{N^i+1}\odot  M^i</script>其中G计算空间维度中所有像素的总和并重塑为其原始形状。<br>$N^i$是第$i$个对象中的点数<br>对象一致性损失计算：<br>MSE是均方误差<script type="math/tex; mode=display">L_{obj}=\sum_{K}^{i=1}MSE(F_o^i,F^i_{avg})</script></li><li>$L_{obj}$边界保留损失计算：<br>合并边缘约束可以有效增强遥感任务中语义分割模型的性能<br>边界全部设置为0<br>计算方法为：<script type="math/tex; mode=display">L_{bdy}=1-BF_1</script><script type="math/tex; mode=display">BF_1=2\times \frac{p_br_b}{p_b+r_b}</script>$p_b$和$r_b$分别为边界的精读和召回率</li><li>总损失<script type="math/tex; mode=display">L_{total}=L_{seg}+\lambda _oL_{obj}+\lambda _bL_{bdy}</script><h3 id="SamLP-A-Customized-Segment-Anything-Model-for-License-Plate-Detection"><a href="#SamLP-A-Customized-Segment-Anything-Model-for-License-Plate-Detection" class="headerlink" title="SamLP A Customized Segment Anything Model for License Plate Detection"></a>SamLP A Customized Segment Anything Model for License Plate Detection</h3>用sam定制的车牌检测模型<br>设计了一种低秩适应（LoRA）微调策略，将额外的参数注入到 SAM 中，并将 SAM 转移到 LP 检测任务中。<br>代码：<a href="https://github.com/Dinghaoxuan/SamLP">Dinghaoxuan/SamLP (github.com)</a><h4 id="贡献："><a href="#贡献：" class="headerlink" title="贡献："></a>贡献：</h4></li><li>将参数高效微调（PEFT）引入到SAM的传输中。具体来说，我们在我们提出的 SamLP 中提出了低秩适应 (LoRA)  调整策略，以使 SAM 适应 LP 检测数据集。LoRA：使SAM 最独特的能力（即位置提示）被抑制</li><li>可提示的（promptable）微调训练策略，及时车牌检测</li><li>少样本迁移能力<br>参数高效微调技术（PEFT）：</li><li>适配器调优Adapter tuning：在模型的每个层中插入小的、可训练的神经网络（称为 “adapters”），而不是调整模型的全部参数。</li><li>提示调优prompt tuning：在输入给模型的提示（prompt）中添加或优化一些 “tokens”，这些 “tokens” 被训练来改善模型的任务性能。</li><li>LoRA调优： 对 Transformer 中每一层的可训练低秩分解矩阵进行微调，即添加较小的可训练矩阵，显着减少可训练参数的数量。<br><img src="https://pic.imgdb.cn/item/659a23fa871b83018a3eef6f.png" alt=""><br>MaskDecoder的输出有：预测输出的掩码$\widehat{S}$，Iou分数Score，feature logits，各三个:<script type="math/tex; mode=display">\widehat{S}=\left \{  \widehat{S_1},\widehat{S_2},\widehat{S_3}\right \}</script><script type="math/tex; mode=display">\widehat{S} ,Score,lodits=Dec_M(F_I,Concat(T_M,T_P))</script><h4 id="pipeline："><a href="#pipeline：" class="headerlink" title="pipeline："></a>pipeline：</h4>具体而言，微调分为两个步骤：LoRA微调和promptable微调。首先，LoRA微调策略在SAM模型的图像编码器和掩码解码器的Transformer块中插入了一些LoRA层。这些LoRA层用于调整图像编码器以提取与车牌相关的特征，并使掩码解码器生成车牌分割掩码。在LoRA微调步骤中，由于输入仅为单个图像I，因此车牌的位置先验是未知的，因此没有提示编码器的信息，LoRA微调的输入提示为None。这意味着提示编码器没有适应车牌检测任务。为了保持SAM模型的可提示分割能力，设计了第二步，即可提示微调。在可提示微调中，来自LoRA微调的车牌分割结果被视为先验提示，用于引导可提示分割的训练。整个图像编码器及其LoRA层被冻结，只有提示编码器和掩码解码器中的LoRA层进行训练，以避免在图像嵌入中发生灾难性的遗忘，并加速可提示微调的训练过程。<h4 id="LoRA调优方法："><a href="#LoRA调优方法：" class="headerlink" title="LoRA调优方法："></a>LoRA调优方法：</h4>预训练语言模型的学习能力在将模型投影到低秩子空间时仍然有效。这意味着预训练模型具有较低的内在维度，在模型适应过程中参数的更新也具有较低的内在秩。<br><img src="https://pic.imgdb.cn/item/659a23fa871b83018a3ef009.png" alt=""><br>基础模型的预训练参数为$W_0 ∈ R^{d×k}$，其中d为输入特征维度，k为输出特征维度。$W_0$的优化方法：<script type="math/tex; mode=display">\widehat{W}_0=W_0+\Delta W=W_0+BA</script></li><li>A矩阵（$A ∈ R^{r×k}$）：这是一个低秩分解矩阵，其中r是低秩的大小，k是输出特征的维度。</li><li>B矩阵（$B ∈ R^{d×r}$）：这也是一个低秩分解矩阵，其中r是低秩的大小，d是输入特征的维度。<br>输入特征 $x ∈ R^{n×d}$ （n 是扁平化输入特征 x 的序列长度）同时与 $W_0$ 和 $ΔW$ 相乘，然后将它们的输出按元素求和作为输出特征 $h ∈ R^{n×k}$：<script type="math/tex; mode=display">h=W_0x+\Delta Wx=W_0x+BAx</script><img src="https://pic.imgdb.cn/item/659a2443871b83018a3ff542.png" alt=""><br>在 LoRA 微调步骤中，输入图像 I 被馈送到具有 LoRA 层的图像编码器中以获得图像嵌入。然后，输入提示P为None，这意味着没有点、框或掩模输入到提示编码器。之后，None 的提示嵌入与掩码标记相结合，然后掩码解码器从图像编码器检索图像嵌入中的高响应区域以生成二进制分割掩码。<script type="math/tex; mode=display">F_I=Enc_I(I)</script><script type="math/tex; mode=display">T_P=Enc_P(None)</script><script type="math/tex; mode=display">\widehat{S} ,Score,lodits=Dec_M(F_I,Concat(T_M,T_P))</script>最后计算Dice损失：<script type="math/tex; mode=display">L=\frac{1}{3}  {\textstyle \sum_{i=1}^{3}}Dice(\widehat{S}_i,S)</script><h4 id="可提示调优Promptable-Fine-tuning"><a href="#可提示调优Promptable-Fine-tuning" class="headerlink" title="可提示调优Promptable Fine-tuning"></a>可提示调优Promptable Fine-tuning</h4>设计了一个迭代细化训练管道，将提示引入分割中。<br><img src="https://s11.ax1x.com/2024/01/07/pizOAtU.png" alt=""></li></ol><h3 id="Adapting-Segment-Anything-Model-for-Change-Detection-in-VHR-Remote-Sensing-Images"><a href="#Adapting-Segment-Anything-Model-for-Change-Detection-in-VHR-Remote-Sensing-Images" class="headerlink" title="Adapting Segment Anything Model for Change Detection in VHR Remote Sensing Images"></a>Adapting Segment Anything Model for Change Detection in VHR Remote Sensing Images</h3><p>采用sam进行 VHR（超高分辨） 遥感图像变化检测<br>为了使 FastSAM 专注于 RS 场景中的某些特定地面物体，提出了一个卷积适配器来聚合面向任务的变化信息。为了利用 SAM 特征固有的语义表示，引入了一个与任务无关的语义学习分支来对双时态 RSI 中的潜在语义进行建模。在本文中，我们的目标是利用 SAM 强大且通用的语义开发能力来提高 CD 的准确性并减少对大量训练样本的依赖。<br>代码：<a href="https://github.com/ggsDing/SAM-CD">ggsDing/SAM-CD: Pytorch code of the SAM-CD (github.com)</a></p><h4 id="贡献：-1"><a href="#贡献：-1" class="headerlink" title="贡献："></a>贡献：</h4><ol><li>SAM-CD 将 FastSAM（SAM 的高效变体）适应 RS 场景，并利用多时态 LCLU 信息来嵌入变化表示。</li><li>将与任务无关的潜在图像学习引入到 CD 框架中。利用 SAM 的通用语义表示功能，生成的模型能够对 RSI 中的底层 LCLU 分布进行建模，以提高变化检测的准确性。这是通过测量多时间特征之间语义表示的相似性来监督的。</li></ol><h4 id="SAM局限性："><a href="#SAM局限性：" class="headerlink" title="SAM局限性："></a>SAM局限性：</h4><p>在某些领域表现出明显的局限性，包括医学图像、制造场景和 RSI 。由于它们主要接受自然图像的训练，因此它们倾向于更多地关注前景物体，并且难以分割小且不规则的物体。在这项工作中，我们利用自适应来微调 SAM（FastSAM），以学习 VHR RSI 中的潜在语义。</p><h4 id="典型的变化检测方法："><a href="#典型的变化检测方法：" class="headerlink" title="典型的变化检测方法："></a>典型的变化检测方法：</h4><p>利用两个连体编码器来提取时间特征，然后使用共享解码器将它们嵌入到变化表示中。<br>字母解释：<br>$(x_1,x_2)$：时间图像对<br>$U()$：编码器网络<br>$V()$：解码器网络<br>$m$：二进制CD图</p><script type="math/tex; mode=display">m=V[U(x_1,x_2)]</script><h4 id="本文方法"><a href="#本文方法" class="headerlink" title="本文方法"></a>本文方法</h4><p>我们期望通过比较底层语义特征可以更好地学习语义变化。通过视觉基础模型，现在可以在没有分类注释的情况下提取地面物体的语义。将视觉基础模型（VFM）作为视觉编码器，表示为$\widehat{U}()$,来提取通用语义特征（而不是时间差异）。得到预测CD图表示为：</p><script type="math/tex; mode=display">m=V[\widehat{U}(x_1),\widehat{U}(x_2)]</script><p><img src="https://s11.ax1x.com/2024/01/07/pizOmc9.png" alt=""><br>首先，我们利用 FastSAM 作为冻结编码器来利用视觉实体。为了更好地推广 RSI，引入了可训练的适配器来适应提取的特征。获得的多尺度 FastSAM 特征在类似 unet 的卷积解码器中进行融合和上采样。然后，除了嵌入变化表示的变化分支之外，我们还引入了一个额外的与任务无关的语义学习分支来对底层潜在语义进行建模。由此产生的 SAM-CD 是语义感知的，因此它可以更好地捕获 VHR RSI 中的对象变化。</p><h4 id="FastSAM-Adaptor"><a href="#FastSAM-Adaptor" class="headerlink" title="FastSAM Adaptor"></a>FastSAM Adaptor</h4><p>首先，我们将FastSAM在1/32、1/16、1/8和1/4的空间尺度上提取的特征进行聚合，表示为f1、f2、f3、f4。每个特征 fi 由相应的适配器 α 处理。<br>字母解释：<br>α ：adaptor适配器<br>$conv$：1x1卷积层<br>$bn$：批量归一化函数（batch normalization）<br>$γ()$：RELU激活函数</p><script type="math/tex; mode=display">f_i^*=\alpha(f_i)=\gamma\left \{ bn[conv(f_i)] \right \}</script><p>在这个过程中减少了通道数以降低冗余。<br>然后，我们采用类似unet的解码器来融合适应后的多尺度特征。对于每个级别的特征 fi，我们将其与解码器块中的较低级别特征 fi+1 融合，其中 di 是解码器中的第 i 层特征。然后我们将得到的特征${d1, d2, d3}$ 连接起来以获得适应 RS 域的语义表示。表示为： </p><script type="math/tex; mode=display">d_1=f_1</script><script type="math/tex; mode=display">d_{i+1}=conv[f_{i+1},upsample(d_i)]</script><h4 id="Task-agnostic-semantic-learning（任务无关的语义学习）"><a href="#Task-agnostic-semantic-learning（任务无关的语义学习）" class="headerlink" title="Task-agnostic semantic learning（任务无关的语义学习）"></a>Task-agnostic semantic learning（任务无关的语义学习）</h4><p>文献表明，多个相关任务的联合学习可以提高每个单个任务的性能。为了提高 CD 的性能，我们引入了一个额外的时间语义学习分支。即对多时相 RSI 进行分类后进行变化分割。对于上文改编后的 SAM 特征 ${d1, d2, d3}$。我们进一步使用卷积运算将它们转换为候选潜在变量$\widehat{l}$ ∈ $R^{k×w×h}$，其中 k 表示 RSI 中感兴趣的语义簇的数量。<br><img src="https://pic.imgdb.cn/item/659a2443871b83018a3ff597.png" alt=""><br>每个$f$表示一个卷积融合操作。<br>我们使用底层的时间约束来监督潜在语义的学习。与明确监督 LCLU 类别学习的文献研究不同，在二进制 CD 任务中，每个采集日期的语义标签不可靠。因此，SAM-CD 通过对齐特征表示来隐式监督双时态潜在变量的学习。对于每个候选潜在变量$\widehat{l}$，使用 softmax 函数对它们进行归一化：</p><script type="math/tex; mode=display">\phi _T(\widehat{l}_i)=\frac{e^{\widehat{l}_i/T }}{ {\textstyle \sum_{j=1}^{n}}e^{\widehat{l}_j/T } }</script><p>其中T是控制输出特征概率分布的温度参数。我们设置 T &gt; 1 以获得更多样化的语义表示。令 ${l1, l2}$为归一化双时态潜在变量，我们期望它们的语义表示在未更改的区域中相似。因此，我们提出时间约束损失$L_t$来衡量它们的时间相似性，计算如下：</p><script type="math/tex; mode=display">L_t(l_1,l_2)=[1-cosine(l_1,l_2)]\cdot c</script><p>其中，c是真实值GT的变体，将未变化的区域注释为 1。这是为了从损失计算中排除更改的区域。进一步利用注意力操作将语义焦点嵌入到变化特征 fc 中，然后将它们映射到变化映射 m 中。通过将 SAM 特征 $d1,d2,d3$ 传入到卷积块中来获得变化特征。注意力嵌入操作如下：</p><script type="math/tex; mode=display">m=conv_2\left \{\sigma[conv_1(l_1\oplus l_2)]\cdot f_c  \right \}</script><p>其中⊕是通道级联操作，σ是sigmoid归一化函数，conv1和conv2是两个用于调整特征通道的卷积模块。这确保了 CD 结果是语义感知的，从而更好地分割语义变化。</p><h4 id="训练细节："><a href="#训练细节：" class="headerlink" title="训练细节："></a>训练细节：</h4><p>SAM-CD仅利用FastSAM的视觉编码器并丢弃提示解码器。在与任务无关的语义学习分支中，语义嵌入块是单个 1×1 卷积层。考虑到典型 CD 应用中通常很少有有趣的 LCLU 类，语义通道数 k 根据经验设置为 8。在变化检测分支中，我们按照文献中的实践，利用 6 层残差卷积块来构建变化嵌入模块。这是为了更好地将语义特征转化为变化表示。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;工作介绍&quot;&gt;&lt;a href=&quot;#工作介绍&quot; class=&quot;headerlink&quot; title=&quot;工作介绍&quot;&gt;&lt;/a&gt;工作介绍&lt;/h2&gt;&lt;p&gt;基于分割大模型sam，通过微调等方法迁移到分割相关领域辅助模型训练。&lt;/p&gt;
&lt;h2 id=&quot;相关文章&quot;&gt;&lt;a href=&quot;</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title>IIM论文笔记</title>
    <link href="http://wwffyy.life/posts/bb54d268.html"/>
    <id>http://wwffyy.life/posts/bb54d268.html</id>
    <published>2023-10-20T10:59:45.000Z</published>
    <updated>2024-01-07T04:12:46.289Z</updated>
    
    <content type="html"><![CDATA[<h1 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h1><p>IIM（Independent Instance Map segmentation）独立实例地图分割，被用于人群定位领域。特点如下：</p><ol><li>端到端框架（end to end）</li><li>每个实例是不重叠的。通过将人群分割成独立的连通分量，获得位置和人群计数（分别为中心和分量的数目）</li><li>创新点：提出可微分二值化模块：（BM）<ul><li>针对不同图像自适应地学习阈值图，以更准确地检测每个实例; </li><li>使用二进制预测和标签的损失直接训练模型。</li></ul></li><li>主要方法：遵循启发式分支，并利用连通分量分割进行人群定位<h1 id="数据"><a href="#数据" class="headerlink" title="数据"></a>数据</h1>本文中所使用的数据类型为独立实例映射（Independent Instance Map），每个实例不重叠<br><img src="https://img-blog.csdnimg.cn/211bc5d4151b48d4bff98dc07fcd1f38.png" alt=""><br><img src="https://img-blog.csdnimg.cn/28bece5258014fa3a038c3e92e0f4c89.png" alt=""><br>其中images文件夹下为真实图片，masks文件夹包含了每张图片对应的独立实例，展示如下：</li></ol><p><img src="https://img-blog.csdnimg.cn/c9ac5c5277804bc7bd92fc362437e7dc.png" alt=""><img src="https://img-blog.csdnimg.cn/484f31888bbc4bd5837f6b1283a8acab.png" alt=""><br><strong>NWPU-Crowd</strong> ：是目前最大和最具挑战性的开源人群数据集。它包含标头点和框标签。共有5109张图片和2133238个注释实例。<br><strong>Shanghai Tech</strong> ：包含两个子集：A部分有482张图像，共241677个实例，B部分包含716张图像，包括88，488个标记的头。<br><strong>UCF-QNRF</strong> ：是一个密集的人群数据集，由1535张图像组成，共有1251642个实例。<br><strong>FDST</strong> ：由13个不同场景中捕获的100个视频组成。它包含15000个帧，共有394081个头，包括点和框标签。</p><h1 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h1><p>首先对人群场景中的头部区域进行置信度预测。图像或像素级二值化模块将置信度图分割成独立的实例图。在推断期间，阈值模块为每个置信图在线预测阈值。最后，通过检测4个连通分量，得到每个独立实例区域的框和中心。<img src="https://img-blog.csdnimg.cn/49ade09f0de0482f8b373cd4649b7cbf.png#pic_center" alt=""></p><h2 id="二值化层"><a href="#二值化层" class="headerlink" title="二值化层"></a>二值化层</h2><p><img src="https://img-blog.csdnimg.cn/f2212564f0764f359ab57c8e90e0358e.png#pic_center" alt=""><br>二值化层的目标是学习阈值T以分割输入图像，使得其输出图像O尽可能接近目标图像G。<br>对于输入I，输出O，二值化层B给出如下定义：<br><img src="https://img-blog.csdnimg.cn/79071cf7f7b74e2196e9d7692807281c.png#pic_center" alt=""><br>其中阈值T是要学习的参数。这个前向过程是不可微的，因为它是一个比较运算。因此，不可能自动计算T的梯度。这里我们定义了一个类似的数学模型来模拟二进制输出O和T之间的关系，其中输出仅与T有关系。<br>假设I ∈ RH×W满足两个条件：</p><ol><li>$H ×W → ∞$; </li><li>$I（i，j）\sim U（0，1）$，均匀分布</li></ol><p>那么，从概率统计的角度来看，累积分布函数FI（i，j）（T）与T的关系如下：<br><img src="https://img-blog.csdnimg.cn/3753d05abb9c46d096a3ef2e0efcffe9.png#pic_center" alt=""><br>其中$P（I（i，j）&lt;T）$表示I中的像素值小于T的概率。</p><h2 id="损失与反向传播"><a href="#损失与反向传播" class="headerlink" title="损失与反向传播"></a>损失与反向传播</h2><p>为了计算反向传播的损失，论文中将输出图像O中的所有像素求和为标量,并进行归一化，定义yˆ：<br><img src="https://img-blog.csdnimg.cn/3c845580dd3740b780522315d60848c9.png#pic_center" alt=""><br>根据上述公式推导，易知y^与阈值T的关系有：<br><img src="https://img-blog.csdnimg.cn/9d44232413b04b62b59369c48a37a422.png#pic_center" alt=""><br>可以看出y ^和阈值T是线性运算，y ^对T的导数值为1。为了使真实标签与预测值具有同样的形式，将G定义为：<br>       <img src="https://img-blog.csdnimg.cn/262ffbd4118d4091b2ac5d0e5dd2354c.png#pic_center" alt=""><br>即将真实数据中所有点的像素值相加并进行归一化<br>根据学习速率α，更新阈值T，公式如下：<br><img src="https://img-blog.csdnimg.cn/19cad6ed422e41d1969c8ac2f420758e.png#pic_center" alt=""><br>$L(yˆ, y)$是真实值G与输出O之间的损失函数，可以选择为$L1loss$或者其他。</p><h2 id="二值化模块"><a href="#二值化模块" class="headerlink" title="二值化模块"></a>二值化模块</h2><p>为了使阈值根据图像内容进行更新，文章根据上述二值化层进一步提出了二值化模块（BM），该模块由阈值编码器和二值化层组成。</p><ul><li>阈值编码器：对图像的特征图进行编码并生成单个值或图</li><li>二值化层：利用该值/图来二值化置信度图并输出实例图</li></ul><p>作为阈值编码器，特征F可以由具有参数Θ的阈值编码器映射到阈值T：<br><img src="https://img-blog.csdnimg.cn/1aeab8a815c444edb58bf3f4195aff13.png#pic_center" alt=""><br>在此公式中，阈值T可以有两种形式，可以是标量或大小与输入I相同的矩阵。这两种阈值表达方式分别对应了固定阈值与逐像素级阈值。固定阈值对于一张图片为一固定标量值，像素级阈值对于每一个像素点处具有不同的阈值。<br><img src="https://img-blog.csdnimg.cn/b08ae747ec874644b1f365d983219d36.png#pic_center" alt=""><br><img src="https://img-blog.csdnimg.cn/1796ad212a714fe483414a164f5d8568.png#pic_center" alt=""><br>对于示意图来说，存在用于二值化层的两个输入：欠分割图像I和阈值T。对于阈值编码器，T的每个像素处的导数为-1。这意味着当梯度流到阈值编码器时，梯度将被反转阈值T。因此，阈值学习器中的参数Θ可以通过如下的梯度下降来优化：<br><img src="https://img-blog.csdnimg.cn/4525b639fb05403b903dee506268176f.png#pic_center" alt=""><br>其中β是阈值编码器的学习速率。除了优化阈值编码器之外，可以用二进制预测和标签来计算硬损失，并且进行从输入I到置信度图的预测网络的反向传播（例如，置信度预测器）。与阈值编码器不同，置信度预测器的梯度具有与损失L（y，y）相同的符号。假设输入I由具有参数θ的置信度预测器输出，θ利用学习速率γ更新为：<br><img src="https://img-blog.csdnimg.cn/9ae0f1d0c70748738e3060f9bdd8ba57.png#pic_center" alt=""><br>阈值编码器和置信度预测器起对抗作用。置信度编码器希望使目标区域具有较高置信度，背景区域具有较低置信度。阈值学习器力求使目标区域具有低阈值，而背景区域具有高阈值。通过这种方式，可以尽可能多地滤除背景噪声，并且可以保留低置信度前景，如小尺度和大尺度头部，用于定位。由于这两个任务是对抗的，我们在主干网络结构（卷积层）和阈值学习器之间添加梯度分离操作。</p><h1 id="框架建立"><a href="#框架建立" class="headerlink" title="框架建立"></a>框架建立</h1><h2 id="置信图预测器（CP）"><a href="#置信图预测器（CP）" class="headerlink" title="置信图预测器（CP）"></a>置信图预测器（CP）</h2><p>我们希望通过检测连接组件来实现人群定位，因此使用高分辨率特征表示的主干网络结构来提取特征。论文中，有两种流行的网络被用作置信度预测器：</p><ol><li>VGG-16 + FPN，其使用VGG-16作为骨干并利用特征金字塔网络来编码多尺度特征; </li><li>HRNet-W48，一种高分辨率网络，具有强大的视觉识别特征表示能力<h2 id="阈值编码器（TE）"><a href="#阈值编码器（TE）" class="headerlink" title="阈值编码器（TE）"></a>阈值编码器（TE）</h2><strong>两种方法：</strong> 这里，提出了两种方案来二值化置信图预测器（CP）的输出：图像级和像素级二值化模块（简称“IBM”和“PBM”）。IBM和PBM分别使用单个值和映射对预测进行二进制化。<br>为了从图像内容学习阈值，设计了阈值编码器（TE），其输入等于原始特征图乘置信度图：$F := I  \bigodot F$ 使用预测的置信图进行哈达玛积(即形状相同的矩阵对应位置的元素相乘）以过滤背景噪声。</li></ol><p><strong>IBM</strong>：在IBM中，应用1 × 1卷积层和全局平均池化（GAP）来输出单个值作为二值化层的可学习阈值。<br><strong>PBM</strong>：IBM只能对于一张输入图像只能学习到一个特定阈值，但不同尺度的头部的置信度分布是非常不同的。因此，PBM中的阈值编码器（TE）被提出以产生像素级阈值图，其由具有PReLU的四个卷积层和具有步长为1的两个大内核平均池化组成。配置如下所示：</p><p><em>Conv：3×3</em>，<em>PReLU</em>; <em>Conv：3×3</em>，<em>PReLU</em>; <em>Conv：3×3</em>，<em>PReLU</em>;<em>Avgpool</em>（平均池化）：15 ×15; Conv：1×1，<em>Avgpool</em>（平均池化）：15×15，<em>Sigmoid</em>。</p><p>在PBM的TE中，为了覆盖大的空间感受野并保存存储器，输入特征被调整为原始尺寸的1/8。此外，在最后两个卷积层之后，应用步长为1的15 × 15平均池化来平滑输出。</p><p><strong>压缩Sigmoid</strong> ：在实验中，TE可能会在阈值图中产生一些很低（小于0.1）或很高（接近1）的值，这使得网络很容易波动。此外，高阈值导致头部区域中的许多空洞。因此，为了限制Sigmoid激活函数的输出范围。提出了压缩Sigmoid，将输出约束在（0.2，0.7）之间，其公式为：<br><img src="https://img-blog.csdnimg.cn/e64d6c871a5944189209e73a72bea58e.png#pic_center" alt=""></p><ul><li>在得到阈值图之后，它将与置信度图一起被馈送到二值化层。然后二值化层产生分割图。最后，通过检测连通区域，得到独立实例的盒和中心。</li></ul><h2 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h2><p>在该框架中涉及两个损失函数：</p><p><strong>置信图学习损失函数</strong>：<br>置信图学习是一个回归问题，MSELoss（均方误差）可以很好地训练它</p><p><strong>阈值映射学习损失函数</strong>：<br>阈值映射学习也是一个回归问题，阈值范围为（0.2，0.7）.因此，L1损失函数被应用于等式中的<br>L（y^，y）用于训练阈值学习器。除此之外，该目标函数为置信度预测器提供梯度。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>总的来说，IIM运行的流程为：</p><ol><li>原图进入置信度预测器（参数为$θ$），可选择$HR-net$或$VGG-16 + FPN$，得到置信度图为$I(x,y)$，尺寸与原图大小相同</li><li>置信度图与原图对应位置元素相乘，得到$F := I  \bigodot F$，作为阈值编码器的输入</li><li>输入数据$F :$进入阈值编码器后经过具有$PReLU$的四个卷积层和池化层后，得到逐像素的阈值图$T(x,y)$</li><li>阈值图$T(x,y)$和置信度图$I(x,y)$同时进入二值化层，对置信度图进行二值化并输出实例图$O(x,y)$</li><li>将输出实例图与真实图转化成标量后进行计算损失，损失函数采用$L1loss$,用于训练阈值编码器，不要忘记在二值化层的梯度反转</li><li>使用$MSELoss$训练置信度预测器<h2 id="模型评估"><a href="#模型评估" class="headerlink" title="模型评估"></a>模型评估</h2>本文中在模型评估时使用的Instance-level Precision（准确率）、Recall（召回率）和F1-measure（F1值）是用于评估二分类模型性能的常用指标<br>准确率（Precision）衡量了模型预测为正例的样本中有多少是真正的正例。准确率可以通过以下公式计算：<script type="math/tex; mode=display">Precision= \frac{TP}{TP+FP}</script>​</li></ol><p>其中，TP表示真阳性（True Positives），FP表示假阳性（False Positives）。准确率越高，模型预测为正例的样本中真正的正例越多。</p><p>召回率（Recall）衡量了模型能够正确预测出多少真正的正例。召回率可以通过以下公式计算：</p><script type="math/tex; mode=display">Recall= \frac{TP}{TP+FN}</script><p>其中，TP表示真阳性（True Positives），FN表示假阴性（False Negatives）。召回率越高，模型能够正确预测出更多的真正的正例。</p><p>F1值（F1-measure）是准确率和召回率的加权调和平均值，综合考虑了模型的准确性和召回能力。F1值可以通过以下公式计算：</p><script type="math/tex; mode=display">F1= \frac{2\cdot Precision\cdot Recall}{Precision+Recall}</script><p>F1值的取值范围为0到1，值越高表示模型的性能越好。</p><h1 id="方法对比"><a href="#方法对比" class="headerlink" title="方法对比"></a>方法对比</h1><p>三个经典的人群定位方法和IIM在四个数据集上的性能：<br><img src="https://img-blog.csdnimg.cn/e019aa6c5f2b452b9b8042f6bd589f6a.png" alt=""></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;概述&quot;&gt;&lt;a href=&quot;#概述&quot; class=&quot;headerlink&quot; title=&quot;概述&quot;&gt;&lt;/a&gt;概述&lt;/h1&gt;&lt;p&gt;IIM（Independent Instance Map segmentation）独立实例地图分割，被用于人群定位领域。特点如下：&lt;/p&gt;</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title>Norm_Softmax_VIT笔记</title>
    <link href="http://wwffyy.life/posts/464412ff.html"/>
    <id>http://wwffyy.life/posts/464412ff.html</id>
    <published>2023-06-03T11:23:49.000Z</published>
    <updated>2023-08-04T11:32:23.861Z</updated>
    
    <content type="html"><![CDATA[<h2 id="一些术语"><a href="#一些术语" class="headerlink" title="一些术语"></a>一些术语</h2><h3 id="batch：批次，一批处理，"><a href="#batch：批次，一批处理，" class="headerlink" title="batch：批次，一批处理，"></a>batch：批次，一批处理，</h3><h3 id="batch-size：表示每个batch有多少样本"><a href="#batch-size：表示每个batch有多少样本" class="headerlink" title="batch_size：表示每个batch有多少样本"></a>batch_size：表示每个batch有多少样本</h3><h3 id="LR（learning-rate-：学习率"><a href="#LR（learning-rate-：学习率" class="headerlink" title="LR（learning rate)：学习率"></a>LR（learning rate)：学习率</h3><h3 id="patch：补丁"><a href="#patch：补丁" class="headerlink" title="patch：补丁"></a>patch：补丁</h3><h3 id="epoch：周期，阶段"><a href="#epoch：周期，阶段" class="headerlink" title="epoch：周期，阶段"></a>epoch：周期，阶段</h3><h3 id="criterion：评判准则（一般用于命名损失函数）"><a href="#criterion：评判准则（一般用于命名损失函数）" class="headerlink" title="criterion：评判准则（一般用于命名损失函数）"></a>criterion：评判准则（一般用于命名损失函数）</h3><h3 id="optimizer：优化器"><a href="#optimizer：优化器" class="headerlink" title="optimizer：优化器"></a>optimizer：优化器</h3><h3 id="BP：反向传播算法（Back-Propagation），"><a href="#BP：反向传播算法（Back-Propagation），" class="headerlink" title="BP：反向传播算法（Back Propagation），"></a>BP：反向传播算法（Back Propagation），</h3><p>通过计算误差的反向传播来更新网络的权重和偏置</p><h3 id="Embedding："><a href="#Embedding：" class="headerlink" title="Embedding："></a>Embedding：</h3><p>是指将高维的离散型数据（如词汇、用户ID等）转换为低维的连续型向量的过程，也可以指转换后的向量</p><h3 id="感受野（Receptive-Field-："><a href="#感受野（Receptive-Field-：" class="headerlink" title="感受野（Receptive Field)："></a>感受野（Receptive Field)：</h3><p>指在神经网络中，输出特征映射上的一个像素点对应在输入图像中的区域。感受野的大小取决于网络的架构和层数，它可以用来衡量网络对输入信息的感知范围和理解能力。</p><p>具体来说，感受野的大小在卷积神经网络(Convolutional Neural Network, CNN)中是递增的，随着网络层数的增加，感受野的大小也随之增加。在前面的卷积层中，每个像素点的感受野通常只是输入图像的一个小区域，但是在后面的卷积层中，每个像素点的感受野可以覆盖整个输入图像。这样，网络就可以学习到更全局的特征和上下文信息，以更好地理解输入图像并提高分类或检测的准确性。</p><p>感受野的大小与卷积核的大小、步幅和填充有关。通常，较大的卷积核、较小的步幅和合适的填充可以增加感受野的大小。此外，池化层(Pooling Layer)也可以增加感受野大小，它通过池化操作对输入特征映射进行缩小，从而扩大感受野范围。</p><p>在神经网络设计中，理解和控制感受野大小可以帮助我们更好地设计网络结构和优化算法，从而提高网络的性能和稳定性。</p><h3 id="MLP（Multilayer-Perceptron，多层感知器）"><a href="#MLP（Multilayer-Perceptron，多层感知器）" class="headerlink" title="MLP（Multilayer Perceptron，多层感知器）"></a>MLP（Multilayer Perceptron，多层感知器）</h3><p>是一种基于前馈神经网络（Feedforward Neural Network）的模型，通常用于处理分类和回归问题。它由多个节点（神经元）组成，每个节点与前一层的所有节点相连。每个神经元的输出值是由前一层的神经元输出值通过权重参数的线性组合和激活函数的非线性变换计算得到的。MLP通常包含一个或多个隐藏层，以及一个输出层。也成为全连接神经网络（Fully Connected Neural Network）</p><h2 id="BN-Details"><a href="#BN-Details" class="headerlink" title="BN Details"></a>BN Details</h2><ol><li>Batch Normalization（BN）（批量归一化）是一种常用的神经网络正则化技术，旨在解决深度神经网络训练过程中的梯度消失和梯度爆炸问题，并改善网络性能和收敛速度。</li><li>为什么？<br> 每一层参数更新，会导致输入分布差距</li><li>是什么？<br>小批量数据计算均值方差，处理归一</li><li>怎么做？<br>B={x1,x2,…xn}<br>计算出该小批量数据的均值与方差，归一化<script type="math/tex; mode=display">\widehat{x}_{i}=\frac{x_{i}-\mu _{B}}{\sqrt{\sigma _{B}^{2}+\varepsilon}}</script><script type="math/tex; mode=display">y_{i}=\gamma \widehat{x}_{i}+\beta</script>乘系数加偏置是为了输入不是集中在-1，1，在通过激活函数后非线性.<h4 id="优缺点："><a href="#优缺点：" class="headerlink" title="优缺点："></a>优缺点：</h4></li><li><p>BN的基本思想是将神经网络中每个隐藏层的输入标准化，以使其均值为0，方差为1。具体来说，BN在每个训练批次中计算出批次输入的均值和方差，然后将其应用于批次中的每个样本。标准化后的样本通过调整缩放因子和偏移因子来恢复其原始输入特征空间，这些因子通过学习得到。这样，即使网络的输入和参数分布发生变化，BN也可以保持网络的稳定性。</p></li><li><p>BN的主要优点是可以提高网络的收敛速度和泛化能力，并具有一定的正则化作用，可以减少过拟合的风险。此外，BN可以减少对学习率和其他超参数的依赖，提高网络的可训练性和可调节性。</p></li><li><p>BN的一些缺点包括增加了网络的计算复杂性和内存使用，特别是在大型网络和GPU计算上。此外，BN对小批量数据的表现可能很差，因为它需要计算均值和方差的无偏估计，必须依赖于有效的批量大小。</p></li><li><p>总之，BN是一种强大的神经网络正则化技术，可以改善网络性能和训练过程的稳定性，特别是在深度神经网络和大型数据集上。</p><h2 id="Layer-Norm"><a href="#Layer-Norm" class="headerlink" title="Layer Norm"></a>Layer Norm</h2><p>Layer Norm（层标准化）是深度学习中一种常用的归一化方法，用于减小模型中许多常见问题，如梯度消失（vanishing gradients）和梯度爆炸（exploding gradients）等。它是对Batch Norm的一种改进，解决了Batch Norm难以应用于序列模型和小批量数据的问题。</p></li></ol><p>Layer Norm对于每个样本的每个特征分别进行归一化，使得每个样本在每个特征上都具有相似的分布，从而提高了模型的泛化性能。在Layer Norm中，对于每个样本的第i个特征，其标准化后的值为：</p><script type="math/tex; mode=display">\widehat{x}_{norm}=\frac{x_{i}-\mu _{B}}{\sqrt{\sigma _{B}^{2}+\varepsilon}}</script><p>其中，均值和方差是在特征维度上计算的，epsilon是一个很小的常量，用于防止除以0</p><script type="math/tex; mode=display">LayerNorm(x) = \gamma * x_{norm} + \beta</script><h3 id="区别"><a href="#区别" class="headerlink" title="区别"></a>区别</h3><ul><li>Layer Norm与Batch Norm的区别在于，Layer Norm是对每一层的输出进行标准化处理，而Batch Norm是对每一个batch进行标准化处理。<h2 id="Softmax-Details"><a href="#Softmax-Details" class="headerlink" title="Softmax Details"></a>Softmax Details</h2>softmax是一种用于多分类问题的激活函数，主要用于将一个向量转换为概率分布。在深度学习中，softmax通常用于输出层，将神经元的输出转换为类别概率。softmax函数的实现原理如下：<ol><li>计算每个神经元的指数值：对于输入向量x中的每个元素xi，计算<script type="math/tex">exp(xi)</script></li><li>计算指数值的和：将每个指数值相加，得到:<script type="math/tex">sum = Σ(exp(xi))</script></li><li>计算每个神经元的概率值：对于每个xi，计算:<script type="math/tex">pi = exp(xi) / sum</script></li><li>输出概率分布：将所有pi组成的向量作为输出，即<script type="math/tex">softmax(x) = [p1, p2, ..., pn]</script><br>其中，n是神经元的数量。softmax函数的输出可以被视为一个概率分布，其中每个元素pi表示输入向量属于第i个类别的概率。<h2 id="Transformer-VIT"><a href="#Transformer-VIT" class="headerlink" title="Transformer/VIT"></a>Transformer/VIT</h2>VIT（Vision Transformer）是用于图像分类的基于Transformer的模型。其步骤如下：</li></ol></li></ul><p><img src="https://pic4.zhimg.com/v2-5afd38bd10b279f3a572b13cda399233_r.jpg" alt="1132"></p><ol><li><p>图像的处理：将输入图像分割成多个小图像（patch）。</p></li><li><p>patch的线性变换：对于每个patch，使用一个线性变换将其转换为一个向量(patch_embesdding)</p></li><li><p>位置编码：为每个patch添加位置编码，以表示它们在原始图像中的位置(posttion_embedding)（与patch_embedding的形状完全一样）</p></li><li><p>输入嵌入：将所有patch的向量和位置编码拼接(相加)在一起，形成一个输入嵌入（input embedding）矩阵。</p></li><li><p>Layer Norm</p></li><li><p>Transformer Encoder：对输入嵌入进行多层Transformer编码器的处理，以便进行全局的特征提取。这里使用Multi-Head Attention方法，具体步骤如下：</p><ul><li><p>特征线性变换：每个输入序列复制3份（Q，K，V），经过线性变换，即将输入序列转换为多个查询（query）、键（key）和值（value）向量。</p></li><li><p>计算注意力分数：对于每个查询向量，计算它与所有键向量的内积，得到注意力分数的一组向量。<script type="math/tex">Output_{p1}=Attention(p1,p2)*p2+Attention(p1,p3)*p3+...+Attention(p1,p64)*p64</script></p></li><li><p>归一化：对注意力分数进行softmax归一化，得到所有键的权重分布。</p></li><li><p>加权求值：将所有值向量按照权重分布加权求和，得到该查询向量的注意力表示:引入wq，wk，wv权重，得到新的Q，K，V</p></li><li><p>多头注意力（Multi-Head）：将上述步骤在多个子空间中分别进行，得到不同子空间的注意力表示。即：wq，wk，wv权重矩阵有H组，计算注意力分数的操作可以进行H次。</p></li><li><p>输出合并：将各子空间的注意力（H次Multi-Head运算的结果）表示拼接成一个向量。使用softmax进行处理。</p></li><li><p>线性变换：将合并后的向量再进行一次线性变换，得到最终输出。</p></li></ul></li><li><p>Norm</p></li><li><p>（全局池化）：对于输出矩阵中的所有特征向量，进行全局池化（如平均池化或最大池化）得到一个全局特征向量。</p></li><li><p>全连接层：将全局特征向量输入到一个全连接层，进行分类预测。为了实现每个像素之间的交互。<br>附：师兄笔记<br><img src="https://i.imgtg.com/2023/06/03/Oqj90I.jpg" alt=""><br><img src="https://i.imgtg.com/2023/06/03/Oqj2MD.jpg" alt=""></p></li></ol><p><link rel="stylesheet" href="https://cdn1.tianli0.top/gh/zhheo/Post-Abstract-AI@0.7/tianli_gpt.css"></p><script>let tianliGPT_postSelector = '#post #article-container';let tianliGPT_key = 'd90e7ff2968fd4a313cd';</script><script src="https://cdn1.tianli0.top/gh/zhheo/Post-Abstract-AI@0.7/tianli_gpt.js"></script>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;一些术语&quot;&gt;&lt;a href=&quot;#一些术语&quot; class=&quot;headerlink&quot; title=&quot;一些术语&quot;&gt;&lt;/a&gt;一些术语&lt;/h2&gt;&lt;h3 id=&quot;batch：批次，一批处理，&quot;&gt;&lt;a href=&quot;#batch：批次，一批处理，&quot; class=&quot;headerli</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title>斯坦福 cs231n笔记</title>
    <link href="http://wwffyy.life/posts/c1ef5b44.html"/>
    <id>http://wwffyy.life/posts/c1ef5b44.html</id>
    <published>2023-04-25T00:42:02.000Z</published>
    <updated>2024-01-07T04:14:03.641Z</updated>
    
    <content type="html"><![CDATA[<p><img src="/images/2.jpg" alt=""></p><h1 id="图像分类"><a href="#图像分类" class="headerlink" title="图像分类"></a>图像分类</h1><h3 id="目标"><a href="#目标" class="headerlink" title="目标"></a>目标</h3><p>所谓图像分类问题，就是已有固定的分类标签集合，然后对于输入的图像，从分类标签集合中找出一个分类标签，最后把分类标签分配给该输入图像。虽然看起来挺简单的，但这可是计算机视觉领域的核心问题之一，并且有着各种各样的实际应用。计算机视觉领域中很多看似不同的问题（比如物体检测和分割），都可以被归结为图像分类问题。</p><h3 id="困难和挑战"><a href="#困难和挑战" class="headerlink" title="困难和挑战"></a>困难和挑战</h3><p>对于人来说，识别出一个像“猫”一样视觉概念是简单至极的，然而从计算机视觉算法的角度来看就值得深思了。以下为计算机视觉算法在图像识别方面遇到的一些困难，图像是以3维数组来表示的，数组中的元素是亮度值。</p><ul><li><strong>视角变化（Viewpoint variation</strong>）**：同一个物体，摄像机可以从多个角度来展现。</li><li><strong>大小变化（Scale variation</strong>）**：物体可视的大小通常是会变化的（不仅是在图片中，在真实世界中大小也是变化的）。</li><li><strong>形变（Deformation</strong>）：很多东西的形状并非一成不变，会有很大变化。</li><li><strong>遮挡（Occlusion</strong>）：目标物体可能被挡住。有时候只有物体的一小部分（可以小到几个像素）是可见的。</li><li><strong>光照条件（Illumination conditions</strong>）：在像素层面上，光照的影响非常大。</li><li><strong>背景干扰（Background clutter</strong>）：物体可能混入背景之中，使之难以被辨认。</li><li><strong>类内差异（Intra-class variation</strong>）：一类物体的个体之间的外形差异很大，比如椅子。这一类物体有许多不同的对象，每个都有自己的外形。</li></ul><p>面对以上所有变化及其组合，好的图像分类模型能够在维持分类结论稳定的同时，保持对类间差异足够敏感。</p><h3 id="最近邻分类器Nearest-Neighbour"><a href="#最近邻分类器Nearest-Neighbour" class="headerlink" title="最近邻分类器Nearest Neighbour"></a>最近邻分类器Nearest Neighbour</h3><p>那么具体如何比较两张图片呢？在本例中，就是比较32（长）x32（宽）x3（RGB）的像素块。最简单的方法就是逐个像素比较，最后将差异值全部加起来。换句话说，就是将两张图片先转化为两个向量<code>I1</code>和<code>I2</code>，然后计算他们的<strong>L1距离：</strong></p><script type="math/tex; mode=display">d_{1}(I_{1},I_{2})=\sum_{p}^{}|I_{1}^{p}-I_{2}^{p}|</script><p>以图片中的一个颜色通道为例来进行说明。两张图片使用L1距离来进行比较。逐个像素求差值，然后将所有差值加起来得到一个数值。如果两张图片一模一样，那么L1距离为0，但是如果两张图片很是不同，那L1值将会非常大。<br>代码操作：首先，将CIFAR-10的数据加载到内存中，并分成4个数组：训练数据和标签，测试数据和标签。在下面的代码中，<strong>Xtr</strong>（大小是50000x32x32x3）存有训练集中所有的图像，<strong>Ytr</strong>是对应的长度为50000的1维数组，存有图像对应的分类标签（从0到9）：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Xtr, Ytr, Xte, Yte = load_CIFAR10(<span class="string">&#x27;data/cifar10/&#x27;</span>) <span class="comment"># a magic function we provide</span></span><br><span class="line"><span class="comment"># flatten out all images to be one-dimensional</span></span><br><span class="line">Xtr_rows = Xtr.reshape(Xtr.shape[<span class="number">0</span>], <span class="number">32</span> * <span class="number">32</span> * <span class="number">3</span>) <span class="comment"># Xtr_rows becomes 50000 x 3072</span></span><br><span class="line">Xte_rows = Xte.reshape(Xte.shape[<span class="number">0</span>], <span class="number">32</span> * <span class="number">32</span> * <span class="number">3</span>) <span class="comment"># Xte_rows becomes 10000 x 3072</span></span><br></pre></td></tr></table></figure><br>训练一个分类器<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">nn = NearestNeighbor() <span class="comment"># create a Nearest Neighbor classifier class</span></span><br><span class="line">nn.train(Xtr_rows, Ytr) <span class="comment"># train the classifier on the training images and labels</span></span><br><span class="line">Yte_predict = nn.predict(Xte_rows) <span class="comment"># predict labels on the test images</span></span><br><span class="line"><span class="comment"># and now print the classification accuracy, which is the average number</span></span><br><span class="line"><span class="comment"># of examples that are correctly predicted (i.e. label matches)</span></span><br><span class="line"><span class="built_in">print</span> <span class="string">&#x27;accuracy: %f&#x27;</span> % ( np.mean(Yte_predict == Yte) )</span><br></pre></td></tr></table></figure><br>作为评价标准，我们常常使用<strong>准确率</strong>，它描述了我们预测正确的得分。请注意以后我们实现的所有分类器都需要有这个API：train(X, y)函数。该函数使用训练集的数据和标签来进行训练。从其内部来看，类应该实现一些关于标签和标签如何被预测的模型。这里还有个predict(X)函数，它的作用是预测输入的新数据的分类标签。现在还没介绍分类器的实现，下面就是使用L1距离的Nearest Neighbor分类器的实现套路：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">NearestNeighbor</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line">  <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">    <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">def</span> <span class="title function_">train</span>(<span class="params">self, X, y</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot; X is N x D where each row is an example. Y is 1-dimension of size N &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># the nearest neighbor classifier simply remembers all the training data</span></span><br><span class="line">    self.Xtr = X</span><br><span class="line">    self.ytr = y</span><br><span class="line"></span><br><span class="line">  <span class="keyword">def</span> <span class="title function_">predict</span>(<span class="params">self, X</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot; X is N x D where each row is an example we wish to predict label for &quot;&quot;&quot;</span></span><br><span class="line">    num_test = X.shape[<span class="number">0</span>]</span><br><span class="line">    <span class="comment"># lets make sure that the output type matches the input type</span></span><br><span class="line">    Ypred = np.zeros(num_test, dtype = self.ytr.dtype)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># loop over all test rows</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> xrange(num_test):</span><br><span class="line">      <span class="comment"># find the nearest training image to the i&#x27;th test image</span></span><br><span class="line">      <span class="comment"># using the L1 distance (sum of absolute value differences)</span></span><br><span class="line">      distances = np.<span class="built_in">sum</span>(np.<span class="built_in">abs</span>(self.Xtr - X[i,:]), axis = <span class="number">1</span>)</span><br><span class="line">      min_index = np.argmin(distances) <span class="comment"># get the index with smallest distance</span></span><br><span class="line">      Ypred[i] = self.ytr[min_index] <span class="comment"># predict the label of the nearest example</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> Ypred</span><br></pre></td></tr></table></figure><br>另一种计算向量间距离的方法，L2距离</p><script type="math/tex; mode=display">d_{1}(I_{1},I_{2})=\sqrt{\sum (I_{1}^{p}-I_{2}^{p})^{2}}</script><p>修改距离计算方法<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">distances = np.sqrt(np.<span class="built_in">sum</span>(np.square(self.Xtr - X[i,:]), axis = <span class="number">1</span>))</span><br></pre></td></tr></table></figure></p><h3 id="K-Nearest-Neighbour分类器"><a href="#K-Nearest-Neighbour分类器" class="headerlink" title="K-Nearest Neighbour分类器"></a>K-Nearest Neighbour分类器</h3><p>最近邻分类器只找最相近的那1个图片的标签，而k近邻是找最相似的k个图片的标签，然后让他们针对测试图片进行投票，最后把票数最高的标签作为对测试图片的预测。所以当k=1的时候，k-Nearest Neighbor分类器就是Nearest Neighbor分类器。从直观感受上就可以看到，更高的k值可以让分类的效果更平滑，使得分类器对于异常值更有抵抗力。<br>k值选取?(超参数)<br>验证集调优方法：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># assume we have Xtr_rows, Ytr, Xte_rows, Yte as before</span></span><br><span class="line"><span class="comment"># recall Xtr_rows is 50,000 x 3072 matrix</span></span><br><span class="line">Xval_rows = Xtr_rows[:<span class="number">1000</span>, :] <span class="comment"># take first 1000 for validation</span></span><br><span class="line">Yval = Ytr[:<span class="number">1000</span>]</span><br><span class="line">Xtr_rows = Xtr_rows[<span class="number">1000</span>:, :] <span class="comment"># keep last 49,000 for train</span></span><br><span class="line">Ytr = Ytr[<span class="number">1000</span>:]</span><br><span class="line"></span><br><span class="line"><span class="comment"># find hyperparameters that work best on the validation set</span></span><br><span class="line">validation_accuracies = []</span><br><span class="line"><span class="keyword">for</span> k <span class="keyword">in</span> [<span class="number">1</span>, <span class="number">3</span>, <span class="number">5</span>, <span class="number">10</span>, <span class="number">20</span>, <span class="number">50</span>, <span class="number">100</span>]:</span><br><span class="line"></span><br><span class="line">  <span class="comment"># use a particular value of k and evaluation on validation data</span></span><br><span class="line">  nn = NearestNeighbor()</span><br><span class="line">  nn.train(Xtr_rows, Ytr)</span><br><span class="line">  <span class="comment"># here we assume a modified NearestNeighbor class that can take a k as input</span></span><br><span class="line">  Yval_predict = nn.predict(Xval_rows, k = k)</span><br><span class="line">  acc = np.mean(Yval_predict == Yval)</span><br><span class="line">  <span class="built_in">print</span> <span class="string">&#x27;accuracy: %f&#x27;</span> % (acc,)</span><br><span class="line"></span><br><span class="line">  <span class="comment"># keep track of what works on the validation set</span></span><br><span class="line">  validation_accuracies.append((k, acc))</span><br></pre></td></tr></table></figure><br>程序结束后，可以作图分析出哪个k值表现最好，然后用这个k值来跑真正的测试集，并作出对算法的评价。（把训练集分成训练集和验证集。使用验证集来对所有超参数调优。最后只在测试集上跑一次并报告结果）<br><strong>交叉验证</strong>：有时候，训练集数量较小（因此验证集的数量更小），会使用一种被称为<strong>交叉验证</strong>的方法，这种方法更加复杂些。还是用刚才的例子，如果是交叉验证集，我们就不是取1000个图像，而是将训练集平均分成5份，其中4份用来训练，1份用来验证。然后我们循环着取其中4份来训练，其中1份来验证，最后取所有5次验证结果的平均值作为算法验证结果。</p><h1 id="线性分类"><a href="#线性分类" class="headerlink" title="线性分类"></a>线性分类</h1><h1 id="神经网络与反向传播"><a href="#神经网络与反向传播" class="headerlink" title="神经网络与反向传播"></a>神经网络与反向传播</h1><h1 id="卷积神经网络"><a href="#卷积神经网络" class="headerlink" title="卷积神经网络"></a>卷积神经网络</h1><h1 id="循环神经网络"><a href="#循环神经网络" class="headerlink" title="循环神经网络"></a>循环神经网络</h1>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;&lt;img src=&quot;/images/2.jpg&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;h1 id=&quot;图像分类&quot;&gt;&lt;a href=&quot;#图像分类&quot; class=&quot;headerlink&quot; title=&quot;图像分类&quot;&gt;&lt;/a&gt;图像分类&lt;/h1&gt;&lt;h3 id=&quot;目标&quot;&gt;&lt;a href=&quot;#目标&quot;</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title>线性代数知识点（遗忘则补充）</title>
    <link href="http://wwffyy.life/posts/376cf89b.html"/>
    <id>http://wwffyy.life/posts/376cf89b.html</id>
    <published>2023-03-18T14:50:20.000Z</published>
    <updated>2024-01-07T04:13:12.399Z</updated>
    
    <content type="html"><![CDATA[<h2 id="第1章-行列式"><a href="#第1章-行列式" class="headerlink" title="第1章 行列式"></a>第1章 行列式</h2><h3 id="1-1-全排列和对换"><a href="#1-1-全排列和对换" class="headerlink" title="1.1 全排列和对换"></a>1.1 全排列和对换</h3><h3 id="1-2-n阶行列式"><a href="#1-2-n阶行列式" class="headerlink" title="1.2 n阶行列式"></a>1.2 n阶行列式</h3><h3 id="1-3-行列式的性质"><a href="#1-3-行列式的性质" class="headerlink" title="1.3 行列式的性质"></a>1.3 行列式的性质</h3><h3 id="1-4-行列式按行（列）展开"><a href="#1-4-行列式按行（列）展开" class="headerlink" title="1.4 行列式按行（列）展开"></a>1.4 行列式按行（列）展开</h3><h2 id="第2章-矩阵及其运算"><a href="#第2章-矩阵及其运算" class="headerlink" title="第2章 矩阵及其运算"></a>第2章 矩阵及其运算</h2><h3 id="2-1-线性方程组和矩阵"><a href="#2-1-线性方程组和矩阵" class="headerlink" title="2.1 线性方程组和矩阵"></a>2.1 线性方程组和矩阵</h3><h3 id="2-2-矩阵的运算"><a href="#2-2-矩阵的运算" class="headerlink" title="2.2 矩阵的运算"></a>2.2 矩阵的运算</h3><h3 id="2-3-逆矩阵"><a href="#2-3-逆矩阵" class="headerlink" title="2.3 逆矩阵"></a>2.3 逆矩阵</h3><h3 id="2-4-Cramer法则"><a href="#2-4-Cramer法则" class="headerlink" title="2.4 Cramer法则"></a>2.4 Cramer法则</h3><h2 id="第3章-矩阵的初等变换与线性方程组"><a href="#第3章-矩阵的初等变换与线性方程组" class="headerlink" title="第3章 矩阵的初等变换与线性方程组"></a>第3章 矩阵的初等变换与线性方程组</h2><h3 id="3-1-矩阵的初等变换"><a href="#3-1-矩阵的初等变换" class="headerlink" title="3.1 矩阵的初等变换"></a>3.1 矩阵的初等变换</h3><h3 id="3-2-矩阵的秩"><a href="#3-2-矩阵的秩" class="headerlink" title="3.2 矩阵的秩"></a>3.2 矩阵的秩</h3><h3 id="3-3-方程组的解"><a href="#3-3-方程组的解" class="headerlink" title="3.3 方程组的解"></a>3.3 方程组的解</h3><h2 id="第4章-向量组的线性相关性"><a href="#第4章-向量组的线性相关性" class="headerlink" title="第4章 向量组的线性相关性"></a>第4章 向量组的线性相关性</h2><h3 id="4-1-向量组及其线性组合"><a href="#4-1-向量组及其线性组合" class="headerlink" title="4.1 向量组及其线性组合"></a>4.1 向量组及其线性组合</h3><h3 id="4-2-向量组的线性相关性"><a href="#4-2-向量组的线性相关性" class="headerlink" title="4.2 向量组的线性相关性"></a>4.2 向量组的线性相关性</h3><h3 id="4-3-向量组的秩"><a href="#4-3-向量组的秩" class="headerlink" title="4.3 向量组的秩"></a>4.3 向量组的秩</h3><h3 id="4-4-线性方程组解的结构"><a href="#4-4-线性方程组解的结构" class="headerlink" title="4.4 线性方程组解的结构"></a>4.4 线性方程组解的结构</h3><h3 id="4-5-向量空间"><a href="#4-5-向量空间" class="headerlink" title="4.5 向量空间"></a>4.5 向量空间</h3><h2 id="第5章-相似矩阵及二次型"><a href="#第5章-相似矩阵及二次型" class="headerlink" title="第5章 相似矩阵及二次型"></a>第5章 相似矩阵及二次型</h2><h3 id="5-1-向量的内积、长度及正交性"><a href="#5-1-向量的内积、长度及正交性" class="headerlink" title="5.1 向量的内积、长度及正交性"></a>5.1 向量的内积、长度及正交性</h3><h3 id="5-2-方阵的特征值与特征向量"><a href="#5-2-方阵的特征值与特征向量" class="headerlink" title="5.2 方阵的特征值与特征向量"></a>5.2 方阵的特征值与特征向量</h3><h3 id="5-3-相似矩阵"><a href="#5-3-相似矩阵" class="headerlink" title="5.3 相似矩阵"></a>5.3 相似矩阵</h3><h3 id="5-4-对称矩阵的对角化"><a href="#5-4-对称矩阵的对角化" class="headerlink" title="5.4 对称矩阵的对角化"></a>5.4 对称矩阵的对角化</h3><h3 id="5-5-二次型及其标准型"><a href="#5-5-二次型及其标准型" class="headerlink" title="5.5 二次型及其标准型"></a>5.5 二次型及其标准型</h3><h3 id="5-6-正定二次型"><a href="#5-6-正定二次型" class="headerlink" title="5.6 正定二次型"></a>5.6 正定二次型</h3>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;第1章-行列式&quot;&gt;&lt;a href=&quot;#第1章-行列式&quot; class=&quot;headerlink&quot; title=&quot;第1章 行列式&quot;&gt;&lt;/a&gt;第1章 行列式&lt;/h2&gt;&lt;h3 id=&quot;1-1-全排列和对换&quot;&gt;&lt;a href=&quot;#1-1-全排列和对换&quot; class=&quot;head</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title>机器学习笔记</title>
    <link href="http://wwffyy.life/posts/5a8a6c8d.html"/>
    <id>http://wwffyy.life/posts/5a8a6c8d.html</id>
    <published>2023-03-13T16:28:33.000Z</published>
    <updated>2024-01-07T04:14:22.306Z</updated>
    
    <content type="html"><![CDATA[<h1 id="统计学习方法"><a href="#统计学习方法" class="headerlink" title="统计学习方法"></a>统计学习方法</h1><p><em>赫尔伯特.西蒙：”如果一个系统能够通过执行某个过程改进它的性能，这就是学习。“</em></p><h3 id="1-1-统计学习-基本概念"><a href="#1-1-统计学习-基本概念" class="headerlink" title="1.1 统计学习 基本概念"></a>1.1 统计学习 基本概念</h3><ul><li>对象： 数据</li><li>目的： 对数据预测与分析</li><li>方法： 监督学习，无监督学习，强化学习</li><li>三要素： 模型，策略，算法</li></ul><h3 id="1-2-统计学习分类"><a href="#1-2-统计学习分类" class="headerlink" title="1.2 统计学习分类"></a>1.2 统计学习分类</h3><h4 id="1-2-1监督学习："><a href="#1-2-1监督学习：" class="headerlink" title="1.2.1监督学习："></a>1.2.1监督学习：</h4><p>学习输入到输出的统计规律。</p><ol><li>输入空间，输出空间，特征空间：输入与输出对成为样本</li><li>联合概率分布：假设输入X输出Y遵循联合分布概率P（X，Y），表示分布密度函数</li><li>假设空间：学习范围的确定</li><li>模型：用训练数据集学习一个模型，再用模型对测试样本集进行预测。由学习系统和预测系统完成。</li></ol><h4 id="1-2-2无监督学习："><a href="#1-2-2无监督学习：" class="headerlink" title="1.2.2无监督学习："></a>1.2.2无监督学习：</h4><p>从无标注数据中学习预测模型，学习数据中的统计规律或潜在结构。</p><ol><li>使用无标注数据学习或训练</li><li>可以用于对已有数据的分析，也可以对未来数据预测</li></ol><h4 id="1-2-3强化学习"><a href="#1-2-3强化学习" class="headerlink" title="1.2.3强化学习"></a>1.2.3强化学习</h4><p>智能系统在与环境的连续互动中学习最优行为策略的机器学习问题，本质为学习最优的序贯决策</p><ol><li>智能系统与环境互动：在每一步t，智能系统在环境中观测到一个状态st和一个奖励rt，采取一个动作at。环境根据智能体选择的动作，决定t+1的状态与奖励。目标是长期奖励的最大化。</li><li>马尔可夫决策过程</li></ol><h4 id="1-2-4半监督学习与主动学习"><a href="#1-2-4半监督学习与主动学习" class="headerlink" title="1.2.4半监督学习与主动学习"></a>1.2.4半监督学习与主动学习</h4><p>半监督学习，使用少量标注数据与大量未标注数据<br>主动学习，机器不断主动给出实例让教师进行标注，然后利用标注数据学习预测模型</p><h3 id="1-3-按模型分类"><a href="#1-3-按模型分类" class="headerlink" title="1.3 按模型分类"></a>1.3 按模型分类</h3><ol><li>概率模型与非概率模型：<ul><li>概率模型：决策树，朴素贝叶斯，隐马尔可夫，条件随机场，概率潜在语义分析，潜在迪利克雷分配，高斯混合模型</li><li>非概率模型：感知机，支持向量机，k近邻，adaboost，kmeans，潜在语义分析，神经网络<br>在监督学习中，概率模型是生成模型，非概率模型是判别模型<br>概率模型可用基本概率加法和乘法模型进行概率推理。</li></ul></li><li>线性模型与非线性模型：<ul><li>针对非概率模型，如果y=f（x）或z=g（x）是线性函数，则称模型为线性模型</li><li>否则为非线性模型</li></ul></li><li>参数化模型与非参数化模型：<ul><li>参数化模型假设模型参数维度固定，模型可用有限维数刻画</li><li>非参数化：维数不固定或无穷大，随训练数据量增大而增大</li></ul></li></ol><h3 id="1-4按算法分类"><a href="#1-4按算法分类" class="headerlink" title="1.4按算法分类"></a>1.4按算法分类</h3><ol><li>在线学习：每次接受一个样本进行预测，之后学习模型</li><li>批量学习：一次接受所有数据，学习模型之后进行预测</li></ol><h3 id="1-5三要素"><a href="#1-5三要素" class="headerlink" title="1.5三要素"></a>1.5三要素</h3><ol><li>模型：模型的假设空间包含所有可能的条件概率或决策函数</li><li>策略：在假设空间中寻找最优模型<ul><li>损失和风险函数：经验风险是模型关于训练样本集的平均损失记作Rexp（f）当样本容量趋于无穷时，经验风险趋于期望风险。</li><li>经验风险最小化和结构风险最小化</li><li>过拟合：为了减少训练误差而使得模型复杂度增加，导致测试误差增大（M次多项式拟合问题）</li></ul></li><li>算法：模型的具体计算方法</li></ol><h3 id="1-6正则化与交叉验证"><a href="#1-6正则化与交叉验证" class="headerlink" title="1.6正则化与交叉验证"></a>1.6正则化与交叉验证</h3><ol><li>正则化：在经验风险上加一个正则化项，模型越复杂，正则化数越大。</li><li>交叉验证：给定的数据切分，组合成为训练集与测试集，反复训练</li></ol><h3 id="1-7泛化能力"><a href="#1-7泛化能力" class="headerlink" title="1.7泛化能力"></a>1.7泛化能力</h3><ul><li>指由该方法学习到的模型对未知数据的预测能力</li><li>设学到的模型为f</li></ul><script type="math/tex; mode=display">R_{exp}(\widehat{f})=E_{p}[L(Y,\widehat{f}(X))]=\int_{\chi * \gamma }^{}L(y,\widehat{f}(x))P(x,y)d_{x}d_{y}</script><h3 id="1-8生成模型与判别模型（在监督学习中）"><a href="#1-8生成模型与判别模型（在监督学习中）" class="headerlink" title="1.8生成模型与判别模型（在监督学习中）"></a>1.8生成模型与判别模型（在监督学习中）</h3><ol><li>生成方法：由数据学习联合概率分布P（X，Y），求出P（Y|X）作为预测的模型：<script type="math/tex; mode=display">P(Y|X)=P(X,Y)/P(X)</script><ul><li>因为模型表示了给定输入X产生输出Y的生成关系</li><li>e.g.朴素贝叶斯，隐马尔可夫</li><li>特点：可以还原出联合分布概率P（X，Y），收敛速度更快</li></ul></li><li>判别方法：直接学习决策函数f（x）或条件概率分布P（X，Y）作为预测的模型。关心的是对给定的输入X，应预测什么样的输出Y<ul><li>特点：直接面对预测，直接学习P（Y|X）或决策函数f（X），可以对数据进行各种程度上的抽象、定义特征并使用特征，可以简化学习问题。</li></ul></li></ol><h3 id="1-9-监督学习应用"><a href="#1-9-监督学习应用" class="headerlink" title="1.9 监督学习应用"></a>1.9 监督学习应用</h3><ol><li>分类问题</li><li>标注问题<ul><li>学习一个模型，使它能够对观测序列给出标记序列作为预测（单词序列预测，词性标注）</li></ul></li><li>回归问题<ul><li>预测输入变量与输出变量之间的关系，特别是输入变量的值发生变化时，输出变量随之发生的变化。</li></ul></li></ol><h3 id="2-1感知机模型"><a href="#2-1感知机模型" class="headerlink" title="2.1感知机模型"></a>2.1感知机模型</h3><ul><li>感知机是二类分类的线性分类模型，输入为特征向量输出为类别，取+1，-1.将实例划分为正负两类分离超平面属于判别模型。<script type="math/tex; mode=display">f(x)=sign(\omega x+b)</script></li><li>$\omega$叫做权值向量，b叫做偏置<script type="math/tex; mode=display">sign(x)=\left\{\begin{matrix}+1 &x\geqslant 0  \\-1& x<0 \\\end{matrix}\right.</script></li><li>假设空间是特征空间中所有线性分类模型</li><li>训练数据集：实例的特征向量及类别（xi，yi）</li></ul><h3 id="2-2感知机学习策略"><a href="#2-2感知机学习策略" class="headerlink" title="2.2感知机学习策略"></a>2.2感知机学习策略</h3><ol><li>线性可分性：存在某个超平面S：$\omega x+b=0$能够将数据集的正负实例点完全正确划分到超平面的两侧，称数据集为线性可分数据集</li><li>感知机学习策略：目标是求得一个能够将训练集正实例点和负实例点完全正确分开的超平面。确定$\omega$和b，使损失最小化。<ul><li>损失函数：误分类点到超平面的总距离<script type="math/tex; mode=display">L(\omega ,b)=-\sum_{xi\in M}^{} y_{i}(\omega x_{i}+b)</script></li></ul></li></ol><h3 id="2-3感知机学习算法"><a href="#2-3感知机学习算法" class="headerlink" title="2.3感知机学习算法"></a>2.3感知机学习算法</h3><ul><li>随机梯度下降：学习率$\eta$(步长）（0-1），</li></ul><ol><li>任选取超平面w0，b0，</li><li>选取数据（xi，yi）</li><li>若<script type="math/tex">y_{i}(\omega x_{i}+b)\leqslant 0</script><script type="math/tex; mode=display">\omega<=\omega +\eta y_{i}x_{i}</script><script type="math/tex; mode=display">b<=b+\eta y_{i}</script></li><li>跳回（2），直到没有误分类点</li></ol><h3 id="2-4感知器学习算法的对偶形式"><a href="#2-4感知器学习算法的对偶形式" class="headerlink" title="2.4感知器学习算法的对偶形式"></a>2.4感知器学习算法的对偶形式</h3><ul><li>基本想法，将w和b表示为实例xi和标记yi的线性组合的形式，通过解其系数球的w和b。初始w和b为0，经过对误分类点的修改后，最后学习到的w和b可以表示为：(这里$\alpha$i=ni$\eta$)<script type="math/tex; mode=display">\omega=\sum_{i=1}^{N} \alpha _{i}x_{i}y_{i}</script><script type="math/tex; mode=display">b=\sum_{i=1}^{N} \alpha _{i}y_{i}</script><ol><li>a=0,b=0</li><li>在训练集中选取数据（xi，yi）</li><li>如果$y<em>{i}(\sum</em>{j=1}^{N}\alpha <em>{j}y</em>{j}x<em>{j}\cdot x</em>{i}+b)\leqslant 0$ :<script type="math/tex; mode=display">\alpha_{i}<=\alpha _{i}+\eta</script><script type="math/tex; mode=display">b<=b+\eta y_{i}</script></li><li>转到2直到没有误分类数据</li></ol></li></ul><h3 id="3-1k近邻算法"><a href="#3-1k近邻算法" class="headerlink" title="3.1k近邻算法"></a>3.1k近邻算法</h3><ul><li>给定一个训练数据集，对于新的训练实例，在训练数据集中找到与该实例最邻近的k个实例，这k个实例多数属于某个类，就把该输入实例分为这个类。</li></ul><h3 id="3-2k近邻模型"><a href="#3-2k近邻模型" class="headerlink" title="3.2k近邻模型"></a>3.2k近邻模型</h3><ol><li>模型：根据训练集，距离度量，k值，以及决策规则，将特征空间划分为一些子空间。</li><li>距离度量：点xi与xj的Lp距离：<script type="math/tex; mode=display">L_{p}(x_{i},x_{j})=(\sum_{l=1}^{n}\left|x_{i}^{(l)}- x_{j}^{(l)}\right|^{p})^{\frac{1}{p}}</script><ul><li>p=2,欧氏距离</li></ul></li></ol><script type="math/tex; mode=display">L_{2}(x_{i},x_{j})=(\sum_{l=1}^{n}\left|x_{i}^{(l)}- x_{j}^{(l)}\right|^{2})^{\frac{1}{2}}</script><ul><li>p=1,曼哈顿距离<script type="math/tex; mode=display">L_{1}(x_{i},x_{j})=\sum_{l=1}^{n}\left|x_{i}^{(l)}- x_{j}^{(l)}\right|</script></li><li>p=无穷<script type="math/tex; mode=display">L_{\infty }(x_{i},x_{j})=max\left|x_{i}^{(l)}- x_{j}^{(l)}\right|</script></li><li>不同距离度量确定的最近邻点是不同的</li></ul><h4 id="k值的选择"><a href="#k值的选择" class="headerlink" title="k值的选择"></a>k值的选择</h4><ul><li>k比较小，模型复杂容易过拟合</li><li>k大，学习误差小，模型简单</li></ul><h3 id="3-3k近邻的实现：kd树（未理解）"><a href="#3-3k近邻的实现：kd树（未理解）" class="headerlink" title="3.3k近邻的实现：kd树（未理解）"></a>3.3k近邻的实现：kd树（未理解）</h3><ul><li>kd树的构造：<ol><li>开始，构造根节点，由根节点生成深度为1的左右子节点。。。将落在切分超平面上的实例点保存在根节点</li><li>重复：对深度为j的结点选择x(l)为切分的坐标轴，l=j(mod k)+1,以该节点对应的区域中所有实例x(l)坐标的中位数为切分点。切分通过切分点并与坐标轴x(l)垂直的超平面实现。将落在切分超平面上的实例点保存在该节点。</li><li>直到两个子区域没有实例时停止。</li></ol></li><li>kd树搜索：</li></ul><h3 id="4-1朴素贝叶斯"><a href="#4-1朴素贝叶斯" class="headerlink" title="4.1朴素贝叶斯"></a>4.1朴素贝叶斯</h3><ol><li>基本方法：x为N维向量，y为类标记（class label），P（X，Y）为x和y的联合概率分布，训练数据集<script type="math/tex">T=\begin{Bmatrix}(x_{1},y_{1}),(x_{2},y_{2}),...,(x_{n},y_{n})\end{Bmatrix}</script><br>由P（X，Y）独立同分布产生。<ul><li>通过训练数据集学习联合分布概率P（X，Y）。先学习先验概率分布及条件概率分布：<script type="math/tex">P(Y=c_{k}),k=1,2,...,K</script></li><li>由于条件概率分布P（X=x，Y=ck）有指数量级的参数，不宜计算。朴素贝叶斯对条件概率分布作了条件独立性的假设：<script type="math/tex; mode=display">P(X=x|Y=c_{k})=P(X^{(1)}=x^{(1)},...,X^{(n)}=x^{(n)}|Y=c_{k})</script><script type="math/tex; mode=display">=\prod_{j=1}^{n}P(X^{(j)}  =x^{(j)}|Y=c_{k})</script></li><li>由学习到生成数据的机制，属于生成模型。条件独立假设等于说用于分类的特征在类确定条件下都是条件独立的。这一假设使朴素贝叶斯变得简单，有时牺牲准确率。</li><li>后验概率计算根据贝叶斯定理：<script type="math/tex; mode=display">P(Y=c_{k}|X=x)=\frac{P(X=x|Y=c_{k})P(Y=c_{k})}{\sum_{k}^{}P(X=x|Y=c_{k})P(Y=c_{k})}</script>将以上假设带入得到朴素贝叶斯分类基本公式：<script type="math/tex; mode=display">P(Y=c_{k}|X=x)=\frac{P(X=x|Y=c_{k})\prod_{j=1}^{n}P(X^{(j)}  =x^{(j)}|Y=c_{k})}{\sum_{k}^{}\prod_{j=1}^{n}P(X^{(j)}  =x^{(j)}|Y=c_{k})P(Y=c_{k})}</script></li><li>朴素贝叶斯分类器可表示为<script type="math/tex; mode=display">y=f(x)=argmaxP(Y=c_{k}|X=x)</script><script type="math/tex; mode=display">=\prod_{j=1}^{n}P(X^{(j)}  =x^{(j)}|Y=c_{k})</script>2.后验概率最大化的含义：为了使期望风险函数（条件期望）最小化，推导出后验概率最大化。</li></ul></li></ol><h3 id="4-2朴素贝叶斯的参数估计"><a href="#4-2朴素贝叶斯的参数估计" class="headerlink" title="4.2朴素贝叶斯的参数估计"></a>4.2朴素贝叶斯的参数估计</h3><ol><li>极大似然估计：<br><em>极大似然估计，通俗理解来说，就是利用已知的样本结果信息，反推最具有可能（最大概率）导致这些样本结果出现的模型参数值！换句话说，极大似然估计提供了一种给定观察数据来评估模型参数的方法，即：“模型已定，参数未知”。</em></li><li>朴素贝叶斯算法：<ul><li>计算先验概率及条件概率<script type="math/tex; mode=display">P(Y=c_{k})=\frac{\sum_{i=1}^{N}I(y_{i}=c_{k})}{N},k=1,2,...K</script><script type="math/tex; mode=display">P(X^{(j)}=a_{jl}|Y=c_{k})=\frac{\sum_{i=1}^{N}I(x_{i}^{(j)}=a_{jl},y_{i}=c_{k})}{\sum_{i=1}^{N}I(y_{i}=c_{k})}</script>j=1,2,…n;l=1,2,…Sj;k=1,2…K</li><li>对于给定的实例<script type="math/tex">x=(x^{(1)},x^{(2)},...x^{(n)})</script><script type="math/tex; mode=display">P(Y=c_{k})\prod_{j=1}^{n}P(X^{(j)}=x^{(j)}|Y=c_{k})</script></li><li>通过计算上式在$P(Y=c_{k})$为何值时取得最大，得出实例x的类。</li></ul></li><li>贝叶斯估计：用极大似然估计可能会出现所求概率值为0的情况。会影响后验概率的计算结果，使分类产生偏差。<script type="math/tex">P(X^{(j)}=a_{jl}|Y=c_{k})=\frac{\sum_{i=1}^{N}I(x_{i}^{(j)}=a_{jl},y_{i}=c_{k})+\lambda }{\sum_{i=1}^{N}I(y_{i}=c_{k})+\lambda S_{j}}</script>在随机变量各个取值的频数上赋予一个正数$\lambda$,常取1.成为拉普拉斯平滑。Sj为x所有可能总数量。同样，先验概率的贝叶斯估计是：<script type="math/tex">P(Y=c_{k})=\frac{\sum_{i=1}^{N}I(y_{i}=c_{k})+\lambda }{N+K\lambda },k=1,2,...K</script>K为所有y可能取值的总数量</li></ol><h3 id="5支持向量机"><a href="#5支持向量机" class="headerlink" title="5支持向量机"></a>5支持向量机</h3><p><em>支持向量机是一种二类分类模型。它的基本模型是定义在特征空间上的间隔最大的线性分类器，间隔最大使它有别于感知机。支持向量机的学习策略是间隔最大化，可形式化为一个求解凸二次规划的问题，等价于正则化的合页损失函数的最小化问题。</em></p><h3 id="5-1线性可分支持向量机与硬间隔最大化"><a href="#5-1线性可分支持向量机与硬间隔最大化" class="headerlink" title="5.1线性可分支持向量机与硬间隔最大化"></a>5.1线性可分支持向量机与硬间隔最大化</h3><ol><li>假设输入空间与特征空间为两个不同的空间。线性可分支持向量机假设这两个空间的元素一一对应，将输入空间中的输入映射为特征空间中的特征向量。假定给定一个特征空间上的训练数据集：<script type="math/tex">T=\begin{Bmatrix}(x_{1},y_{1}),(x_{2},y_{2}),...,(x_{n},y_{n})\end{Bmatrix}</script><br>其中：<script type="math/tex">x_{i}\in \chi =R^{n},y_{i}\in \gamma =\begin{Bmatrix}+1,-1\end{Bmatrix},i=1,2,...N</script><br>yi=1,称xi为正例；yi=-1，称xi为负例。假设训练数据是线性可分的，学习的目标是在特征空间中找到一个分离超平面，能将实例分到不同的类。分离超平面对应方程wx+b=0，由法向量w和截距决定。法向量指向的一侧为正类，一侧为负类。<br>当训练数据集线性可分时，由于存在无穷个分离超平面。感知机利用误分类最小的策略，，解有无穷多个；线性可分支持向量机利用间隔最大化求解最优分离超平面，这时，解是唯一的。<br>超平面：<script type="math/tex">\omega^{*} x+b^{*}=0</script><br>相应的分类决策函数：<script type="math/tex; mode=display">f(x)=sign(\omega^{*} x+b^{*})</script></li><li><p>函数间隔和几何间隔<br>一般来说，一个点距离超平面的远近可以表示分类预测的确信程度。在超平面$\omega x+b=0$<br>确定的情况下，用$\left| \omega x+b\right|$来相对表示x距离超平面的远近。$\omega x+b=0$的符号与标记y的符号是否一致来表示分类是否正确。<br>对于某一样本点的函数间隔：</p><script type="math/tex; mode=display">\widehat{\gamma _{i}}=y_{i}(\omega x_{i}+b)</script><ul><li>定义：超平面 （w，b）关于训练集T的函数间隔为超平面（w，b）关于T中所有样本点（xi，yi）的函数间隔的最小值：<script type="math/tex; mode=display">\widehat{\gamma }=min\widehat{\gamma _{i}}</script>函数间隔可以表示分类预测的正确性和确信度。但如果等比例改变w和b，超平面没有改变，但是函数间隔变化。因此，我们要对分离超平面的法向量w加以约束，如规范化||w||=1，使得间隔是确定的，这时间隔为几何间隔：<script type="math/tex; mode=display">\widehat{\gamma _{i}}=y_{i}(\frac{\omega}{\begin{Vmatrix}\omega \end{Vmatrix}} x_{i}+\frac{b}{\begin{Vmatrix}\omega \end{Vmatrix}})</script>同理，定义超平面（w，b）关于训练数据集T的几何间隔为各样本几何间隔最小值。<script type="math/tex; mode=display">\widehat{\gamma }=min\widehat{\gamma _{i}}</script></li></ul></li><li>硬间隔最大化：<ul><li>最大间隔分离超平面：<script type="math/tex; mode=display">\underset{\omega ,b}{min} \frac{1}{2}\begin{Vmatrix}\omega \end{Vmatrix}^{2}</script><script type="math/tex; mode=display">s.t.\ \  y_{i}(\omega x_{i}+b)-1\geqslant 0,i=1,2,...N</script>凸二次规划问题</li></ul></li><li>支持向量与间隔边界：在线性可分发情况下，训练样本数据集中的样本点中与分离超平面距离最近的样本点的实例成为支持向量.支持向量是使下式成立的点。<script type="math/tex">y_{i}(\omega x_{i}+b)-1= 0</script>对yi=+1的正例点，支持向量在超平面<script type="math/tex">H1:\omega x+b=1</script>对于yi=-1的负例点，支持向量在超平面<script type="math/tex">H2:\omega x+b=-1</script>在H1和H2上的点就是支持向量。</li></ol><h3 id="5-2软间隔最大化"><a href="#5-2软间隔最大化" class="headerlink" title="5.2软间隔最大化"></a>5.2软间隔最大化</h3><h3 id="6神经网络"><a href="#6神经网络" class="headerlink" title="6神经网络"></a>6神经网络</h3><ul><li>简介（以下由chatgpt生成）<br>神经网络算法是一类基于生物神经系统思维方式和人工智能技术的计算模型，被广泛应用于机器学习、图像识别、自然语言处理、语音识别、控制系统等领域。常用的神经网络算法包括以下几种：</li></ul><ol><li><p>前馈神经网络（Feedforward Neural Networks）：是最基本、最常用的神经网络算法，其特点是信息传递是单向的，不具有反馈和循环。可以通过调节网络的层数、神经元数量和权重等参数进行训练和优化。</p></li><li><p>卷积神经网络（Convolutional Neural Networks）：主要用于图像识别和计算机视觉等领域，其结构由多层卷积层、池化层和全连接层构成，可以有效地提取图像特征。</p></li><li><p>循环神经网络（Recurrent Neural Networks）：具有时间依赖性，可以处理序列数据、自然语言处理、语音识别等任务。其具有循环结构，可以记住过去的状态信息，并且通过反馈机制将当前状态与前面的状态相连。</p></li><li><p>长短期记忆网络（Long Short-Term Memory Networks）：是一种特殊的循环神经网络，能够有效地解决长时间依赖问题，主要应用于自然语言处理、语音识别、机器翻译等领域。</p></li><li><p>自编码器（Autoencoders）：是一种无监督学习方法，可以用于数据压缩、特征提取、图像去噪等任务，其原理是通过学习数据的低维表示来重构输入数据。</p></li></ol><p>神经网络算法的优点是能够进行大规模并行计算、处理非线性问题、具有自适应性和学习能力等，但也存在训练难度大、容易陷入局部最优解等问题。<br>当人们谈论神经网络时，通常指的是深度神经网络（Deep Neural Networks，DNNs），它们是一种人工神经网络（Artificial Neural Networks，ANNs）的变种。在深度学习中，神经网络是最流行的算法之一，用于解决各种任务，包括图像识别、自然语言处理和语音识别。</p><p>神经网络算法的基本原理是通过学习数据的规律来构建一个函数，将输入映射到输出。这个函数由神经网络的各个层组成，每个层都由多个神经元（或称为节点）组成。每个神经元接收到来自前一层的输入，并通过一些权重和偏置来计算输出。</p><p>神经网络的训练过程通常使用反向传播算法（Backpropagation）来优化神经元的权重和偏置。这个算法通过计算损失函数的梯度来更新神经元的权重和偏置，以使模型的预测更加准确。损失函数通常是一个衡量模型预测值与真实值之间差距的函数。</p><p>底层逻辑方面，神经网络是一种前馈网络，即数据从输入层开始流向输出层，中间没有反馈或循环。神经网络中每个神经元的输出是通过对它们的输入进行一系列数学运算来计算得出的。这些运算通常包括对输入进行加权求和，然后使用激活函数来转换成神经元的输出。</p><p>在深度神经网络中，通常有多个隐藏层，每个隐藏层都有多个神经元。每个隐藏层可以看作是对输入的一种抽象表示，它将输入中的一些特征提取出来并传递给下一层，最终得到输出。通过增加隐藏层和神经元的数量，神经网络可以学习更加复杂的模式和规律。</p><h3 id="7最速下降法"><a href="#7最速下降法" class="headerlink" title="7最速下降法"></a>7最速下降法</h3><ul><li>输入：目标函数f（x），梯度函数<script type="math/tex">g(x)=\triangledown f(x)</script>,精度$\varepsilon$.</li><li>输出：f（x）的极小值点x*</li></ul><ol><li>取初始值$x^{(0)}$,置k=0</li><li>计算$f(x^{(k)})$</li><li>计算梯度$g<em>{k}=g(x^{(k)})$,当$\begin{Vmatrix}g</em>{k}\end{Vmatrix}&lt;\varepsilon$时停止迭代，令x^{*}=x^{(k)}，否则令$p<em>{k}=-g(x^{(k)})$,求$\lambda</em>{k}$,,使：<script type="math/tex; mode=display">f(x^{(k)}+\lambda _{k}p_{k})=\underset{\lambda \geqslant 0}{min}f(x^{(k)}+\lambda p_{k})</script></li><li>置$x^{(k+1)}=x^{(k)}+\lambda <em>{k}p</em>{k}$,计算 $f(x^{(k+1)})$<br>当 $\begin{Vmatrix}f(x^{(k+1)}-f(x)^{(k)})\end{Vmatrix}&lt;\varepsilon$ 或： $\begin{Vmatrix}x^{(k+1)}-x^{(k)}\end{Vmatrix}&lt;\varepsilon$ 时，停止迭代，令 $x^{*}=x^{(k+1)}$</li><li>否则，令k=k+1，转到3.<br>计算量小，存储量小，必收敛<br>局部最优，收敛速度慢，大多数条件下线性收敛</li></ol><h3 id="8牛顿法"><a href="#8牛顿法" class="headerlink" title="8牛顿法"></a>8牛顿法</h3><ul><li>优点：收敛速度快，大多数情况下超线性收敛</li><li>缺点：二阶梯度正定时才下降，计算量大，当牛顿法无法产生下降方向时，用最速下降法代替</li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;统计学习方法&quot;&gt;&lt;a href=&quot;#统计学习方法&quot; class=&quot;headerlink&quot; title=&quot;统计学习方法&quot;&gt;&lt;/a&gt;统计学习方法&lt;/h1&gt;&lt;p&gt;&lt;em&gt;赫尔伯特.西蒙：”如果一个系统能够通过执行某个过程改进它的性能，这就是学习。“&lt;/em&gt;&lt;/p&gt;
&lt;</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title>已删除的笔记</title>
    <link href="http://wwffyy.life/posts/d63b390f.html"/>
    <id>http://wwffyy.life/posts/d63b390f.html</id>
    <published>2023-03-06T08:18:32.000Z</published>
    <updated>2024-03-29T00:57:56.190Z</updated>
    
    
    
    
    
  </entry>
  
  <entry>
    <title>Hello World</title>
    <link href="http://wwffyy.life/posts/4a17b156.html"/>
    <id>http://wwffyy.life/posts/4a17b156.html</id>
    <published>2023-03-01T01:21:09.565Z</published>
    <updated>2024-01-07T04:13:25.810Z</updated>
    
    <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;Welcome to &lt;a href=&quot;https://hexo.io/&quot;&gt;Hexo&lt;/a&gt;! This is your very first post. Check &lt;a href=&quot;https://hexo.io/docs/&quot;&gt;documentation&lt;/a&gt; for</summary>
      
    
    
    
    
  </entry>
  
</feed>
