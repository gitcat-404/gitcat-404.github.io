<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>启蛰海</title>
  
  <subtitle>挽弦暮笙</subtitle>
  <link href="http://wwffyy.life/atom.xml" rel="self"/>
  
  <link href="http://wwffyy.life/"/>
  <updated>2024-04-10T07:20:38.409Z</updated>
  <id>http://wwffyy.life/</id>
  
  <author>
    <name>whisper</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>李沐深度学习部分笔记及练习</title>
    <link href="http://wwffyy.life/posts/26875fc2.html"/>
    <id>http://wwffyy.life/posts/26875fc2.html</id>
    <published>2024-04-10T06:54:20.286Z</published>
    <updated>2024-04-10T07:20:38.409Z</updated>
    
    <content type="html"><![CDATA[<h1 id="预备知识"><a href="#预备知识" class="headerlink" title="预备知识"></a>预备知识</h1><h2 id="数据操作"><a href="#数据操作" class="headerlink" title="数据操作"></a>数据操作</h2><h3 id="广播机制"><a href="#广播机制" class="headerlink" title="广播机制"></a>广播机制</h3><p>对形状不同的张量进行相加操作，规则为a矩阵复制列，b矩阵复制行，将元素相加<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">a = torch.arange(<span class="number">3</span>).reshape((<span class="number">3</span>, <span class="number">1</span>))</span><br><span class="line">b = torch.arange(<span class="number">2</span>).reshape((<span class="number">1</span>, <span class="number">2</span>))</span><br><span class="line">a, b</span><br><span class="line">a + b</span><br></pre></td></tr></table></figure><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">(tensor([[<span class="number">0</span>],</span><br><span class="line">         [<span class="number">1</span>],</span><br><span class="line">         [<span class="number">2</span>]]),</span><br><span class="line"> tensor([[<span class="number">0</span>, <span class="number">1</span>]]))</span><br><span class="line">tensor([[<span class="number">0</span>, <span class="number">1</span>],</span><br><span class="line">        [<span class="number">1</span>, <span class="number">2</span>],</span><br><span class="line">        [<span class="number">2</span>, <span class="number">3</span>]])</span><br></pre></td></tr></table></figure></p><h3 id="节省内存"><a href="#节省内存" class="headerlink" title="节省内存"></a>节省内存</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Z = torch.zeros_like(Y)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;id(Z):&#x27;</span>, <span class="built_in">id</span>(Z))</span><br><span class="line">Z[:] = X + Y</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;id(Z):&#x27;</span>, <span class="built_in">id</span>(Z))</span><br></pre></td></tr></table></figure><p>发现Z的id未变化，减少了内存开销</p><h3 id="转换numpy对象"><a href="#转换numpy对象" class="headerlink" title="转换numpy对象"></a>转换numpy对象</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">A = X.numpy()</span><br><span class="line">B = torch.tensor(A)</span><br><span class="line"><span class="built_in">type</span>(A), <span class="built_in">type</span>(B)</span><br></pre></td></tr></table></figure><p>将大小为1的张量转换为python标量，调用item函数，或者float，int等函数进行类型转换<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">a = torch.tensor([<span class="number">3.5</span>])</span><br><span class="line">a, a.item(), <span class="built_in">float</span>(a), <span class="built_in">int</span>(a)</span><br><span class="line">(tensor([<span class="number">3.5000</span>]), <span class="number">3.5</span>, <span class="number">3.5</span>, <span class="number">3</span>)</span><br></pre></td></tr></table></figure></p><h3 id="练习"><a href="#练习" class="headerlink" title="练习"></a>练习</h3><ol><li>运行本节中的代码。将本节中的条件语句<code>X == Y</code>更改为<code>X &lt; Y</code>或<code>X &gt; Y</code>，然后看看你可以得到什么样的张量。<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tensor([[<span class="literal">False</span>, <span class="literal">False</span>, <span class="literal">False</span>, <span class="literal">False</span>], [ <span class="literal">True</span>, <span class="literal">True</span>, <span class="literal">True</span>, <span class="literal">True</span>], [ <span class="literal">True</span>, <span class="literal">True</span>, <span class="literal">True</span>, <span class="literal">True</span>]])</span><br></pre></td></tr></table></figure>会发现同样得到了与原tensor大小相同的逻辑值</li><li>用其他形状（例如三维张量）替换广播机制中按元素操作的两个张量。结果是否与预期相同？<br>如果两个张量在某个维度上的大小不同，其中一个的大小必须是1，这样它就可以在该维度上进行扩展。<br>如果两个张量在某个维度上的大小都是1，或者其中一个张量在该维度上不存在（即它的大小在该维度上为1），则在该维度上的大小将被设置为较大的那个大小。<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">a = torch.tensor([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>])</span><br><span class="line">b = torch.tensor([<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>])</span><br><span class="line">result = a + b</span><br><span class="line"><span class="built_in">print</span>(result)</span><br><span class="line">a_3d = a.unsqueeze(<span class="number">0</span>).unsqueeze(-<span class="number">1</span>)</span><br><span class="line">b_3d = b.unsqueeze(<span class="number">0</span>).unsqueeze(<span class="number">0</span>) </span><br><span class="line">result_3d = a_3d + b_3d</span><br><span class="line"><span class="built_in">print</span>(result_3d)</span><br><span class="line"></span><br><span class="line">tensor([<span class="number">5</span>, <span class="number">7</span>, <span class="number">9</span>]) tensor([[[<span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>], [<span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>], [<span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>]]])</span><br></pre></td></tr></table></figure><h2 id="数据预处理"><a href="#数据预处理" class="headerlink" title="数据预处理"></a>数据预处理</h2><h3 id="数据集读取"><a href="#数据集读取" class="headerlink" title="数据集读取"></a>数据集读取</h3>基本操作：创建文件、路径组合、文件写入<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line">os.makedirs(os.path.join(<span class="string">&#x27;..&#x27;</span>, <span class="string">&#x27;data&#x27;</span>), exist_ok=<span class="literal">True</span>)</span><br><span class="line">data_file = os.path.join(<span class="string">&#x27;..&#x27;</span>, <span class="string">&#x27;data&#x27;</span>, <span class="string">&#x27;house_tiny.csv&#x27;</span>)</span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(data_file, <span class="string">&#x27;w&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">    f.write(<span class="string">&#x27;NumRooms,Alley,Price\n&#x27;</span>)  <span class="comment"># 列名</span></span><br></pre></td></tr></table></figure>读取csv方法<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line">data = pd.read_csv(data_file)</span><br></pre></td></tr></table></figure><h3 id="缺失值处理"><a href="#缺失值处理" class="headerlink" title="缺失值处理"></a>缺失值处理</h3>对于一组数据：<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">   NumRooms Alley   Price</span><br><span class="line"><span class="number">0</span>       NaN  Pave  <span class="number">127500</span></span><br><span class="line"><span class="number">1</span>       <span class="number">2.0</span>   NaN  <span class="number">106000</span></span><br><span class="line"><span class="number">2</span>       <span class="number">4.0</span>   NaN  <span class="number">178100</span></span><br><span class="line"><span class="number">3</span>       NaN   NaN  <span class="number">140000</span></span><br></pre></td></tr></table></figure>均值插值法<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">inputs = inputs.fillna(inputs.mean())</span><br></pre></td></tr></table></figure>独热编码<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">inputs = pd.get_dummies(inputs, dummy_na=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure><h3 id="练习-1"><a href="#练习-1" class="headerlink" title="练习"></a>练习</h3>创建包含更多行和列的原始数据集。</li><li>删除缺失值最多的列。</li><li>将预处理后的数据集转换为张量格式。<br>首先创建数据<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">data = &#123; <span class="string">&#x27;NumRooms&#x27;</span>: [np.nan, <span class="number">2.0</span>, <span class="number">4.0</span>, np.nan, <span class="number">3.0</span>], <span class="string">&#x27;Alley&#x27;</span>: [<span class="string">&#x27;Pave&#x27;</span>, np.nan, np.nan, np.nan, <span class="string">&#x27;Grvl&#x27;</span>], <span class="string">&#x27;Price&#x27;</span>: [<span class="number">127500</span>, <span class="number">106000</span>, <span class="number">178100</span>, <span class="number">140000</span>, <span class="number">165000</span>], <span class="string">&#x27;Garage&#x27;</span>: [<span class="number">1</span>, <span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">2</span>], <span class="string">&#x27;YearBuilt&#x27;</span>: [<span class="number">2000</span>, <span class="number">1995</span>, <span class="number">2005</span>, <span class="number">2010</span>, <span class="number">2003</span>] &#125;</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">df.dropna(axis=<span class="number">1</span>, inplace=<span class="literal">True</span>) </span><br><span class="line">imputer = SimpleImputer(strategy=<span class="string">&#x27;mean&#x27;</span>) </span><br><span class="line">df_imputed = pd.DataFrame(imputer.fit_transform(df), columns=df.columns) </span><br><span class="line">tensor_data = torch.tensor(df_imputed.values) <span class="comment">#转为tensor</span></span><br></pre></td></tr></table></figure><h2 id="线性代数"><a href="#线性代数" class="headerlink" title="线性代数"></a>线性代数</h2></li></ol><h3 id="矩阵的转置"><a href="#矩阵的转置" class="headerlink" title="矩阵的转置"></a>矩阵的转置</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">A.T</span><br></pre></td></tr></table></figure><h3 id="张量基本算法"><a href="#张量基本算法" class="headerlink" title="张量基本算法"></a>张量基本算法</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">A = torch.arange(<span class="number">20</span>, dtype=torch.float32).reshape(<span class="number">5</span>, <span class="number">4</span>)</span><br><span class="line">B = A.clone()  <span class="comment"># 通过分配新内存，将A的一个副本分配给B</span></span><br><span class="line">A, A + B</span><br></pre></td></tr></table></figure><p>.clone()类似于深拷贝，B的反向传播不影响A。<br><em>Hadamard积</em>：数学符号⊙，指两个相同形状的矩阵（或向量）对应位置上元素相乘的结果。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">A * B</span><br></pre></td></tr></table></figure></p><h3 id="降维"><a href="#降维" class="headerlink" title="降维"></a>降维</h3><p>默认情况下，调用求和函数会沿所有的轴降低张量的维度，使它变为一个标量。 我们还可以指定张量沿哪一个轴来通过求和降低维度。 以矩阵为例，为了通过求和所有行的元素来降维（轴0），可以在调用函数时指定<code>axis=0</code>。 由于输入矩阵沿0轴降维以生成输出向量，因此输入轴0的维数在输出形状中消失。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">A_sum_axis0 = A.<span class="built_in">sum</span>(axis=<span class="number">0</span>)</span><br><span class="line">A_sum_axis0, A_sum_axis0.shape</span><br><span class="line"></span><br><span class="line">(tensor([<span class="number">40.</span>, <span class="number">45.</span>, <span class="number">50.</span>, <span class="number">55.</span>]), torch.Size([<span class="number">4</span>]))</span><br><span class="line"></span><br><span class="line">A_sum_axis1 = A.<span class="built_in">sum</span>(axis=<span class="number">1</span>)</span><br><span class="line">A_sum_axis1, A_sum_axis1.shape</span><br><span class="line"></span><br><span class="line">(tensor([ <span class="number">6.</span>, <span class="number">22.</span>, <span class="number">38.</span>, <span class="number">54.</span>, <span class="number">70.</span>]), torch.Size([<span class="number">5</span>]))</span><br></pre></td></tr></table></figure><br>沿着行和列对矩阵求和，等价于对矩阵的所有元素进行求和。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">A.<span class="built_in">sum</span>(axis=[<span class="number">0</span>, <span class="number">1</span>])  <span class="comment"># 结果和A.sum()相同</span></span><br></pre></td></tr></table></figure></p><h3 id="非降维求和"><a href="#非降维求和" class="headerlink" title="非降维求和"></a>非降维求和</h3><p>如果我们想沿某个轴计算<code>A</code>元素的累积总和， 比如<code>axis=0</code>（按行计算），可以调用<code>cumsum</code>函数。 此函数不会沿任何轴降低输入张量的维度。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">A.cumsum(axis=<span class="number">0</span>)</span><br></pre></td></tr></table></figure></p><h3 id="点积"><a href="#点积" class="headerlink" title="点积"></a>点积</h3><ul><li>点积也称为内积或数量积，是两个向量之间的运算。</li><li>对于两个长度相同的向量a和b，它们的点积为a·b = a₁b₁ + a₂b₂ + … + aₙbₙ，其中aᵢ和bᵢ分别表示向量a和b的第i个元素。</li><li>点积的结果是一个标量，表示了两个向量在同一方向上的投影的乘积。<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.dot(x, y)</span><br></pre></td></tr></table></figure><h3 id="矩阵乘法"><a href="#矩阵乘法" class="headerlink" title="矩阵乘法"></a>矩阵乘法</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.mm(A, B)</span><br></pre></td></tr></table></figure><h3 id="范数"><a href="#范数" class="headerlink" title="范数"></a>范数</h3>范数（Norm）是一个函数，通常用来衡量向量的大小或长度。范数的定义通常满足以下性质：</li></ul><ol><li><strong>非负性（Non-negativity）</strong>：对于任意向量x，其范数必须为非负数，即∥x∥ ≥ 0，且当且仅当x=0时，范数为0。</li><li><strong>齐次性（Homogeneity）</strong>：对于任意标量α，向量x的范数乘以α等于向量αx的范数，即∥αx∥ = |α| ∥x∥。</li><li><strong>三角不等式（Triangle Inequality）</strong>：对于任意两个向量x和y，有∥x+y∥ ≤ ∥x∥ + ∥y∥。<br>常见向量范数包括：</li><li><strong>L1范数</strong>：向量中所有元素的绝对值之和，表示为∥x∥₁ = |x₁| + |x₂| + … + |xₙ|。</li><li><strong>L2范数</strong>：向量中所有元素的平方和的平方根，表示为∥x∥₂ = √(x₁² + x₂² + … + xₙ²)。</li><li><strong>L∞范数</strong>：向量中所有元素的绝对值的最大值，表示为∥x∥₊ = max(|x₁|, |x₂|, …, |xₙ|)。<br>范数在机器学习和优化问题中经常被用来作为正则化项，添加到损失函数中，帮助控制模型的复杂度并避免过拟合。</li></ol><h3 id="练习-2"><a href="#练习-2" class="headerlink" title="练习"></a>练习</h3><ol><li>对于任意方阵A，A+AT是对称的</li><li>对于形状为(2, 3, 4)的张量X，len(X)的输出结果是2、</li><li>不是。在Python中，len()函数用于获取对象的长度或大小，对于张量来说，len(X)返回的是张量的第一个维度的大小，而不是张量特定轴的长度。、</li><li>考虑一个具有形状(3, 4, 5)的张量，对它在轴0、1、2上求和，输出的形状为(1, 1, 1)，即一个标量值。<h2 id="微积分"><a href="#微积分" class="headerlink" title="微积分"></a>微积分</h2><h3 id="练习-3"><a href="#练习-3" class="headerlink" title="练习"></a>练习</h3></li><li>如果有函数u=f(x,y,z)，其中 x=x(a,b)，y=y(a,b)，z=z(a,b)，根据链式法则，u 对 a 的导数可以表示为：<script type="math/tex; mode=display">\frac{du}{da}=\frac{∂u}{∂x}\frac{dx}{da}+\frac{∂u}{∂y}\frac{dy}{da}+\frac{∂u}{∂z}\frac{dz}{da}</script><h2 id="自动微分"><a href="#自动微分" class="headerlink" title="自动微分"></a>自动微分</h2>深度学习框架通过自动计算导数，即<em>自动微分</em>（automatic differentiation）来加快求导。 实际中，根据设计好的模型，系统会构建一个<em>计算图</em>（computational graph）， 来跟踪计算是哪些数据通过哪些操作组合起来产生输出。 自动微分使系统能够随后反向传播梯度。 这里，<em>反向传播</em>（backpropagate）意味着跟踪整个计算图，填充关于每个参数的偏导数。<br>在我们计算y关于x的梯度之前，需要一个地方来存储梯度。 重要的是，我们不会在每次对一个参数求导时都分配新的内存。 因为我们经常会成千上万次地更新相同的参数，每次都分配新的内存可能很快就会将内存耗尽。 注意，一个标量函数关于向量x的梯度是向量，并且与x具有相同的形状。<br>举例说明：y=2x*x的反向传播<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">x = np.arange(<span class="number">4.0</span>)</span><br><span class="line"><span class="comment"># 通过调用attach_grad来为一个张量的梯度分配内存</span></span><br><span class="line">x <span class="comment"># array([0., 1., 2., 3.])</span></span><br><span class="line">x.requires_grad_(<span class="literal">True</span>)  <span class="comment"># 等价于x=torch.arange(4.0,requires_grad=True)</span></span><br><span class="line">x.grad  <span class="comment"># 默认值是None</span></span><br><span class="line">y = <span class="number">2</span> * np.dot(x, x)</span><br><span class="line">y <span class="comment"># array(28.)</span></span><br><span class="line">y.backward()</span><br><span class="line">x.grad <span class="comment"># tensor([ 0.,  4.,  8., 12.]) x.grad == 4 * x</span></span><br></pre></td></tr></table></figure><h3 id="非标量变量的反向传播"><a href="#非标量变量的反向传播" class="headerlink" title="非标量变量的反向传播"></a>非标量变量的反向传播</h3>当<code>y</code>不是标量时，向量<code>y</code>关于向量<code>x</code>的导数的最自然解释是一个矩阵。 对于高阶和高维的<code>y</code>和<code>x</code>，求导的结果可以是一个高阶张量。</li></ol><p>然而，虽然这些更奇特的对象确实出现在高级机器学习中（包括深度学习中）， 但当调用向量的反向计算时，我们通常会试图计算一批训练样本中每个组成部分的损失函数的导数。 这里，我们的目的不是计算微分矩阵，而是单独计算批量中每个样本的偏导数之和。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 对非标量调用backward需要传入一个gradient参数，该参数指定微分函数关于self的梯度。</span></span><br><span class="line"><span class="comment"># 本例只想求偏导数的和，所以传递一个1的梯度是合适的</span></span><br><span class="line">x.grad.zero_()</span><br><span class="line">y = x * x</span><br><span class="line"><span class="comment"># 等价于y.backward(torch.ones(len(x)))</span></span><br><span class="line">y.<span class="built_in">sum</span>().backward()</span><br><span class="line">x.grad <span class="comment"># tensor([0., 2., 4., 6.])</span></span><br></pre></td></tr></table></figure></p><h3 id="分离计算"><a href="#分离计算" class="headerlink" title="分离计算"></a>分离计算</h3><p>有时，我们希望将某些计算移动到记录的计算图之外。 例如，假设<code>y</code>是作为<code>x</code>的函数计算的，而<code>z</code>则是作为<code>y</code>和<code>x</code>的函数计算的。 想象一下，我们想计算<code>z</code>关于<code>x</code>的梯度，但由于某种原因，希望将<code>y</code>视为一个常数， 并且只考虑到<code>x</code>在<code>y</code>被计算后发挥的作用。</p><p>这里可以分离<code>y</code>来返回一个新变量<code>u</code>，该变量与<code>y</code>具有相同的值， 但丢弃计算图中如何计算<code>y</code>的任何信息。 换句话说，梯度不会向后流经<code>u</code>到<code>x</code>。 因此，下面的反向传播函数计算<code>z=u*x</code>关于<code>x</code>的偏导数，同时将<code>u</code>作为常数处理， 而不是<code>z=x*x*x</code>关于<code>x</code>的偏导数。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">x.grad.zero_()</span><br><span class="line">y = x * x</span><br><span class="line">u = y.detach()</span><br><span class="line">z = u * x</span><br><span class="line"></span><br><span class="line">z.<span class="built_in">sum</span>().backward()</span><br><span class="line">x.grad == u <span class="comment"># tensor([True, True, True, True])</span></span><br></pre></td></tr></table></figure><br>由于记录了<code>y</code>的计算结果，我们可以随后在<code>y</code>上调用反向传播， 得到<code>y=x*x</code>关于的<code>x</code>的导数，即<code>2*x</code>。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">x.grad.zero_()</span><br><span class="line">y.<span class="built_in">sum</span>().backward()</span><br><span class="line">x.grad == <span class="number">2</span> * x <span class="comment"># tensor([True, True, True, True])</span></span><br></pre></td></tr></table></figure></p><h3 id="练习-4"><a href="#练习-4" class="headerlink" title="练习"></a>练习</h3><ol><li>为什么计算二阶导数比一阶导数的开销要更大？<br>计算二阶导数需要首先计算一阶导数，然后再对一阶导数进行求导；计算二阶导数需要应用链式法则多次，涉及到更多的函数组合和嵌套。</li><li>在运行反向传播函数之后，立即再次运行它，看看会发生什么？<br>会报错：RuntimeError: Trying to backward through the graph a second time</li><li>在控制流的例子中，我们计算<code>d</code>关于<code>a</code>的导数，如果将变量<code>a</code>更改为随机向量或矩阵，会发生什么？<br>如果<code>a</code>是一个随机向量或矩阵，那么计算其导数的过程会考虑到<code>a</code>的每个元素，并计算相应的雅可比矩阵。（雅可比矩阵是一个将一个向量值函数的梯度向量（或梯度向量的转置）表示为每个自变量的偏导数的矩阵）</li></ol><h2 id="查阅文档"><a href="#查阅文档" class="headerlink" title="查阅文档"></a>查阅文档</h2><h3 id="查找模块中的所有函数和类"><a href="#查找模块中的所有函数和类" class="headerlink" title="查找模块中的所有函数和类"></a>查找模块中的所有函数和类</h3><p>了知道模块中可以调用哪些函数和类，可以调用<code>dir</code>函数。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">dir</span>(torch.distributions)) </span><br><span class="line"><span class="comment"># [&#x27;AbsTransform&#x27;, &#x27;AffineTransform&#x27;, &#x27;Bernoulli&#x27;, &#x27;Beta&#x27;, &#x27;Binomial&#x27;, &#x27;CatTransform&#x27;, &#x27;Categorical&#x27;, &#x27;Cauchy&#x27;, &#x27;Chi2&#x27;, &#x27;ComposeTransform&#x27;, &#x27;ContinuousBernoulli&#x27;, &#x27;CorrCholeskyTransform&#x27;, ....</span></span><br></pre></td></tr></table></figure><br>通常可以忽略以“<code>__</code>”（双下划线）开始和结束的函数，它们是Python中的特殊对象， 或以单个“<code>_</code>”（单下划线）开始的函数，它们通常是内部函数。</p><h3 id="查找特定函数和类的用法"><a href="#查找特定函数和类的用法" class="headerlink" title="查找特定函数和类的用法"></a>查找特定函数和类的用法</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">help</span>(torch.ones)</span><br></pre></td></tr></table></figure><h1 id="线性神经网络"><a href="#线性神经网络" class="headerlink" title="线性神经网络"></a>线性神经网络</h1><h2 id="线性回归"><a href="#线性回归" class="headerlink" title="线性回归"></a>线性回归</h2><p><em>回归</em>（regression）是能为一个或多个自变量与因变量之间关系建模的一类方法。 在自然科学和社会科学领域，回归经常用来表示输入和输出之间的关系。</p><h3 id="线性模型"><a href="#线性模型" class="headerlink" title="线性模型"></a>线性模型</h3><script type="math/tex; mode=display">\hat{y}=w^Tx+b</script><p>权重与偏置</p><h3 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h3><p><em>损失函数</em>（loss function）能够量化目标的实际值与预测值之间的差距。 通常我们会选择非负数作为损失，且数值越小表示损失越小，完美预测时的损失为0。 回归问题中最常用的损失函数是平方误差函数。</p><h2 id="从零实现线性回归"><a href="#从零实现线性回归" class="headerlink" title="从零实现线性回归"></a>从零实现线性回归</h2><h3 id="练习-5"><a href="#练习-5" class="headerlink" title="练习"></a>练习</h3><ol><li>如果我们将权重初始化为零，会发生什么。算法仍然有效吗？<br>可能会影响网络训练速度，收敛性和最终性能。</li></ol><ul><li><strong>随机初始化</strong>：将权重初始化为随机值是一种常用的做法。通过随机初始化，可以避免权重对称性，从而使每个神经元学习到不同的特征。常见的做法是从某个分布（如均匀分布或正态分布）中随机采样权重值。</li><li><strong>全0初始化</strong>：将权重初始化为全0是一种简单的初始化方法。然而，如果所有权重都初始化为0，那么每个神经元在前向传播时计算的值将是相同的，这会导致每个神经元学习相同的特征。这种情况下，网络无法有效地学习复杂的特征，训练过程可能会出现问题。</li></ul><ol><li>假设试图为电压和电流的关系建立一个模型。自动微分可以用来学习模型的参数吗?<br>V=R×I<br>R 是电阻。我们希望通过给定的电压和电流的数据样本，学习到电阻 R 的值。<br>我们可以将电阻 R 视为模型的参数，然后定义一个损失函数来衡量模型预测的电压与实际观测值之间的差异。</li><li>能基于<a href="https://en.wikipedia.org/wiki/Planck%27s_law">普朗克定律</a>使用光谱能量密度来确定物体的温度吗？可以</li><li>计算二阶导数时可能会遇到什么问题？这些问题可以如何解决？<br>计算量大，计算复杂度高，由于计算机表示的精读限制，可能出现数值不稳定问题<br>符号计算：对于简单的函数和表达式，可以使用符号计算来精确地计算二阶导数，避免数值计算的误差。在进行数值计算时，可以采用数值稳定的算法和技巧，例如使用高精度算法或避免数值不稳定的操作。</li><li>为什么在<code>squared_loss</code>（均方损失）函数中需要使用<code>reshape</code>函数？<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">squared_loss</span>(<span class="params">y_hat, y</span>):  <span class="comment">#@save</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;均方损失&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">return</span> (y_hat - y.reshape(y_hat.shape)) ** <span class="number">2</span> / <span class="number">2</span></span><br></pre></td></tr></table></figure>我们需要将真实值<code>y</code>的形状转换为和预测值<code>y_hat</code>的形状相同</li><li>尝试使用不同的学习率，观察损失函数值下降的快慢</li><li>如果样本个数不能被批量大小整除，<code>data_iter</code>函数的行为会有什么变化？<br>最后一个批次的大小会小于设定的批量大小。</li></ol><h2 id="线性回归的简洁实现"><a href="#线性回归的简洁实现" class="headerlink" title="线性回归的简洁实现"></a>线性回归的简洁实现</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch.utils <span class="keyword">import</span> data</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br><span class="line"></span><br><span class="line">true_w = torch.tensor([<span class="number">2</span>, -<span class="number">3.4</span>])</span><br><span class="line">true_b = <span class="number">4.2</span></span><br><span class="line">features, labels = d2l.synthetic_data(true_w, true_b, <span class="number">1000</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">load_array</span>(<span class="params">data_arrays, batch_size, is_train=<span class="literal">True</span></span>):  <span class="comment">#@save</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;构造一个PyTorch数据迭代器&quot;&quot;&quot;</span></span><br><span class="line">    dataset = data.TensorDataset(*data_arrays)</span><br><span class="line">    <span class="keyword">return</span> data.DataLoader(dataset, batch_size, shuffle=is_train)</span><br><span class="line"></span><br><span class="line">batch_size = <span class="number">10</span></span><br><span class="line">data_iter = load_array((features, labels), batch_size)</span><br><span class="line"><span class="built_in">next</span>(<span class="built_in">iter</span>(data_iter))</span><br></pre></td></tr></table></figure><h3 id="定义模型"><a href="#定义模型" class="headerlink" title="定义模型"></a>定义模型</h3><p>正如我们在构造<code>nn.Linear</code>时指定输入和输出尺寸一样， 现在我们能直接访问参数以设定它们的初始值。 我们通过<code>net[0]</code>选择网络中的第一个图层， 然后使用<code>weight.data</code>和<code>bias.data</code>方法访问参数。 我们还可以使用替换方法<code>normal_</code>和<code>fill_</code>来重写参数值。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line">net = nn.Sequential(nn.Linear(<span class="number">2</span>, <span class="number">1</span>))</span><br><span class="line">net[<span class="number">0</span>].weight.data.normal_(<span class="number">0</span>, <span class="number">0.01</span>)</span><br><span class="line">net[<span class="number">0</span>].bias.data.fill_(<span class="number">0</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure></p><h1 id="多层感知机"><a href="#多层感知机" class="headerlink" title="多层感知机"></a>多层感知机</h1><h2 id="多层感知机-1"><a href="#多层感知机-1" class="headerlink" title="多层感知机"></a>多层感知机</h2><h3 id="激活函数"><a href="#激活函数" class="headerlink" title="激活函数"></a>激活函数</h3><p>RELU</p><script type="math/tex; mode=display">ReLU(x)=max(x,0)</script><p>sigmoid函数</p><script type="math/tex; mode=display">sigmoid(x)=\frac{1}{1+exp(-x)}</script><p>tanh函数</p><script type="math/tex; mode=display">tanh(x)=\frac{1-exp(-2x)}{1+exp(-2x)}</script><h3 id="练习-6"><a href="#练习-6" class="headerlink" title="练习"></a>练习</h3><ol><li>计算pReLU激活函数的导数。<script type="math/tex; mode=display">f(x)=\begin{cases}x & \text{ if } x>0 \\\alpha x  & \text{ otherwise } \end{cases}</script>导数为<script type="math/tex; mode=display">{f}'(x)=\begin{cases}1 & \text{ if } x>0 \\\alpha   & \text{ otherwise } \end{cases}</script></li><li>假设我们有一个非线性单元，将它一次应用于一个小批量的数据。这会导致什么样的问题？</li></ol><ul><li><strong>过拟合</strong>：在小批量上应用非线性单元可能导致模型在训练集上过拟合，因为模型可能会过度记住小批量的特定模式或噪声。</li><li><strong>收敛速度</strong>：在小批量上应用非线性单元可能会影响模型的收敛速度，因为模型可能需要更多的迭代才能收敛到最优解。<h2 id="多层感知机简洁实现"><a href="#多层感知机简洁实现" class="headerlink" title="多层感知机简洁实现"></a>多层感知机简洁实现</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br><span class="line">net = nn.Sequential(nn.Flatten(),</span><br><span class="line">                    nn.Linear(<span class="number">784</span>, <span class="number">256</span>),</span><br><span class="line">                    nn.ReLU(),</span><br><span class="line">                    nn.Linear(<span class="number">256</span>, <span class="number">10</span>))</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">init_weights</span>(<span class="params">m</span>):</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">type</span>(m) == nn.Linear:</span><br><span class="line">        nn.init.normal_(m.weight, std=<span class="number">0.01</span>)</span><br><span class="line"></span><br><span class="line">net.apply(init_weights)</span><br><span class="line">batch_size, lr, num_epochs = <span class="number">256</span>, <span class="number">0.1</span>, <span class="number">10</span></span><br><span class="line">loss = nn.CrossEntropyLoss(reduction=<span class="string">&#x27;none&#x27;</span>)</span><br><span class="line">trainer = torch.optim.SGD(net.parameters(), lr=lr)</span><br><span class="line"></span><br><span class="line">train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)</span><br><span class="line">d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, trainer)</span><br></pre></td></tr></table></figure></li></ul><h3 id="练习-7"><a href="#练习-7" class="headerlink" title="练习"></a>练习</h3><ol><li>尝试添加不同数量的隐藏层，修改学习率。</li><li>尝试不同的激活函数，哪个效果最好？<br>MSEloss</li><li>尝试不同的方案来初始化权重，什么方法效果最好？<br>正态分布初始化</li></ol><h2 id="模型选择，欠拟合，过拟合"><a href="#模型选择，欠拟合，过拟合" class="headerlink" title="模型选择，欠拟合，过拟合"></a>模型选择，欠拟合，过拟合</h2><p>将模型在训练数据上拟合的比在潜在分布中更接近的现象称为<em>过拟合（overfitting）， 用于对抗过拟合的技术称为正则化（regularization）。<br>在训练参数化机器学习模型时， </em>权重衰减<em>（weight decay）是最广泛使用的正则化的技术之一， 它通常也被称为L2</em>正则化_。 这项技术通过函数与零的距离来衡量函数的复杂度， 因为在所有函数f中，函数f=0（所有输入都得到值0） 在某种意义上是最简单的。这个正则化项通常是权重的平方和或绝对值和，以惩罚模型的复杂度。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">train_concise</span>(<span class="params">wd</span>):</span><br><span class="line">    net = nn.Sequential(nn.Linear(num_inputs, <span class="number">1</span>))</span><br><span class="line">    <span class="keyword">for</span> param <span class="keyword">in</span> net.parameters():</span><br><span class="line">        param.data.normal_()</span><br><span class="line">    loss = nn.MSELoss(reduction=<span class="string">&#x27;none&#x27;</span>)</span><br><span class="line">    num_epochs, lr = <span class="number">100</span>, <span class="number">0.003</span></span><br><span class="line">    <span class="comment"># 偏置参数没有衰减</span></span><br><span class="line">    trainer = torch.optim.SGD([</span><br><span class="line">        &#123;<span class="string">&quot;params&quot;</span>:net[<span class="number">0</span>].weight,<span class="string">&#x27;weight_decay&#x27;</span>: wd&#125;,</span><br><span class="line">        &#123;<span class="string">&quot;params&quot;</span>:net[<span class="number">0</span>].bias&#125;], lr=lr)</span><br><span class="line">    animator = d2l.Animator(xlabel=<span class="string">&#x27;epochs&#x27;</span>, ylabel=<span class="string">&#x27;loss&#x27;</span>, yscale=<span class="string">&#x27;log&#x27;</span>,xlim=[<span class="number">5</span>, num_epochs], legend=[<span class="string">&#x27;train&#x27;</span>, <span class="string">&#x27;test&#x27;</span>])</span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epochs):</span><br><span class="line">        <span class="keyword">for</span> X, y <span class="keyword">in</span> train_iter:</span><br><span class="line">            trainer.zero_grad()</span><br><span class="line">            l = loss(net(X), y)</span><br><span class="line">            l.mean().backward()</span><br><span class="line">            trainer.step()</span><br><span class="line">        <span class="keyword">if</span> (epoch + <span class="number">1</span>) % <span class="number">5</span> == <span class="number">0</span>:</span><br><span class="line">            animator.add(epoch + <span class="number">1</span>,</span><br><span class="line">            (d2l.evaluate_loss(net, train_iter, loss),</span><br><span class="line">            d2l.evaluate_loss(net, test_iter, loss)))</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;w的L2范数：&#x27;</span>, net[<span class="number">0</span>].weight.norm().item())</span><br></pre></td></tr></table></figure><h3 id="练习-8"><a href="#练习-8" class="headerlink" title="练习"></a>练习</h3><ol><li>在本节的估计问题中使用$\lambda$的值进行实验。绘制训练和测试精度关于$\lambda$的函数。观察到了什么？<br>λ 太大后，train和test的loss会变得很大，太小后，train的loss会低，但是test的loss会很高。</li><li>使用验证集来找到最佳值λ。它真的是最优值吗？这有关系吗？<br>它是对于该验证集的最优值，但不是全局最优值，会随着训练集与验证集而变化。</li><li>回顾训练误差和泛化误差之间的关系。除了权重衰减、增加训练数据、使用适当复杂度的模型之外，还能想出其他什么方法来处理过拟合？<br>Dropout，数据增强，模型集成</li></ol><h2 id="暂退法（Dropout）"><a href="#暂退法（Dropout）" class="headerlink" title="暂退法（Dropout）"></a>暂退法（Dropout）</h2><p>暂退法在前向传播过程中，计算每一内部层的同时注入噪声，这已经成为训练神经网络的常用技术。 这种方法之所以被称为<em>暂退法</em>，因为我们从表面上看是在训练过程中丢弃（drop out）一些神经元。 在整个训练过程的每一次迭代中，标准暂退法包括在计算下一层之前将当前层中的一些节点置零。</p><h3 id="从零开始实现"><a href="#从零开始实现" class="headerlink" title="从零开始实现"></a>从零开始实现</h3><p>要实现单层的暂退法函数， 我们从均匀分布U[0,1]中抽取样本，样本数与这层神经网络的维度一致。 然后我们保留那些对应样本大于p的节点，把剩下的丢弃。</p><p>在下面的代码中，我们实现 <code>dropout_layer</code> 函数， 该函数以<code>dropout</code>的概率丢弃张量输入<code>X</code>中的元素， 如上所述重新缩放剩余部分：将剩余部分除以<code>1.0-dropout</code>。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">dropout_layer</span>(<span class="params">X, dropout</span>):</span><br><span class="line">    <span class="keyword">assert</span> <span class="number">0</span> &lt;= dropout &lt;= <span class="number">1</span></span><br><span class="line">    <span class="comment"># 在本情况中，所有元素都被丢弃</span></span><br><span class="line">    <span class="keyword">if</span> dropout == <span class="number">1</span>:</span><br><span class="line">        <span class="keyword">return</span> torch.zeros_like(X)</span><br><span class="line">    <span class="comment"># 在本情况中，所有元素都被保留</span></span><br><span class="line">    <span class="keyword">if</span> dropout == <span class="number">0</span>:</span><br><span class="line">        <span class="keyword">return</span> X</span><br><span class="line">    mask = (torch.rand(X.shape) &gt; dropout).<span class="built_in">float</span>()</span><br><span class="line">    <span class="keyword">return</span> mask * X / (<span class="number">1.0</span> - dropout)</span><br></pre></td></tr></table></figure><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">X= torch.arange(<span class="number">16</span>, dtype = torch.float32).reshape((<span class="number">2</span>, <span class="number">8</span>))</span><br><span class="line"><span class="built_in">print</span>(X)</span><br><span class="line"><span class="built_in">print</span>(dropout_layer(X, <span class="number">0.</span>))</span><br><span class="line"><span class="built_in">print</span>(dropout_layer(X, <span class="number">0.5</span>))</span><br><span class="line"><span class="built_in">print</span>(dropout_layer(X, <span class="number">1.</span>))</span><br><span class="line"><span class="comment">#tensor([[ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.],</span></span><br><span class="line"><span class="comment">#        [ 8.,  9., 10., 11., 12., 13., 14., 15.]])</span></span><br><span class="line"><span class="comment">#tensor([[ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.],</span></span><br><span class="line"><span class="comment">#        [ 8.,  9., 10., 11., 12., 13., 14., 15.]])</span></span><br><span class="line"><span class="comment">#tensor([[ 0.,  2.,  0.,  6.,  0.,  0.,  0., 14.],</span></span><br><span class="line"><span class="comment">#       [16., 18.,  0., 22.,  0., 26., 28., 30.]])</span></span><br><span class="line"><span class="comment">#tensor([[0., 0., 0., 0., 0., 0., 0., 0.],</span></span><br><span class="line"><span class="comment">#        [0., 0., 0., 0., 0., 0., 0., 0.]])</span></span><br></pre></td></tr></table></figure></p><h3 id="定义模型-1"><a href="#定义模型-1" class="headerlink" title="定义模型"></a>定义模型</h3><p>我们可以将暂退法应用于每个隐藏层的输出（在激活函数之后）， 并且可以为每一层分别设置暂退概率： 常见的技巧是在靠近输入层的地方设置较低的暂退概率。 下面的模型将第一个和第二个隐藏层的暂退概率分别设置为0.2和0.5， 并且暂退法只在训练期间有效。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">dropout1, dropout2 = <span class="number">0.2</span>, <span class="number">0.5</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Net</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, num_inputs, num_outputs, num_hiddens1, num_hiddens2,</span></span><br><span class="line"><span class="params">                 is_training = <span class="literal">True</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(Net, self).__init__()</span><br><span class="line">        self.num_inputs = num_inputs</span><br><span class="line">        self.training = is_training</span><br><span class="line">        self.lin1 = nn.Linear(num_inputs, num_hiddens1)</span><br><span class="line">        self.lin2 = nn.Linear(num_hiddens1, num_hiddens2)</span><br><span class="line">        self.lin3 = nn.Linear(num_hiddens2, num_outputs)</span><br><span class="line">        self.relu = nn.ReLU()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, X</span>):</span><br><span class="line">        H1 = self.relu(self.lin1(X.reshape((-<span class="number">1</span>, self.num_inputs))))</span><br><span class="line">        <span class="comment"># 只有在训练模型时才使用dropout</span></span><br><span class="line">        <span class="keyword">if</span> self.training == <span class="literal">True</span>:</span><br><span class="line">            <span class="comment"># 在第一个全连接层之后添加一个dropout层</span></span><br><span class="line">            H1 = dropout_layer(H1, dropout1)</span><br><span class="line">        H2 = self.relu(self.lin2(H1))</span><br><span class="line">        <span class="keyword">if</span> self.training == <span class="literal">True</span>:</span><br><span class="line">            <span class="comment"># 在第二个全连接层之后添加一个dropout层</span></span><br><span class="line">            H2 = dropout_layer(H2, dropout2)</span><br><span class="line">        out = self.lin3(H2)</span><br><span class="line">        <span class="keyword">return</span> out</span><br><span class="line"></span><br><span class="line">net = Net(num_inputs, num_outputs, num_hiddens1, num_hiddens2)</span><br></pre></td></tr></table></figure></p><h3 id="简介实现"><a href="#简介实现" class="headerlink" title="简介实现"></a>简介实现</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">net = nn.Sequential(nn.Flatten(),</span><br><span class="line">        nn.Linear(<span class="number">784</span>, <span class="number">256</span>),</span><br><span class="line">        nn.ReLU(),</span><br><span class="line">        <span class="comment"># 在第一个全连接层之后添加一个dropout层</span></span><br><span class="line">        nn.Dropout(dropout1),</span><br><span class="line">        nn.Linear(<span class="number">256</span>, <span class="number">256</span>),</span><br><span class="line">        nn.ReLU(),</span><br><span class="line">        <span class="comment"># 在第二个全连接层之后添加一个dropout层</span></span><br><span class="line">        nn.Dropout(dropout2),</span><br><span class="line">        nn.Linear(<span class="number">256</span>, <span class="number">10</span>))</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">init_weights</span>(<span class="params">m</span>):</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">type</span>(m) == nn.Linear:</span><br><span class="line">        nn.init.normal_(m.weight, std=<span class="number">0.01</span>)</span><br><span class="line"></span><br><span class="line">net.apply(init_weights)</span><br><span class="line">trainer = torch.optim.SGD(net.parameters(), lr=lr)</span><br><span class="line">d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, trainer)</span><br></pre></td></tr></table></figure><h3 id="练习-9"><a href="#练习-9" class="headerlink" title="练习"></a>练习</h3><ol><li>如果更改第一层和第二层的暂退法概率，会发生什么情况？具体地说，如果交换这两个层，会发生什么情况？设计一个实验来回答这些问题，定量描述该结果，并总结定性的结论。<br>在调换第一和第二层概率后，结果基本没有什么变化，但是改变第一层和第二层的概率总和后，结果会有明显的变化。</li><li>增加训练轮数，并将使用暂退法和不使用暂退法时获得的结果进行比较。<br>没有dropout的训练效果会更好，但是泛化性不够好。</li><li>为什么在测试时通常不使用暂退法？<br>为了保持模型的一致性和稳定性，在测试时，我们希望模型能够产生确定性的预测结果，而不是随机性的输出。</li><li>如果我们将暂退法应用到权重矩阵的各个权重，而不是激活值，会发生什么？<br>train_loss会下降的比较慢。</li></ol><h2 id="数值稳定性与模型初始化"><a href="#数值稳定性与模型初始化" class="headerlink" title="数值稳定性与模型初始化"></a>数值稳定性与模型初始化</h2><h3 id="梯度消失和梯度爆炸"><a href="#梯度消失和梯度爆炸" class="headerlink" title="梯度消失和梯度爆炸"></a>梯度消失和梯度爆炸</h3><p><strong>梯度消失</strong>：当网络的层数较多时，梯度在反向传播过程中可能会变得非常小，甚至接近于零。这样，深层网络中较早层的参数将无法得到有效更新，导致模型无法学习到有效的特征表示，从而影响模型的性能。<br><strong>梯度爆炸</strong>：与梯度消失相反，梯度爆炸指的是在反向传播过程中，梯度变得非常大，甚至超出了计算机能够表示的范围。这样会导致参数更新过大，模型参数发散，无法收敛到有效的解决方案。<br>为了解决梯度消失和梯度爆炸问题，可以采取以下几种方法：</p><ol><li><p><strong>权重初始化</strong>：合适的权重初始化可以帮助减少梯度消失和爆炸的可能性。例如，使用较小的随机数来初始化权重。</p></li><li><p><strong>梯度裁剪</strong>：在反向传播过程中，如果梯度的范数超过了设定的阈值，可以对梯度进行裁剪，将其限制在一个合理的范围内，防止梯度爆炸。</p></li><li><p><strong>使用激活函数</strong>：合适的激活函数可以帮助缓解梯度消失和梯度爆炸问题。例如，ReLU等激活函数在一定程度上可以防止梯度消失。</p></li><li><p><strong>Batch Normalization</strong>：批量归一化可以减少内部协变量漂移，并有助于稳定训练过程，从而减少梯度消失和爆炸的可能性。</p></li><li><p><strong>使用更深层次的结构</strong>：使用一些技术，如残差连接（Residual Connections），可以帮助信息在网络中更好地传播，减少梯度消失的影响。</p><h1 id="深度学习计算"><a href="#深度学习计算" class="headerlink" title="深度学习计算"></a>深度学习计算</h1><h2 id="层和块"><a href="#层和块" class="headerlink" title="层和块"></a>层和块</h2><p><em>块</em>（block）可以描述单个层、由多个层组成的组件或整个模型本身。从编程的角度来看，块由类（class）表示。 它的任何子类都必须定义一个将其输入转换为输出的前向传播函数， 并且必须存储任何必需的参数。</p><h3 id="自定义块"><a href="#自定义块" class="headerlink" title="自定义块"></a>自定义块</h3><p>一个多层感知器块：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">MLP</span>(nn.Module):</span><br><span class="line">    <span class="comment"># 用模型参数声明层。这里，我们声明两个全连接的层</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="comment"># 调用MLP的父类Module的构造函数来执行必要的初始化。</span></span><br><span class="line">        <span class="comment"># 这样，在类实例化时也可以指定其他函数参数，例如模型参数params（稍后将介绍）</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.hidden = nn.Linear(<span class="number">20</span>, <span class="number">256</span>)  <span class="comment"># 隐藏层</span></span><br><span class="line">        self.out = nn.Linear(<span class="number">256</span>, <span class="number">10</span>)  <span class="comment"># 输出层</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 定义模型的前向传播，即如何根据输入X返回所需的模型输出</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, X</span>):</span><br><span class="line">        <span class="comment"># 注意，这里我们使用ReLU的函数版本，其在nn.functional模块中定义。</span></span><br><span class="line">        <span class="keyword">return</span> self.out(F.relu(self.hidden(X)))</span><br></pre></td></tr></table></figure><p>使用：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">net = MLP()</span><br><span class="line">net(X)</span><br></pre></td></tr></table></figure><h3 id="顺序块"><a href="#顺序块" class="headerlink" title="顺序块"></a>顺序块</h3><p>自己定义一个sequential类</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">MySequential</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, *args</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="keyword">for</span> idx, module <span class="keyword">in</span> <span class="built_in">enumerate</span>(args):</span><br><span class="line">            <span class="comment"># 这里，module是Module子类的一个实例。我们把它保存在&#x27;Module&#x27;类的成员</span></span><br><span class="line">            <span class="comment"># 变量_modules中。_module的类型是OrderedDict</span></span><br><span class="line">            self._modules[<span class="built_in">str</span>(idx)] = module</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, X</span>):</span><br><span class="line">        <span class="comment"># OrderedDict保证了按照成员添加的顺序遍历它们</span></span><br><span class="line">        <span class="keyword">for</span> block <span class="keyword">in</span> self._modules.values():</span><br><span class="line">            X = block(X)</span><br><span class="line">        <span class="keyword">return</span> X</span><br></pre></td></tr></table></figure><p>使用方法：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">net = MySequential(nn.Linear(<span class="number">20</span>, <span class="number">256</span>), nn.ReLU(), nn.Linear(<span class="number">256</span>, <span class="number">10</span>))</span><br><span class="line">net(X)</span><br></pre></td></tr></table></figure><p>我们可以混合搭配各种组合块的方法：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">NestMLP</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.net = nn.Sequential(nn.Linear(<span class="number">20</span>, <span class="number">64</span>), nn.ReLU(),</span><br><span class="line">                                 nn.Linear(<span class="number">64</span>, <span class="number">32</span>), nn.ReLU())</span><br><span class="line">        self.linear = nn.Linear(<span class="number">32</span>, <span class="number">16</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, X</span>):</span><br><span class="line">        <span class="keyword">return</span> self.linear(self.net(X))</span><br><span class="line"></span><br><span class="line">chimera = nn.Sequential(NestMLP(), nn.Linear(<span class="number">16</span>, <span class="number">20</span>), FixedHiddenMLP())</span><br><span class="line">chimera(X)</span><br></pre></td></tr></table></figure><h3 id="练习-10"><a href="#练习-10" class="headerlink" title="练习"></a>练习</h3></li><li>如果将<code>MySequential</code>中存储块的方式更改为Python列表，会出现什么样的问题？<br>print(net) 会报错，不能更清晰的看到网络结构。</li><li>实现一个块，它以两个块为参数，例如<code>net1</code>和<code>net2</code>，并返回前向传播中两个网络的串联输出。这也被称为平行块。<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">paarallelblock</span>(nn.Module):</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self,net1,net2</span>):</span><br><span class="line"><span class="built_in">super</span>(parallelblock,self).__init__()</span><br><span class="line">self.net1 = net1</span><br><span class="line">self.net2 = net2</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self,x</span>):</span><br><span class="line">out1 = self.net1(x)</span><br><span class="line">out2 = self.net2(x)</span><br><span class="line"><span class="keyword">return</span> torch.cat((out1,out2),dim = <span class="number">1</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure></li><li>假设我们想要连接同一网络的多个实例。实现一个函数，该函数生成同一个块的多个实例，并在此基础上构建更大的网络。<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">block1</span>():</span><br><span class="line">    <span class="keyword">return</span> nn.Sequential(nn.Linear(<span class="number">4</span>, <span class="number">8</span>), nn.ReLU(),</span><br><span class="line">                         nn.Linear(<span class="number">8</span>, <span class="number">4</span>), nn.ReLU())</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">block2</span>():</span><br><span class="line">    net = nn.Sequential()</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">4</span>):</span><br><span class="line">        <span class="comment"># 在这里嵌套</span></span><br><span class="line">        net.add_module(<span class="string">f&#x27;block <span class="subst">&#123;i&#125;</span>&#x27;</span>, block1())</span><br><span class="line">    <span class="keyword">return</span> net</span><br><span class="line"></span><br><span class="line">rgnet = nn.Sequential(block2(), nn.Linear(<span class="number">4</span>, <span class="number">1</span>))</span><br><span class="line">rgnet(X)</span><br></pre></td></tr></table></figure><h2 id="参数管理"><a href="#参数管理" class="headerlink" title="参数管理"></a>参数管理</h2><h3 id="参数访问"><a href="#参数访问" class="headerlink" title="参数访问"></a>参数访问</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"></span><br><span class="line">net = nn.Sequential(nn.Linear(<span class="number">4</span>, <span class="number">8</span>), nn.ReLU(), nn.Linear(<span class="number">8</span>, <span class="number">1</span>))</span><br><span class="line">X = torch.rand(size=(<span class="number">2</span>, <span class="number">4</span>))</span><br><span class="line">net(X)</span><br><span class="line"><span class="built_in">print</span>(net[<span class="number">2</span>].state_dict())</span><br><span class="line"><span class="comment"># OrderedDict([(&#x27;weight&#x27;, tensor([[-0.0427, -0.2939, -0.1894,  0.0220, -0.1709, -0.1522, -0.0334, -0.2263]])), (&#x27;bias&#x27;, tensor([0.0887]))])</span></span><br></pre></td></tr></table></figure>每个参数都表示为参数类的一个实例。 要对参数执行任何操作，首先我们需要访问底层的数值。 有几种方法可以做到这一点。有些比较简单，而另一些则比较通用。 下面的代码从第二个全连接层（即第三个神经网络层）提取偏置， 提取后返回的是一个参数类实例，并进一步访问该参数的值。<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(<span class="built_in">type</span>(net[<span class="number">2</span>].bias))</span><br><span class="line"><span class="built_in">print</span>(net[<span class="number">2</span>].bias)</span><br><span class="line"><span class="built_in">print</span>(net[<span class="number">2</span>].bias.data)</span><br><span class="line"><span class="comment"># &lt;class &#x27;torch.nn.parameter.Parameter&#x27;&gt;</span></span><br><span class="line"><span class="comment"># Parameter containing:</span></span><br><span class="line"><span class="comment"># tensor([0.0887], requires_grad=True)</span></span><br><span class="line"><span class="comment"># tensor([0.0887])</span></span><br></pre></td></tr></table></figure><h3 id="一次性访问所有参数"><a href="#一次性访问所有参数" class="headerlink" title="一次性访问所有参数"></a>一次性访问所有参数</h3>递归整个树来提取每个子块的参数：<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(*[(name, param.shape) <span class="keyword">for</span> name, param <span class="keyword">in</span> net[<span class="number">0</span>].named_parameters()])</span><br><span class="line"><span class="comment"># (&#x27;weight&#x27;, torch.Size([8, 4])) (&#x27;bias&#x27;, torch.Size([8]))</span></span><br><span class="line"><span class="built_in">print</span>(*[(name, param.shape) <span class="keyword">for</span> name, param <span class="keyword">in</span> net.named_parameters()])</span><br><span class="line"></span><br></pre></td></tr></table></figure>这为我们提供了另一种访问网络参数的方式，如下所示。<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">net.state_dict()[<span class="string">&#x27;2.bias&#x27;</span>].data</span><br><span class="line"><span class="comment"># tensor([0.0887])</span></span><br></pre></td></tr></table></figure>观察网络是如何工作的：<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">block1</span>():</span><br><span class="line">    <span class="keyword">return</span> nn.Sequential(nn.Linear(<span class="number">4</span>, <span class="number">8</span>), nn.ReLU(),</span><br><span class="line">                         nn.Linear(<span class="number">8</span>, <span class="number">4</span>), nn.ReLU())</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">block2</span>():</span><br><span class="line">    net = nn.Sequential()</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">4</span>):</span><br><span class="line">        <span class="comment"># 在这里嵌套</span></span><br><span class="line">        net.add_module(<span class="string">f&#x27;block <span class="subst">&#123;i&#125;</span>&#x27;</span>, block1())</span><br><span class="line">    <span class="keyword">return</span> net</span><br><span class="line"></span><br><span class="line">rgnet = nn.Sequential(block2(), nn.Linear(<span class="number">4</span>, <span class="number">1</span>))</span><br><span class="line">rgnet(X)</span><br><span class="line"><span class="built_in">print</span>(regnet)</span><br><span class="line"><span class="comment"># Sequential(</span></span><br><span class="line">  <span class="comment">#(0): Sequential(</span></span><br><span class="line">    <span class="comment">#(block 0): Sequential(</span></span><br><span class="line">      <span class="comment">#(0): Linear(in_features=4, out_features=8, bias=True)</span></span><br><span class="line">      <span class="comment">#(1): ReLU()</span></span><br><span class="line">      <span class="comment">#(2): Linear(in_features=8, out_features=4, bias=True)</span></span><br><span class="line">      <span class="comment">#(3): ReLU()</span></span><br><span class="line">    <span class="comment">#)</span></span><br><span class="line">    <span class="comment">#(block 1): Sequential(</span></span><br><span class="line">      <span class="comment">#(0): Linear(...</span></span><br></pre></td></tr></table></figure><h3 id="参数绑定"><a href="#参数绑定" class="headerlink" title="参数绑定"></a>参数绑定</h3>我们希望在多个层间共享参数，可以定义一个稠密层，使用它的参数来设置另一个层的参数：<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 我们需要给共享层一个名称，以便可以引用它的参数</span></span><br><span class="line">shared = nn.Linear(<span class="number">8</span>, <span class="number">8</span>)</span><br><span class="line">net = nn.Sequential(nn.Linear(<span class="number">4</span>, <span class="number">8</span>), nn.ReLU(),</span><br><span class="line">                    shared, nn.ReLU(),</span><br><span class="line">                    shared, nn.ReLU(),</span><br><span class="line">                    nn.Linear(<span class="number">8</span>, <span class="number">1</span>))</span><br><span class="line">net(X)</span><br><span class="line"><span class="comment"># 检查参数是否相同</span></span><br><span class="line"><span class="built_in">print</span>(net[<span class="number">2</span>].weight.data[<span class="number">0</span>] == net[<span class="number">4</span>].weight.data[<span class="number">0</span>])</span><br><span class="line">net[<span class="number">2</span>].weight.data[<span class="number">0</span>, <span class="number">0</span>] = <span class="number">100</span></span><br><span class="line"><span class="comment"># 确保它们实际上是同一个对象，而不只是有相同的值</span></span><br><span class="line"><span class="built_in">print</span>(net[<span class="number">2</span>].weight.data[<span class="number">0</span>] == net[<span class="number">4</span>].weight.data[<span class="number">0</span>])</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tensor([<span class="literal">True</span>, <span class="literal">True</span>, <span class="literal">True</span>, <span class="literal">True</span>, <span class="literal">True</span>, <span class="literal">True</span>, <span class="literal">True</span>, <span class="literal">True</span>])</span><br><span class="line">tensor([<span class="literal">True</span>, <span class="literal">True</span>, <span class="literal">True</span>, <span class="literal">True</span>, <span class="literal">True</span>, <span class="literal">True</span>, <span class="literal">True</span>, <span class="literal">True</span>])</span><br></pre></td></tr></table></figure>会发现$shared$层在net中被使用了两次，但它们依然拥有相同的参数，它们不仅值相等，而且由相同的张量表示。因此我们改变其中一个参数，另一个参数也会改变。问题：当参数绑定时，梯度会发生什么情况？答案时由于模型参数包含梯度，因此在反向传播期间死三个神经网络层和第五个神经网络层的梯度会加到一起。</li></ol><h3 id="练习-11"><a href="#练习-11" class="headerlink" title="练习"></a>练习</h3><p>为什么共享参数是个好主意？</p><ol><li>可以减少模型的参数量，降低复杂度</li><li>提高泛化能力</li><li>在循环神经网络/卷积神经网络中参数共享可以帮助模型更好地捕捉数据的时空特征，提高学习效率<h2 id="自定义层"><a href="#自定义层" class="headerlink" title="自定义层"></a>自定义层</h2>首先构造一个没有任何参数的层（减去均值）<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">CenteredLayer</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, X</span>):</span><br><span class="line">        <span class="keyword">return</span> X - X.mean()</span><br></pre></td></tr></table></figure>将层作为组件合并到更复杂的模型中：<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">net = nn.Sequential(nn.Linear(<span class="number">8</span>, <span class="number">128</span>), CenteredLayer())</span><br></pre></td></tr></table></figure><h3 id="设计带参数的层"><a href="#设计带参数的层" class="headerlink" title="设计带参数的层"></a>设计带参数的层</h3>(其实是自定义了参数)<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">MyLinear</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, in_units, units</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.weight = nn.Parameter(torch.randn(in_units, units))</span><br><span class="line">        self.bias = nn.Parameter(torch.randn(units,))</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, X</span>):</span><br><span class="line">        linear = torch.matmul(X, self.weight.data) + self.bias.data</span><br><span class="line">        <span class="keyword">return</span> F.relu(linear)</span><br></pre></td></tr></table></figure>初始化<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">linear = MyLinear(<span class="number">5</span>,<span class="number">3</span>)</span><br><span class="line">linear.weight</span><br><span class="line">linear(torch.rand(<span class="number">2</span>,<span class="number">5</span>))</span><br></pre></td></tr></table></figure><h3 id="练习-12"><a href="#练习-12" class="headerlink" title="练习"></a>练习</h3>设计一个返回输入数据的傅里叶前半部分的层<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">FourierFrontHalf</span>(nn.Module): </span><br><span class="line"><span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>): <span class="built_in">super</span>(FourierFrontHalf, self).__init__() </span><br><span class="line"><span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>): <span class="comment"># 计算傅里叶变换 </span></span><br><span class="line">x_fft = torch.fft.fft(x) <span class="comment"># 获取前半部分 </span></span><br><span class="line">front_half = x_fft[:, :x_fft.shape[<span class="number">1</span>] // <span class="number">2</span>] </span><br><span class="line"><span class="keyword">return</span> front_half</span><br></pre></td></tr></table></figure><h2 id="读写文件"><a href="#读写文件" class="headerlink" title="读写文件"></a>读写文件</h2><h3 id="基本load，save操作"><a href="#基本load，save操作" class="headerlink" title="基本load，save操作"></a>基本load，save操作</h3>对于单个张量可以调用load和save函数分别读写它们。<br>这两个函数都要求我们提供一个名称，<code>save</code>要求将要保存的变量作为输入。<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line">form torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.nn <span class="keyword">import</span> functional <span class="keyword">as</span> F</span><br><span class="line">x = torch.arrange(<span class="number">4</span>)</span><br><span class="line">torch.save(x,<span class="string">&#x27;x-file&#x27;</span>)</span><br></pre></td></tr></table></figure>将存储在文件中的数据读回内存：<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">x2 = torch.load(<span class="string">&#x27;x-file&#x27;</span>)</span><br></pre></td></tr></table></figure>存储一个张量列表，然后把它们（分别）读回内存。<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">y = torch.zeros(<span class="number">4</span>)</span><br><span class="line">torch.save([x, y],<span class="string">&#x27;x-files&#x27;</span>)</span><br><span class="line">x2, y2 = torch.load(<span class="string">&#x27;x-files&#x27;</span>)</span><br><span class="line">(x2, y2)</span><br></pre></td></tr></table></figure>可以写入或读取从字符串映射到张量的字典。 当我们要读取或写入模型中的所有权重时，这很方便<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">mydict = &#123;<span class="string">&#x27;x&#x27;</span>: x, <span class="string">&#x27;y&#x27;</span>: y&#125;</span><br><span class="line">torch.save(mydict, <span class="string">&#x27;mydict&#x27;</span>)</span><br><span class="line">mydict2 = torch.load(<span class="string">&#x27;mydict&#x27;</span>)</span><br><span class="line">mydict2</span><br><span class="line"><span class="comment"># &#123;&#x27;x&#x27;: tensor([0, 1, 2, 3]), &#x27;y&#x27;: tensor([0., 0., 0., 0.])&#125;</span></span><br></pre></td></tr></table></figure><h3 id="模型的加载与保存"><a href="#模型的加载与保存" class="headerlink" title="模型的加载与保存"></a>模型的加载与保存</h3>深度学习框架提供了内置函数来保存和加载整个网络。 需要注意的一个重要细节是，这将保存模型的参数而不是保存整个模型。<br>多层感知机模型参数存储：<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">MLP</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.hidden = nn.Linear(<span class="number">20</span>, <span class="number">256</span>)</span><br><span class="line">        self.output = nn.Linear(<span class="number">256</span>, <span class="number">10</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="keyword">return</span> self.output(F.relu(self.hidden(x)))</span><br><span class="line"></span><br><span class="line">net = MLP()</span><br><span class="line">X = torch.randn(size=(<span class="number">2</span>, <span class="number">20</span>))</span><br><span class="line">Y = net(X)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.save(net.state_dict(), <span class="string">&#x27;mlp.params&#x27;</span>)</span><br></pre></td></tr></table></figure>为了恢复模型，我们实例化了原始多层感知机模型的一个备份。 这里我们不需要随机初始化模型参数，而是直接读取文件中存储的参数：<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">clone = MLP()</span><br><span class="line">clone.load_state_dict(torch.load(<span class="string">&#x27;mlp.params&#x27;</span>))</span><br><span class="line">clone.<span class="built_in">eval</span>()</span><br></pre></td></tr></table></figure><h3 id="练习-13"><a href="#练习-13" class="headerlink" title="练习"></a>练习</h3></li><li>即使不需要将经过训练的模型部署到不同的设备上，存储模型参数还有什么实际的好处？<br> 节省内存，模型压缩，快速部署，方便下一次训练</li><li>假设我们只想复用网络的一部分，以将其合并到不同的网络架构中。比如想在一个新的网络中使用之前网络的前两层，该怎么做？</li></ol><ul><li><strong>提取需要复用的部分</strong>：从原始网络中提取前两层的参数。</li><li><strong>构建新网络</strong>：构建一个新的网络，将提取的部分作为其中的一部分。（以resnet为例）<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">from</span> torchvision.models <span class="keyword">import</span> resnet18</span><br><span class="line"><span class="comment"># 加载预训练的ResNet模型</span></span><br><span class="line">pretrained_model = resnet18(pretrained=<span class="literal">True</span>)</span><br><span class="line"><span class="comment"># 提取前两层</span></span><br><span class="line">conv1 = pretrained_model.conv1</span><br><span class="line">bn1 = pretrained_model.bn1</span><br><span class="line"><span class="comment"># 构建新网络</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">NewNetwork</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(NewNetwork, self).__init__()</span><br><span class="line">        self.conv1 = conv1</span><br><span class="line">        self.bn1 = bn1</span><br><span class="line">        <span class="comment"># 添加新的层</span></span><br><span class="line">        self.conv2 = nn.Conv2d(<span class="number">64</span>, <span class="number">128</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>)</span><br><span class="line">        self.bn2 = nn.BatchNorm2d(<span class="number">128</span>)</span><br><span class="line">        self.relu = nn.ReLU(inplace=<span class="literal">True</span>)</span><br><span class="line">        self.pool = nn.MaxPool2d(kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>)</span><br><span class="line">        <span class="comment"># 其他层...</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = self.conv1(x)</span><br><span class="line">        x = self.bn1(x)</span><br><span class="line">        x = self.relu(x)</span><br><span class="line">        x = self.conv2(x)</span><br><span class="line">        x = self.bn2(x)</span><br><span class="line">        x = self.relu(x)</span><br><span class="line">        x = self.pool(x)</span><br><span class="line">        <span class="comment"># 其他层的前向传播...</span></span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"><span class="comment"># 创建新网络的实例</span></span><br><span class="line">new_network = NewNetwork()</span><br></pre></td></tr></table></figure></li></ul><ol><li>如何同时保存网络架构和参数？需要对架构加上什么限制？<br>将参数与架构同时保存在同一文件中，（实例化模型并保存）加载时分开加载：<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 假设我有一个模型SimpleModel()</span></span><br><span class="line">model = SimpleModel() </span><br><span class="line">torch.save(&#123;<span class="string">&#x27;model_state_dict&#x27;</span>: model.state_dict(), <span class="string">&#x27;model_architecture&#x27;</span>: SimpleModel&#125;, <span class="string">&#x27;model.pth&#x27;</span>)</span><br></pre></td></tr></table></figure>加载时：<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 加载模型</span></span><br><span class="line">checkpoint = torch.load(<span class="string">&#x27;model.pth&#x27;</span>)</span><br><span class="line">model_architecture = checkpoint[<span class="string">&#x27;model_architecture&#x27;</span>]</span><br><span class="line">model = model_architecture()</span><br><span class="line">model.load_state_dict(checkpoint[<span class="string">&#x27;model_state_dict&#x27;</span>])</span><br></pre></td></tr></table></figure></li></ol><h2 id="GPU"><a href="#GPU" class="headerlink" title="GPU"></a>GPU</h2><p>PyTorch中，CPU和GPU可以用<code>torch.device(&#39;cpu&#39;)</code> 和<code>torch.device(&#39;cuda&#39;)</code>表示。 应该注意的是，<code>cpu</code>设备意味着所有物理CPU和内存， 这意味着PyTorch的计算将尝试使用所有CPU核心。 然而，<code>gpu</code>设备只代表一个卡和相应的显存。 如果有多个GPU，我们使用<code>torch.device(f&#39;cuda:&#123;i&#125;&#39;)</code> 来表示第�块GPU（�从0开始）。 另外，<code>cuda:0</code>和<code>cuda</code>是等价的。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"></span><br><span class="line">torch.device(<span class="string">&#x27;cpu&#x27;</span>), torch.device(<span class="string">&#x27;cuda&#x27;</span>), torch.device(<span class="string">&#x27;cuda:1&#x27;</span>)</span><br></pre></td></tr></table></figure><br>查询gpu数量<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.cuda.device_count()</span><br></pre></td></tr></table></figure><br>可以定义一个函数，可以在不存在所有所需gpu的情况下运行代码：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">try_gpu</span>(<span class="params">i=<span class="number">0</span></span>):  <span class="comment">#@save</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;如果存在，则返回gpu(i)，否则返回cpu()&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">if</span> torch.cuda.device_count() &gt;= i + <span class="number">1</span>:</span><br><span class="line">        <span class="keyword">return</span> torch.device(<span class="string">f&#x27;cuda:<span class="subst">&#123;i&#125;</span>&#x27;</span>)</span><br><span class="line">    <span class="keyword">return</span> torch.device(<span class="string">&#x27;cpu&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">try_all_gpus</span>():  <span class="comment">#@save</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;返回所有可用的GPU，如果没有GPU，则返回[cpu(),]&quot;&quot;&quot;</span></span><br><span class="line">    devices = [torch.device(<span class="string">f&#x27;cuda:<span class="subst">&#123;i&#125;</span>&#x27;</span>)</span><br><span class="line">             <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(torch.cuda.device_count())]</span><br><span class="line">    <span class="keyword">return</span> devices <span class="keyword">if</span> devices <span class="keyword">else</span> [torch.device(<span class="string">&#x27;cpu&#x27;</span>)]</span><br><span class="line"></span><br><span class="line">try_gpu(), try_gpu(<span class="number">10</span>), try_all_gpus()</span><br></pre></td></tr></table></figure><br>默认创建张量是在CPU上的<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">X = torch.ones(<span class="number">2</span>, <span class="number">3</span>, device=try_gpu())</span><br><span class="line">X</span><br></pre></td></tr></table></figure><br>可以在指定的gpu上创建张量：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Y = torch.rand(<span class="number">2</span>, <span class="number">3</span>, device=try_gpu(<span class="number">1</span>))</span><br><span class="line">Y</span><br></pre></td></tr></table></figure><br>这时，X在’cuda:0’上，Y在‘cuda:1’上，如果我们要计算<code>X + Y</code>，我们需要决定在哪里执行这个操作。们可以将<code>X</code>传输到第二个GPU并在那里执行操作。 <em>不要</em>简单地<code>X</code>加上<code>Y</code>，因为这会导致异常， 运行时引擎不知道该怎么做：它在同一设备上找不到数据会导致失败。 由于<code>Y</code>位于第二个GPU上，所以我们需要将<code>X</code>移到那里， 然后才能执行相加运算。<br>可以将X复制到与Y相同的GPU进行相加<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Z = X.cuda(<span class="number">1</span>)</span><br><span class="line">Y + Z</span><br></pre></td></tr></table></figure><br>将模型参数放在GPU上：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">net = nn.Sequential(nn.Linear(<span class="number">3</span>, <span class="number">1</span>))</span><br><span class="line">net = net.to(device=try_gpu())</span><br></pre></td></tr></table></figure></p><h3 id="练习-14"><a href="#练习-14" class="headerlink" title="练习"></a>练习</h3><ol><li>尝试一个计算量更大的任务，比如大矩阵的乘法，看看CPU和GPU之间的速度差异。再试一个计算量很小的任务呢？</li></ol><ul><li>大任务测试：<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="comment"># 设置矩阵大小</span></span><br><span class="line">size = <span class="number">5000</span></span><br><span class="line"><span class="comment"># 创建两个大矩阵</span></span><br><span class="line">a = torch.rand(size, size)</span><br><span class="line">b = torch.rand(size, size)</span><br><span class="line"><span class="comment"># 在CPU上进行矩阵乘法</span></span><br><span class="line">start_time = time.time()</span><br><span class="line">result_cpu = torch.matmul(a, b)</span><br><span class="line">cpu_time = time.time() - start_time</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;CPU time: <span class="subst">&#123;cpu_time:<span class="number">.6</span>f&#125;</span> seconds&quot;</span>)</span><br><span class="line"><span class="comment"># 如果可以使用GPU，进行相同的测试</span></span><br><span class="line"><span class="keyword">if</span> torch.cuda.is_available():</span><br><span class="line">    a = a.cuda()</span><br><span class="line">    b = b.cuda()</span><br><span class="line">    start_time = time.time()</span><br><span class="line">    result_gpu = torch.matmul(a, b)</span><br><span class="line">    gpu_time = time.time() - start_time</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;GPU time: <span class="subst">&#123;gpu_time:<span class="number">.6</span>f&#125;</span> seconds&quot;</span>)</span><br></pre></td></tr></table></figure>CPU time: 1.693618 seconds<br>GPU time: 0.643845 seconds<br>小任务：<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 设置矩阵大小</span></span><br><span class="line">size = <span class="number">100</span></span><br><span class="line"><span class="comment"># 创建两个小矩阵</span></span><br><span class="line">a = torch.rand(size, size)</span><br><span class="line">b = torch.rand(size, size)</span><br><span class="line"><span class="comment"># 在CPU上进行矩阵乘法</span></span><br><span class="line">start_time = time.time()</span><br><span class="line">result_cpu = torch.matmul(a, b)</span><br><span class="line">cpu_time = time.time() - start_time</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;CPU time (small task): <span class="subst">&#123;cpu_time:<span class="number">.6</span>f&#125;</span> seconds&quot;</span>)</span><br><span class="line"><span class="comment"># 如果可以使用GPU，进行相同的测试</span></span><br><span class="line"><span class="keyword">if</span> torch.cuda.is_available():</span><br><span class="line">    a = a.cuda()</span><br><span class="line">    b = b.cuda()</span><br><span class="line">    start_time = time.time()</span><br><span class="line">    result_gpu = torch.matmul(a, b)</span><br><span class="line">    gpu_time = time.time() - start_time</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;GPU time (small task): <span class="subst">&#123;gpu_time:<span class="number">.6</span>f&#125;</span> seconds&quot;</span>)</span><br></pre></td></tr></table></figure>CPU time (small task): 0.010012 seconds<br>GPU time (small task): 0.002001 seconds</li></ul><ol><li>我们应该如何在GPU上读写模型参数？<br>可以通过<em>model.parameters</em>获取模型终端 参数<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> param <span class="keyword">in</span> model.parameters(): <span class="built_in">print</span>(param)</span><br></pre></td></tr></table></figure></li><li>测量计算1000个100×100矩阵的矩阵乘法所需的时间，并记录输出矩阵的Frobenius范数，一次记录一个结果，而不是在GPU上保存日志并仅传输最终结果。<br>首先，Frobenius范数可以看作是矩阵中所有元素的平方和的平方根，类似于向量的欧几里得范数（L2范数）在矩阵上的推广。<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="comment"># 设置矩阵大小和数量</span></span><br><span class="line">matrix_size = <span class="number">100</span></span><br><span class="line">num_matrices = <span class="number">1000</span></span><br><span class="line">device = torch.device(<span class="string">&#x27;cuda&#x27;</span>)</span><br><span class="line">frobenius_norms = []</span><br><span class="line"><span class="comment"># 开始计时</span></span><br><span class="line">start_time = time.time()</span><br><span class="line"><span class="comment"># 进行矩阵乘法并记录Frobenius范数</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(num_matrices):</span><br><span class="line">    <span class="comment"># 创建两个随机矩阵并移动到GPU</span></span><br><span class="line">    a = torch.rand(matrix_size, matrix_size, device=device)</span><br><span class="line">    b = torch.rand(matrix_size, matrix_size, device=device)</span><br><span class="line">    <span class="comment"># 在GPU上进行矩阵乘法</span></span><br><span class="line">    result = torch.matmul(a, b)</span><br><span class="line">    <span class="comment"># 计算Frobenius范数并移动到CPU</span></span><br><span class="line">    frob_norm = torch.norm(result, p=<span class="string">&#x27;fro&#x27;</span>).cpu().item()</span><br><span class="line">    <span class="comment"># 记录结果</span></span><br><span class="line">    frobenius_norms.append(frob_norm)</span><br><span class="line"><span class="comment"># 结束计时</span></span><br><span class="line">end_time = time.time()</span><br><span class="line">total_time = end_time - start_time</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Total time for <span class="subst">&#123;num_matrices&#125;</span> matrix multiplications: <span class="subst">&#123;total_time:<span class="number">.6</span>f&#125;</span> seconds&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Frobenius norms of output matrices:&quot;</span>, frobenius_norms)</span><br><span class="line"></span><br></pre></td></tr></table></figure>Total time for 1000 matrix multiplications: 4.377934 seconds<h1 id="卷积神经网络"><a href="#卷积神经网络" class="headerlink" title="卷积神经网络"></a>卷积神经网络</h1><h2 id="全连接到卷积"><a href="#全连接到卷积" class="headerlink" title="全连接到卷积"></a>全连接到卷积</h2></li><li>为什么平移不变性可能也不是好主意呢？<br> 平移不变性意味着卷积层对输入数据的平移具有不变性，这在很多情况下是有益的，因为它允许模型识别对象，无论它们在图像中的位置如何。然而，在某些情况下，平移不变性可能不是一个好主意。例如，如果图像中对象的位置对于任务非常重要，那么平移不变性可能会导致信息丢失。</li><li>当从图像边界像素获取隐藏表示时，我们需要思考哪些问题？<br> 我们需要考虑边界效应和填充策略。由于卷积核在边界处可能无法完全覆盖所有像素，这可能导致边界像素的信息不足。为了解决这个问题，我们可以采用填充（padding）策略</li><li>描述一个类似的音频卷积层的架构。<br> 音频数据通常以一维时间序列的形式出现，因此音频卷积层通常使用一维卷积（1D convolution）。这种卷积层在时间轴上滑动卷积核，提取音频信号的局部特征。例如，一个音频卷积层可以由一系列一维卷积层组成，每个层使用不同大小的卷积核和步长来捕捉不同时间尺度上的特征。这类似于图像卷积层使用二维卷积提取空间特征。</li><li>卷积层也适合于文本数据吗？为什么？<br> 在文本处理中，卷积层可以用来捕捉词汇或短语的局部模式。例如，通过在嵌入层（embedding layer）之后应用一维卷积层，模型可以学习到文本中词语的组合特征。<h2 id="图像卷积"><a href="#图像卷积" class="headerlink" title="图像卷积"></a>图像卷积</h2><h3 id="互相关运算"><a href="#互相关运算" class="headerlink" title="互相关运算"></a>互相关运算</h3>在二维互相关运算中，卷积窗口从输入张量的左上角开始，从左到右、从上到下滑动。 当卷积窗口滑动到新一个位置时，包含在该窗口中的部分张量与卷积核张量进行按元素相乘，得到的张量再求和得到一个单一的标量值，由此我们得出了这一位置的输出张量值。<h3 id="卷积层"><a href="#卷积层" class="headerlink" title="卷积层"></a>卷积层</h3>卷积层中的两个被训练的参数是卷积核权重和标量偏置。 就像我们之前随机初始化全连接层一样，在训练基于卷积层的模型时，我们也随机初始化卷积核权重。<h3 id="感受野"><a href="#感受野" class="headerlink" title="感受野"></a>感受野</h3>在卷积神经网络中，对于某一层的任意元素x，其_感受野（receptive field）是指在前向传播期间可能影响x计算的所有元素（来自所有先前层）。<h3 id="练习-15"><a href="#练习-15" class="headerlink" title="练习"></a>练习</h3></li><li>在我们创建的<code>Conv2D</code>自动求导时，有什么错误消息？<br> 没有使用PyTorch的操作来确保梯度可以被追踪</li><li>如何通过改变输入张量和卷积核张量，将互相关运算表示为矩阵乘法？<br> 把输入和卷积展平，卷积核每滑动一个步幅取一个样本。</li><li>手工设计一些卷积核。<ol><li>二阶导数的核的形式是什么？<br> 二阶导数卷积核就是拉普拉斯算子[[0,1,0],[1,-4,1],[0,1,0]]  </li><li>积分的核的形式是什么？<br> d+1<h2 id="填充与步幅"><a href="#填充与步幅" class="headerlink" title="填充与步幅"></a>填充与步幅</h2><h3 id="通用公式"><a href="#通用公式" class="headerlink" title="通用公式"></a>通用公式</h3><script type="math/tex; mode=display">o = [( i + 2p - k) / s] + 1</script>其中：</li></ol></li></ol><ul><li>i：输入尺寸input</li><li>o：输出output</li><li>s：步长stride、</li><li>p：填充padding（一般都是零）</li><li>k：卷积核（kernel）大小<h3 id="展开公式"><a href="#展开公式" class="headerlink" title="展开公式"></a>展开公式</h3><script type="math/tex; mode=display">[(n_h-k_h+p_h+s_h)/s_h]*[(n_w-k_w+p_w+s_w)/s_w]</script><h2 id="多输入多输出通道"><a href="#多输入多输出通道" class="headerlink" title="多输入多输出通道"></a>多输入多输出通道</h2><h3 id="多输入通道"><a href="#多输入通道" class="headerlink" title="多输入通道"></a>多输入通道</h3>当输入包含多个通道时，需要构造一个与输入数据具有相同输入通道数的卷积核，以便与输入数据进行互相关运算。<h3 id="多输出通道"><a href="#多输出通道" class="headerlink" title="多输出通道"></a>多输出通道</h3>在最流行的神经网络架构中，随着神经网络层数的加深，我们常会增加输出通道的维数，通过减少空间分辨率以获得更大的通道深度。直观地说，我们可以将每个通道看作对不同特征的响应。而现实可能更为复杂一些，因为每个通道不是独立学习的，而是为了共同使用而优化的。因此，多输出通道并不仅是学习多个单通道的检测器。用$c_i$和$c_o$分别表示输入和输出通道的数目，并让$k_h$和$k_w$为卷积核的高度和宽度。为了获得多个通道的输出，我们可以为每个输出通道创建一个形状为$c_i<em>k_h</em>k_w$的卷积核张量，这样卷积核的形状是$c_o<em>c_i</em>k_h*k_w$。在互相关运算中，每个输出通道先获取所有输入通道，再以对应该输出通道的卷积核计算出结果。<h3 id="1-1卷积"><a href="#1-1卷积" class="headerlink" title="1 * 1卷积"></a>1 * 1卷积</h3><h2 id="汇聚层（pooling池化层）"><a href="#汇聚层（pooling池化层）" class="headerlink" title="汇聚层（pooling池化层）"></a>汇聚层（pooling池化层）</h2>它具有双重目的：降低卷积层对位置的敏感性，同时降低对空间降采样表示的敏感性。<h3 id="最大汇聚层和平均汇聚层"><a href="#最大汇聚层和平均汇聚层" class="headerlink" title="最大汇聚层和平均汇聚层"></a>最大汇聚层和平均汇聚层</h3>与卷积层类似，汇聚层运算符由一个固定形状的窗口组成，该窗口根据其步幅大小在输入的所有区域上滑动，为固定形状窗口（有时称为<em>汇聚窗口</em>）遍历的每个位置计算一个输出。 然而，不同于卷积层中的输入与卷积核之间的互相关计算，汇聚层不包含参数。 相反，池运算是确定性的，我们通常计算汇聚窗口中所有元素的最大值或平均值。这些操作分别称为<em>最大汇聚层</em>（maximum pooling）和<em>平均汇聚层</em>（average pooling）。</li></ul><p>在这两种情况下，与互相关运算符一样，汇聚窗口从输入张量的左上角开始，从左往右、从上往下的在输入张量内滑动。在汇聚窗口到达的每个位置，它计算该窗口中输入子张量的最大值或平均值。计算最大值或平均值是取决于使用了最大汇聚层还是平均汇聚层。</p><h3 id="练习-16"><a href="#练习-16" class="headerlink" title="练习"></a>练习</h3><p>除了平均汇聚层和最大汇聚层，是否有其它函数可以考虑（提示：回想一下softmax）？为什么它不流行？<br>除了平均汇聚层（Average Pooling）和最大汇聚层（Max Pooling），还可以考虑使用Softmax汇聚层。Softmax汇聚层是一种在汇聚操作中应用Softmax函数的方法，它可以为汇聚区域内的每个像素分配一个权重，这些权重根据像素的相对大小进行加权和。<br>Softmax汇聚层的一个潜在优点是它可以提供一种更加灵活和可学习的汇聚机制，因为它根据输入的特征自适应地调整汇聚区域内像素的权重。不流行的原因有：计算复杂度，训练困难（梯度小时或梯度爆炸），解释性差。</p><h2 id="卷积神经网络（LeNet）"><a href="#卷积神经网络（LeNet）" class="headerlink" title="卷积神经网络（LeNet）"></a>卷积神经网络（LeNet）</h2><ul><li>卷积编码器：由两个卷积层组成;</li><li>全连接层密集块：由三个全连接层组成。<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br><span class="line"></span><br><span class="line">net = nn.Sequential(</span><br><span class="line">    nn.Conv2d(<span class="number">1</span>, <span class="number">6</span>, kernel_size=<span class="number">5</span>, padding=<span class="number">2</span>), nn.Sigmoid(),</span><br><span class="line">    nn.AvgPool2d(kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>),</span><br><span class="line">    nn.Conv2d(<span class="number">6</span>, <span class="number">16</span>, kernel_size=<span class="number">5</span>), nn.Sigmoid(),</span><br><span class="line">    nn.AvgPool2d(kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>),</span><br><span class="line">    nn.Flatten(),</span><br><span class="line">    nn.Linear(<span class="number">16</span> * <span class="number">5</span> * <span class="number">5</span>, <span class="number">120</span>), nn.Sigmoid(),</span><br><span class="line">    nn.Linear(<span class="number">120</span>, <span class="number">84</span>), nn.Sigmoid(),</span><br><span class="line">    nn.Linear(<span class="number">84</span>, <span class="number">10</span>))</span><br><span class="line">X = torch.rand(size=(<span class="number">1</span>, <span class="number">1</span>, <span class="number">28</span>, <span class="number">28</span>), dtype=torch.float32)</span><br><span class="line"><span class="keyword">for</span> layer <span class="keyword">in</span> net:</span><br><span class="line">    X = layer(X)</span><br><span class="line">    <span class="built_in">print</span>(layer.__class__.__name__,<span class="string">&#x27;output shape:\t&#x27;</span>,X.shape)</span><br></pre></td></tr></table></figure></li></ul><h3 id="练习-17"><a href="#练习-17" class="headerlink" title="练习"></a>练习</h3><ol><li>将平均汇聚层替换为最大汇聚层，会发生什么？<br> 导致模型捕捉更加显著的特征，因为最大汇聚层倾向于保留最强的信号，而忽略较弱的信号。这种变化可能会提高模型的性能，特别是在处理具有明显特征的图像时。</li><li>尝试构建一个基于LeNet的更复杂的网络，以提高其准确性。<ol><li>调整卷积窗口大小。</li><li>调整输出通道的数量。</li><li>调整激活函数（如ReLU）。</li><li>调整卷积层的数量。</li><li>调整全连接层的数量。</li><li>调整学习率和其他训练细节（例如，初始化和轮数）。<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">ImprovedLeNet</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(ImprovedLeNet, self).__init__()</span><br><span class="line">        self.conv1 = nn.Conv2d(<span class="number">1</span>, <span class="number">16</span>, kernel_size=<span class="number">5</span>, stride=<span class="number">1</span>, padding=<span class="number">2</span>) </span><br><span class="line">        self.conv2 = nn.Conv2d(<span class="number">16</span>, <span class="number">32</span>, kernel_size=<span class="number">5</span>, stride=<span class="number">1</span>, padding=<span class="number">2</span>) </span><br><span class="line">        self.conv3 = nn.Conv2d(<span class="number">32</span>, <span class="number">64</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>)</span><br><span class="line">        self.pool = nn.MaxPool2d(kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>)</span><br><span class="line">        self.fc1 = nn.Linear(<span class="number">64</span> * <span class="number">7</span> * <span class="number">7</span>, <span class="number">120</span>)</span><br><span class="line">        self.fc2 = nn.Linear(<span class="number">120</span>, <span class="number">84</span>)</span><br><span class="line">        self.fc3 = nn.Linear(<span class="number">84</span>, <span class="number">10</span>)</span><br><span class="line">        self.relu = nn.ReLU()</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = self.pool(self.relu(self.conv1(x)))</span><br><span class="line">        x = self.pool(self.relu(self.conv2(x)))</span><br><span class="line">        x = self.pool(self.relu(self.conv3(x)))</span><br><span class="line">        x = x.view(-<span class="number">1</span>, <span class="number">64</span> * <span class="number">7</span> * <span class="number">7</span>)</span><br><span class="line">        x = self.relu(self.fc1(x))</span><br><span class="line">        x = self.relu(self.fc2(x))</span><br><span class="line">        x = self.fc3(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line">model = ImprovedLeNet()</span><br><span class="line"></span><br><span class="line">criterion = nn.CrossEntropyLoss()</span><br><span class="line">optimizer = optim.Adam(model.parameters(), lr=<span class="number">0.001</span>)  </span><br></pre></td></tr></table></figure></li></ol></li></ol><h1 id="现代卷积网络"><a href="#现代卷积网络" class="headerlink" title="现代卷积网络"></a>现代卷积网络</h1><h2 id="深度卷积神经网络"><a href="#深度卷积神经网络" class="headerlink" title="深度卷积神经网络"></a>深度卷积神经网络</h2><p>在计算机视觉中，直接将神经网络与其他机器学习方法进行比较也许不公平。这是因为，卷积神经网络的输入是由原始像素值或是经过简单预处理（例如居中、缩放）的像素值组成的。但在使用传统机器学习方法时，从业者永远不会将原始像素作为输入。在传统机器学习方法中，计算机视觉流水线是由经过人的手工精心设计的特征流水线组成的。对于这些传统方法，大部分的进展都来自于对特征有了更聪明的想法，并且学习到的算法往往归于事后的解释。</p><h3 id="Alexnet"><a href="#Alexnet" class="headerlink" title="Alexnet"></a>Alexnet</h3><p>在AlexNet的第一层，卷积窗口的形状是11×11。 由于ImageNet中大多数图像的宽和高比MNIST图像的多10倍以上，因此，需要一个更大的卷积窗口来捕获目标。 第二层中的卷积窗口形状被缩减为5×5，然后是3×3。 此外，在第一层、第二层和第五层卷积层之后，加入窗口形状为3×3、步幅为2的最大汇聚层。 而且，AlexNet的卷积通道数目是LeNet的10倍。<br>在最后一个卷积层后有两个全连接层，分别有4096个输出。 这两个巨大的全连接层拥有将近1GB的模型参数。<br>此外，AlexNet将sigmoid激活函数改为更简单的ReLU激活函数。 一方面，ReLU激活函数的计算更简单，它不需要如sigmoid激活函数那般复杂的求幂运算。<br>AlexNet通过暂退法（dropout）控制全连接层的模型复杂度，而LeNet只使用了权重衰减。</p><h3 id="练习-18"><a href="#练习-18" class="headerlink" title="练习"></a>练习</h3><ol><li>AlexNet对Fashion-MNIST数据集来说可能太复杂了。<ol><li>尝试简化模型以加快训练速度，同时确保准确性不会显著下降。</li><li>设计一个更好的模型，可以直接在28×28图像上工作。<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">class</span> <span class="title class_">SimpleCNN</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(SimpleCNN, self).__init__()</span><br><span class="line">        self.conv1 = nn.Conv2d(<span class="number">1</span>, <span class="number">32</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>)</span><br><span class="line">        self.conv2 = nn.Conv2d(<span class="number">32</span>, <span class="number">64</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>)</span><br><span class="line">        self.pool = nn.MaxPool2d(kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>, padding=<span class="number">0</span>)</span><br><span class="line">        self.fc1 = nn.Linear(<span class="number">64</span> * <span class="number">7</span> * <span class="number">7</span>, <span class="number">128</span>)</span><br><span class="line">        self.fc2 = nn.Linear(<span class="number">128</span>, <span class="number">10</span>)</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = F.relu(self.conv1(x))</span><br><span class="line">        x = self.pool(x)</span><br><span class="line">        x = F.relu(self.conv2(x))</span><br><span class="line">        x = self.pool(x)</span><br><span class="line">        x = x.view(-<span class="number">1</span>, <span class="number">64</span> * <span class="number">7</span> * <span class="number">7</span>)</span><br><span class="line">        x = F.relu(self.fc1(x))</span><br><span class="line">        x = self.fc2(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line">model = SimpleCNN()</span><br><span class="line"><span class="built_in">print</span>(model)</span><br></pre></td></tr></table></figure></li></ol></li><li>分析了AlexNet的计算性能。<ol><li>在AlexNet中主要是哪部分占用显存？<br> 全连接的权重和偏置</li><li>在AlexNet中主要是哪部分需要更多的计算？<br> 卷积层</li><li>计算结果时显存带宽如何？<br>显存带宽决定了数据（如权重、激活值和梯度）在GPU内存和计算单元之间传输的速度<h2 id="使用块的网络（VGG）"><a href="#使用块的网络（VGG）" class="headerlink" title="使用块的网络（VGG）"></a>使用块的网络（VGG）</h2><h3 id="VGG块"><a href="#VGG块" class="headerlink" title="VGG块"></a>VGG块</h3>经典卷积神经网络基本组成部分：</li></ol></li><li>卷积层</li><li>激活函数</li><li><p>pooling汇聚层<br>vggblock的实现：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">vgg_block</span>(<span class="params">num_convs, in_channels, out_channels</span>):</span><br><span class="line">    layers = []</span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(num_convs):</span><br><span class="line">        layers.append(nn.Conv2d(in_channels, out_channels,</span><br><span class="line">                                kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>))</span><br><span class="line">        layers.append(nn.ReLU())</span><br><span class="line">        in_channels = out_channels</span><br><span class="line">    layers.append(nn.MaxPool2d(kernel_size=<span class="number">2</span>,stride=<span class="number">2</span>))</span><br><span class="line">    <span class="keyword">return</span> nn.Sequential(*layers)</span><br></pre></td></tr></table></figure><p>接受三个参数，分别对应于卷积层的数量num_convs、输入通道的数量in_channels和输出通道的数量out_channels。由于该网络使用8个卷积层和3个全连接层，因此它通常被称为VGG-11。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">vgg</span>(<span class="params">conv_arch</span>):</span><br><span class="line">    conv_blks = []</span><br><span class="line">    in_channels = <span class="number">1</span></span><br><span class="line">    <span class="comment"># 卷积层部分</span></span><br><span class="line">    <span class="keyword">for</span> (num_convs, out_channels) <span class="keyword">in</span> conv_arch:</span><br><span class="line">        conv_blks.append(vgg_block(num_convs, in_channels, out_channels))</span><br><span class="line">        in_channels = out_channels</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> nn.Sequential(</span><br><span class="line">        *conv_blks, nn.Flatten(),</span><br><span class="line">        <span class="comment"># 全连接层部分</span></span><br><span class="line">        nn.Linear(out_channels * <span class="number">7</span> * <span class="number">7</span>, <span class="number">4096</span>), nn.ReLU(), nn.Dropout(<span class="number">0.5</span>),</span><br><span class="line">        nn.Linear(<span class="number">4096</span>, <span class="number">4096</span>), nn.ReLU(), nn.Dropout(<span class="number">0.5</span>),</span><br><span class="line">        nn.Linear(<span class="number">4096</span>, <span class="number">10</span>))</span><br><span class="line"></span><br><span class="line">net = vgg(conv_arch)</span><br></pre></td></tr></table></figure><p>可以看出，在定义网络时，可以先用列表来存储（append）块卷积层，最后再return nn.sequential(*list)。<br>其中超参数conv_arch定义了每个VGG块李卷积层的个数和输出通道数。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conv_arch = ((<span class="number">1</span>, <span class="number">64</span>), (<span class="number">1</span>, <span class="number">128</span>), (<span class="number">2</span>, <span class="number">256</span>), (<span class="number">2</span>, <span class="number">512</span>), (<span class="number">2</span>, <span class="number">512</span>))</span><br></pre></td></tr></table></figure></li><li><p>打印层的尺寸时，我们只看到8个结果，而不是11个结果。剩余的3层信息去哪了？<br> 后三层的2个卷积看成了一个块，所以只有8层</p></li><li>与AlexNet相比，VGG的计算要慢得多，而且它还需要更多的显存。分析出现这种情况的原因。<br> VGG的网络更深，参数量更大，并且VGG的线性层参数更多</li><li><p>尝试将Fashion-MNIST数据集图像的高度和宽度从224改为96。这对实验有什么影响？</p></li><li><p>请参考VGG论文中的表1构建其他常见模型，如VGG-16或VGG-19。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">vgg_block</span>(<span class="params">num_convs, in_channels, out_channels</span>):</span><br><span class="line">    layers = []</span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(num_convs):</span><br><span class="line">        layers.append(nn.Conv2d(in_channels, out_channels,</span><br><span class="line">                                kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>))</span><br><span class="line">        layers.append(nn.ReLU())</span><br><span class="line"></span><br><span class="line">        in_channels = out_channels</span><br><span class="line">    <span class="keyword">if</span> out_channels &gt;= <span class="number">256</span>:</span><br><span class="line">        layers.append(nn.Conv2d(out_channels, out_channels, kernel_size=<span class="number">1</span>))</span><br><span class="line">        layers.append(nn.ReLU())</span><br><span class="line">    layers.append(nn.MaxPool2d(kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>))</span><br><span class="line">    <span class="keyword">return</span> nn.Sequential(*layers)</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">vgg16</span>(<span class="params">conv_arch</span>):</span><br><span class="line">    conv_blks = []</span><br><span class="line">    in_channels = <span class="number">1</span></span><br><span class="line">    <span class="comment"># 卷积层部分</span></span><br><span class="line">    <span class="keyword">for</span> (num_convs, out_channels) <span class="keyword">in</span> conv_arch:</span><br><span class="line">        conv_blks.append(vgg_block(num_convs, in_channels, out_channels))</span><br><span class="line">        in_channels = out_channels</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> nn.Sequential(</span><br><span class="line">        *conv_blks, nn.Flatten(),</span><br><span class="line">        <span class="comment"># 全连接层部分</span></span><br><span class="line">        nn.Linear(out_channels * <span class="number">7</span> * <span class="number">7</span>, <span class="number">4096</span>), nn.ReLU(), nn.Dropout(<span class="number">0.5</span>),</span><br><span class="line">        nn.Linear(<span class="number">4096</span>, <span class="number">4096</span>), nn.ReLU(), nn.Dropout(<span class="number">0.5</span>),</span><br><span class="line">        nn.Linear(<span class="number">4096</span>, <span class="number">10</span>))</span><br><span class="line">conv_arch = ((<span class="number">2</span>, <span class="number">64</span>), (<span class="number">2</span>, <span class="number">128</span>), (<span class="number">2</span>, <span class="number">256</span>), (<span class="number">2</span>, <span class="number">512</span>), (<span class="number">2</span>, <span class="number">512</span>))</span><br><span class="line">ratio = <span class="number">4</span></span><br><span class="line">small_conv_arch = [(pair[<span class="number">0</span>], pair[<span class="number">1</span>] // ratio) <span class="keyword">for</span> pair <span class="keyword">in</span> conv_arch]</span><br><span class="line">net = vgg16(small_conv_arch)</span><br></pre></td></tr></table></figure><h2 id="网络中的网络（NiN）"><a href="#网络中的网络（NiN）" class="headerlink" title="网络中的网络（NiN）"></a>网络中的网络（NiN）</h2><p>LeNet、AlexNet和VGG都有一个共同的设计模式：通过一系列的卷积层与汇聚层来提取空间结构特征；然后通过全连接层对特征的表征进行处理。 AlexNet和VGG对LeNet的改进主要在于如何扩大和加深这两个模块。 或者，可以想象在这个过程的早期使用全连接层。然而，如果使用了全连接层，可能会完全放弃表征的空间结构。 <em>网络中的网络</em>（<em>NiN</em>）提供了一个非常简单的解决方案：在每个像素的通道上分别使用多层感知机</p><h3 id="NiN块"><a href="#NiN块" class="headerlink" title="NiN块"></a>NiN块</h3><p>卷积层的输入和输出由四维张量组成，张量的每个轴分别对应样本，通道，高度和宽度。另外全连接额层的输入和输出时对应于样本和特征的二维张量。</p></li></ol><ul><li>NiN使用由一个卷积层和多个1×1卷积层组成的块。该块可以在卷积神经网络中使用，以允许更多的每像素非线性。</li><li>NiN去除了容易造成过拟合的全连接层，将它们替换为全局平均汇聚层（即在所有位置上进行求和）。该汇聚层通道数量为所需的输出数量（例如，Fashion-MNIST的输出为10）。</li><li>移除全连接层可减少过拟合，同时显著减少NiN的参数。</li><li>NiN的设计影响了许多后续卷积神经网络的设计。<h3 id="练习-19"><a href="#练习-19" class="headerlink" title="练习"></a>练习</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">nin_block</span>(<span class="params">in_channels, out_channels, kernel_size, strides, padding</span>):</span><br><span class="line">    <span class="keyword">return</span> nn.Sequential(</span><br><span class="line">        nn.Conv2d(in_channels, out_channels, kernel_size, strides, padding),</span><br><span class="line">        nn.ReLU(),</span><br><span class="line">        nn.Conv2d(out_channels, out_channels, kernel_size=<span class="number">1</span>), nn.ReLU(),</span><br><span class="line">        nn.Conv2d(out_channels, out_channels, kernel_size=<span class="number">1</span>), nn.ReLU())</span><br><span class="line"></span><br><span class="line">net = nn.Sequential(</span><br><span class="line">    nin_block(<span class="number">1</span>, <span class="number">96</span>, kernel_size=<span class="number">11</span>, strides=<span class="number">4</span>, padding=<span class="number">0</span>),</span><br><span class="line">    nn.MaxPool2d(<span class="number">3</span>, stride=<span class="number">2</span>),</span><br><span class="line">    nin_block(<span class="number">96</span>, <span class="number">256</span>, kernel_size=<span class="number">5</span>, strides=<span class="number">1</span>, padding=<span class="number">2</span>),</span><br><span class="line">    nn.MaxPool2d(<span class="number">3</span>, stride=<span class="number">2</span>),</span><br><span class="line">    nin_block(<span class="number">256</span>, <span class="number">384</span>, kernel_size=<span class="number">3</span>, strides=<span class="number">1</span>, padding=<span class="number">1</span>),</span><br><span class="line">    nn.MaxPool2d(<span class="number">3</span>, stride=<span class="number">2</span>),</span><br><span class="line">    nn.Dropout(<span class="number">0.5</span>),</span><br><span class="line">    <span class="comment"># 标签类别数是10</span></span><br><span class="line">    nin_block(<span class="number">384</span>, <span class="number">10</span>, kernel_size=<span class="number">3</span>, strides=<span class="number">1</span>, padding=<span class="number">1</span>),</span><br><span class="line">    nn.AdaptiveAvgPool2d((<span class="number">1</span>, <span class="number">1</span>)),</span><br><span class="line">    <span class="comment"># 将四维的输出转成二维的输出，其形状为(批量大小,10)</span></span><br><span class="line">    nn.Flatten())</span><br></pre></td></tr></table></figure></li></ul><h2 id="含并行连结的网络（googleNet）"><a href="#含并行连结的网络（googleNet）" class="headerlink" title="含并行连结的网络（googleNet）"></a>含并行连结的网络（googleNet）</h2><p>在GoogLeNet中，基本的卷积块被称为<em>Inception块</em>（Inception block）。<br>![[Pasted image 20240402145836.png]]<br>inception块由四条并行路径组成，前三条路径使用1 <em> 1，3 </em> 3，5 <em> 5，卷积层，从不同空间大小中提取信息。中间的两条路径在输入上执行1 </em> 1卷积，以减少通道数，降低模型复杂性。第四条路径使用3 <em> 3最大汇聚层，然后使用1 </em> 1卷积层来改变通道数。这四条路径使用合适的填充使输入和输出的高和宽一直，最后我们将每条线路的输出在通道维度上连接，构成Inception块的输出。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.nn <span class="keyword">import</span> functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Inception</span>(nn.Module):</span><br><span class="line">    <span class="comment"># c1--c4是每条路径的输出通道数</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, in_channels, c1, c2, c3, c4, **kwargs</span>):</span><br><span class="line">        <span class="built_in">super</span>(Inception, self).__init__(**kwargs)</span><br><span class="line">        <span class="comment"># 线路1，单1x1卷积层</span></span><br><span class="line">        self.p1_1 = nn.Conv2d(in_channels, c1, kernel_size=<span class="number">1</span>)</span><br><span class="line">        <span class="comment"># 线路2，1x1卷积层后接3x3卷积层</span></span><br><span class="line">        self.p2_1 = nn.Conv2d(in_channels, c2[<span class="number">0</span>], kernel_size=<span class="number">1</span>)</span><br><span class="line">        self.p2_2 = nn.Conv2d(c2[<span class="number">0</span>], c2[<span class="number">1</span>], kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>)</span><br><span class="line">        <span class="comment"># 线路3，1x1卷积层后接5x5卷积层</span></span><br><span class="line">        self.p3_1 = nn.Conv2d(in_channels, c3[<span class="number">0</span>], kernel_size=<span class="number">1</span>)</span><br><span class="line">        self.p3_2 = nn.Conv2d(c3[<span class="number">0</span>], c3[<span class="number">1</span>], kernel_size=<span class="number">5</span>, padding=<span class="number">2</span>)</span><br><span class="line">        <span class="comment"># 线路4，3x3最大汇聚层后接1x1卷积层</span></span><br><span class="line">        self.p4_1 = nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>)</span><br><span class="line">        self.p4_2 = nn.Conv2d(in_channels, c4, kernel_size=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        p1 = F.relu(self.p1_1(x))</span><br><span class="line">        p2 = F.relu(self.p2_2(F.relu(self.p2_1(x))))</span><br><span class="line">        p3 = F.relu(self.p3_2(F.relu(self.p3_1(x))))</span><br><span class="line">        p4 = F.relu(self.p4_2(self.p4_1(x)))</span><br><span class="line">        <span class="comment"># 在通道维度上连结输出</span></span><br><span class="line">        <span class="keyword">return</span> torch.cat((p1, p2, p3, p4), dim=<span class="number">1</span>)</span><br></pre></td></tr></table></figure></p><h3 id="googlenet模型"><a href="#googlenet模型" class="headerlink" title="googlenet模型"></a>googlenet模型</h3><p>GoogLeNet一共使用9个Inception块和全局平均汇聚层的堆叠来生成其估计值。Inception块之间的最大汇聚层可降低维度。<br>![[Pasted image 20240402150750.png]]<br>模块逐渐实现：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">b1 = nn.Sequential(nn.Conv2d(<span class="number">1</span>, <span class="number">64</span>, kernel_size=<span class="number">7</span>, stride=<span class="number">2</span>, padding=<span class="number">3</span>),</span><br><span class="line">                   nn.ReLU(),</span><br><span class="line">                   nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>))</span><br></pre></td></tr></table></figure><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">b2 = nn.Sequential(nn.Conv2d(<span class="number">64</span>, <span class="number">64</span>, kernel_size=<span class="number">1</span>),</span><br><span class="line">                   nn.ReLU(),</span><br><span class="line">                   nn.Conv2d(<span class="number">64</span>, <span class="number">192</span>, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>),</span><br><span class="line">                   nn.ReLU(),</span><br><span class="line">                   nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">b3 = nn.Sequential(Inception(<span class="number">192</span>, <span class="number">64</span>, (<span class="number">96</span>, <span class="number">128</span>), (<span class="number">16</span>, <span class="number">32</span>), <span class="number">32</span>),</span><br><span class="line">                   Inception(<span class="number">256</span>, <span class="number">128</span>, (<span class="number">128</span>, <span class="number">192</span>), (<span class="number">32</span>, <span class="number">96</span>), <span class="number">64</span>),</span><br><span class="line">                   nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">b4 = nn.Sequential(Inception(<span class="number">480</span>, <span class="number">192</span>, (<span class="number">96</span>, <span class="number">208</span>), (<span class="number">16</span>, <span class="number">48</span>), <span class="number">64</span>),</span><br><span class="line">                   Inception(<span class="number">512</span>, <span class="number">160</span>, (<span class="number">112</span>, <span class="number">224</span>), (<span class="number">24</span>, <span class="number">64</span>), <span class="number">64</span>),</span><br><span class="line">                   Inception(<span class="number">512</span>, <span class="number">128</span>, (<span class="number">128</span>, <span class="number">256</span>), (<span class="number">24</span>, <span class="number">64</span>), <span class="number">64</span>),</span><br><span class="line">                   Inception(<span class="number">512</span>, <span class="number">112</span>, (<span class="number">144</span>, <span class="number">288</span>), (<span class="number">32</span>, <span class="number">64</span>), <span class="number">64</span>),</span><br><span class="line">                   Inception(<span class="number">528</span>, <span class="number">256</span>, (<span class="number">160</span>, <span class="number">320</span>), (<span class="number">32</span>, <span class="number">128</span>), <span class="number">128</span>),</span><br><span class="line">                   nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">b5 = nn.Sequential(Inception(<span class="number">832</span>, <span class="number">256</span>, (<span class="number">160</span>, <span class="number">320</span>), (<span class="number">32</span>, <span class="number">128</span>), <span class="number">128</span>),</span><br><span class="line">                   Inception(<span class="number">832</span>, <span class="number">384</span>, (<span class="number">192</span>, <span class="number">384</span>), (<span class="number">48</span>, <span class="number">128</span>), <span class="number">128</span>),</span><br><span class="line">                   nn.AdaptiveAvgPool2d((<span class="number">1</span>,<span class="number">1</span>)),</span><br><span class="line">                   nn.Flatten())</span><br><span class="line"></span><br><span class="line">net = nn.Sequential(b1, b2, b3, b4, b5, nn.Linear(<span class="number">1024</span>, <span class="number">10</span>))</span><br></pre></td></tr></table></figure></p><h3 id="练习-20"><a href="#练习-20" class="headerlink" title="练习"></a>练习</h3><ol><li>使用GoogLeNet的最小图像大小是多少？<br> 224 * 224</li><li>将AlexNet、VGG和NiN的模型参数大小与GoogLeNet进行比较。后两个网络架构是如何显著减少模型参数大小的？<br> AlexNet：6000万参数<br> VGG16：1.38亿参数<br> NiN：很少（1 <em> 1卷积）<br> 相比之下，<em>*GoogLeNet</em></em>引入了Inception模块，这些模块可以在不增加太多参数的情况下增加网络的深度和宽度。<h2 id="批量规范化"><a href="#批量规范化" class="headerlink" title="批量规范化"></a>批量规范化</h2>这是一种流行且有效的技术，可持续加速深层网络的收敛速度。结合残差块，可以训练100层以上的网络。<br>在训练神经网络时出现的一些实际挑战：<br>首先，数据预处理的方式会对最终结果产生巨大影响。<br>第二，对于典型的多层感知机或卷积神经网络，当我们训练时，中间层的变量可能有更广的变化范围。不论是沿着从输入到输出的层，跨同一层中的单元，或是随着时间的推移，模型参数会随着训练更新而变换莫测。假设这些变量分布中的这种偏移可能会阻碍网络的收敛，直观来说，如果一个层的可变值是另一个层的100倍，者可能需要对学习率进行补偿调整。<br>第三，更深层的网络很复杂，容易过拟合。这意味着正则化变得重要。</li></ol><p>批量规范化用于单个可选层（也可以应用到所有的层），原理：在每次训练迭代中，我们首先规范化输入，同伙减去其均值并除以标准差，其中两者均基于当前小批量处理。接下来，我们应用比例系数和比例偏移.正是由于这个基于批量统计的标准化，才有了批量规范化的名称。<br>请注意，如果我们尝试使用大小为1的小批量应用批量规范化，我们将无法学到任何东西，在减去均值后，这个单元将为0。所以，只有使用足够大的小批量，批量规范化才是有效且稳定的。在应用批量规范化时，批量的大小选择可能比没有批量规范化时重要。<br>从形式上来说，用$x$表示来自一个小批量的输入，批量规范化BN根据以下表达式转换x：</p><script type="math/tex; mode=display">BN(x)=\gamma \odot \frac{x-\mu_B}{\sigma_B}+\beta</script><h3 id="从零实现"><a href="#从零实现" class="headerlink" title="从零实现"></a>从零实现</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">batch_norm</span>(<span class="params">X, gamma, beta, moving_mean, moving_var, eps, momentum</span>):</span><br><span class="line">    <span class="comment"># 通过is_grad_enabled来判断当前模式是训练模式还是预测模式</span></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> torch.is_grad_enabled():</span><br><span class="line">        <span class="comment"># 如果是在预测模式下，直接使用传入的移动平均所得的均值和方差</span></span><br><span class="line">        X_hat = (X - moving_mean) / torch.sqrt(moving_var + eps)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">assert</span> <span class="built_in">len</span>(X.shape) <span class="keyword">in</span> (<span class="number">2</span>, <span class="number">4</span>)</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">len</span>(X.shape) == <span class="number">2</span>:</span><br><span class="line">            <span class="comment"># 使用全连接层的情况，计算特征维上的均值和方差</span></span><br><span class="line">            mean = X.mean(dim=<span class="number">0</span>)</span><br><span class="line">            var = ((X - mean) ** <span class="number">2</span>).mean(dim=<span class="number">0</span>)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="comment"># 使用二维卷积层的情况，计算通道维上（axis=1）的均值和方差。</span></span><br><span class="line">            <span class="comment"># 这里我们需要保持X的形状以便后面可以做广播运算</span></span><br><span class="line">            mean = X.mean(dim=(<span class="number">0</span>, <span class="number">2</span>, <span class="number">3</span>), keepdim=<span class="literal">True</span>)</span><br><span class="line">            var = ((X - mean) ** <span class="number">2</span>).mean(dim=(<span class="number">0</span>, <span class="number">2</span>, <span class="number">3</span>), keepdim=<span class="literal">True</span>)</span><br><span class="line">        <span class="comment"># 训练模式下，用当前的均值和方差做标准化</span></span><br><span class="line">        X_hat = (X - mean) / torch.sqrt(var + eps)</span><br><span class="line">        <span class="comment"># 更新移动平均的均值和方差</span></span><br><span class="line">        moving_mean = momentum * moving_mean + (<span class="number">1.0</span> - momentum) * mean</span><br><span class="line">        moving_var = momentum * moving_var + (<span class="number">1.0</span> - momentum) * var</span><br><span class="line">    Y = gamma * X_hat + beta  <span class="comment"># 缩放和移位</span></span><br><span class="line">    <span class="keyword">return</span> Y, moving_mean.data, moving_var.data</span><br></pre></td></tr></table></figure><h3 id="简明实现"><a href="#简明实现" class="headerlink" title="简明实现"></a>简明实现</h3><p>使用Batchnorm2d(num_feature)<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nn.Conv2d(<span class="number">1</span>, <span class="number">6</span>, kernel_size=<span class="number">5</span>), nn.BatchNorm2d(<span class="number">6</span>), nn.Sigmoid(),</span><br></pre></td></tr></table></figure><br>使用方法为输入上一层通道数即可。</p><h3 id="争议"><a href="#争议" class="headerlink" title="争议"></a>争议</h3><p>直观地说，批量规范化被认为可以使优化更加平滑。 然而，我们必须小心区分直觉和对我们观察到的现象的真实解释。 回想一下，我们甚至不知道简单的神经网络（多层感知机和传统的卷积神经网络）为什么如此有效。 即使在暂退法和权重衰减的情况下，它们仍然非常灵活，因此无法通过常规的学习理论泛化保证来解释它们是否能够泛化到看不见的数据。</p><h3 id="注："><a href="#注：" class="headerlink" title="注："></a>注：</h3><ul><li>批量规范化在全连接层和卷积层的使用略有不同。</li><li>批量规范化层和暂退层一样，在训练模式和预测模式下计算不同。<h3 id="练习-21"><a href="#练习-21" class="headerlink" title="练习"></a>练习</h3></li></ul><ol><li><p>在使用批量规范化之前，我们是否可以从全连接层或卷积层中删除偏置参数？为什么？<br> 可以，因为批量规范化的一个作用就是对每个特征通道进行归一化，使得偏置的影响减小。因此，如果我们使用批量规范化，偏置参数的作用可能会变得不那么重要，甚至可以被省略。</p></li><li><p>我们是否需要在每个层中进行批量规范化？尝试一下？<br> 尝试在每个层中都进行批量规范化可能会导致网络过度约束，使得网络学习能力下降。</p></li><li>可以通过批量规范化来替换暂退法吗？行为会如何改变？<br> 它们是两种不同的正则化技术，不能简单替换。但可以结合使用。暂退法是在训练过程中随机将一部分神经元的输出置为0，可以减少过拟合问题。批量规范化可以额对每个特征通道的激活值进行归一化，有助于模型加速收敛，减少梯度消失问题。但如果把所有批量归一化转换为暂退法回事模型训练变慢或者不稳定。</li><li>查看高级API中有关<code>BatchNorm</code>的在线文档，以查看其他批量规范化的应用。<br> 在PyTorch中，BatchNorm是通过<code>torch.nn.BatchNorm</code>模块实现的。这个模块可以对输入的特征图（feature maps）进行规范化，具体来说，它会计算每个通道的均值和方差，并使用这些统计数据来规范化输入数据。BatchNorm的参数包括<code>num_features</code>（输入特征的数量）、<code>eps</code>（数值稳定性参数）、<code>momentum</code>（动量，用于更新均值和方差的移动平均）、<code>affine</code>（是否进行可学习的缩放和偏移变换）等</li><li>研究思路：可以应用的其他“规范化”转换？可以应用概率积分变换吗？全秩协方差估计可以么？<br> 面临复杂度高和对大规模数据不适用的问题，因此在深度学习中并不常见。</li></ol><h2 id="残差网络"><a href="#残差网络" class="headerlink" title="残差网络"></a>残差网络</h2><h3 id="函数类"><a href="#函数类" class="headerlink" title="函数类"></a>函数类</h3><p>嵌套函数类可以避免创造的新体系偏离原来的近似。<br>残差网络的核心思想是：每个附加层都应该更容易地包含原始函数作为其元素之一。 于是，<em>残差块</em>（residual blocks）便诞生了，这个设计对如何建立深层神经网络产生了深远的影响。</p><h3 id="残差块"><a href="#残差块" class="headerlink" title="残差块"></a>残差块</h3><p>要求残差块的输入于输出通道数相同，使得它们可以相加，如果想改变通道数，需要引入一个额外的1 * 1卷积来讲输入变换成需要的形状后在做相加运算。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.nn <span class="keyword">import</span> functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Residual</span>(nn.Module):  <span class="comment">#@save</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, input_channels, num_channels,</span></span><br><span class="line"><span class="params">                 use_1x1conv=<span class="literal">False</span>, strides=<span class="number">1</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.conv1 = nn.Conv2d(input_channels, num_channels,</span><br><span class="line">                               kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>, stride=strides)</span><br><span class="line">        self.conv2 = nn.Conv2d(num_channels, num_channels,</span><br><span class="line">                               kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">if</span> use_1x1conv:</span><br><span class="line">            self.conv3 = nn.Conv2d(input_channels, num_channels,</span><br><span class="line">                                   kernel_size=<span class="number">1</span>, stride=strides)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.conv3 = <span class="literal">None</span></span><br><span class="line">        self.bn1 = nn.BatchNorm2d(num_channels)</span><br><span class="line">        self.bn2 = nn.BatchNorm2d(num_channels)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, X</span>):</span><br><span class="line">        Y = F.relu(self.bn1(self.conv1(X)))</span><br><span class="line">        Y = self.bn2(self.conv2(Y))</span><br><span class="line">        <span class="keyword">if</span> self.conv3:</span><br><span class="line">            X = self.conv3(X)</span><br><span class="line">        Y += X</span><br><span class="line">        <span class="keyword">return</span> F.relu(Y)</span><br></pre></td></tr></table></figure></p><h3 id="resnet模型"><a href="#resnet模型" class="headerlink" title="resnet模型"></a>resnet模型</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">b1 = nn.Sequential(nn.Conv2d(<span class="number">1</span>, <span class="number">64</span>, kernel_size=<span class="number">7</span>, stride=<span class="number">2</span>, padding=<span class="number">3</span>),</span><br><span class="line">                   nn.BatchNorm2d(<span class="number">64</span>), nn.ReLU(),</span><br><span class="line">                   nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">resnet_block</span>(<span class="params">input_channels, num_channels, num_residuals,</span></span><br><span class="line"><span class="params">                 first_block=<span class="literal">False</span></span>):</span><br><span class="line">    blk = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(num_residuals):</span><br><span class="line">        <span class="keyword">if</span> i == <span class="number">0</span> <span class="keyword">and</span> <span class="keyword">not</span> first_block:</span><br><span class="line">            blk.append(Residual(input_channels, num_channels,</span><br><span class="line">                                use_1x1conv=<span class="literal">True</span>, strides=<span class="number">2</span>))</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            blk.append(Residual(num_channels, num_channels))</span><br><span class="line">    <span class="keyword">return</span> blk</span><br><span class="line"></span><br><span class="line">b2 = nn.Sequential(*resnet_block(<span class="number">64</span>, <span class="number">64</span>, <span class="number">2</span>, first_block=<span class="literal">True</span>))</span><br><span class="line">b3 = nn.Sequential(*resnet_block(<span class="number">64</span>, <span class="number">128</span>, <span class="number">2</span>))</span><br><span class="line">b4 = nn.Sequential(*resnet_block(<span class="number">128</span>, <span class="number">256</span>, <span class="number">2</span>))</span><br><span class="line">b5 = nn.Sequential(*resnet_block(<span class="number">256</span>, <span class="number">512</span>, <span class="number">2</span>))</span><br></pre></td></tr></table></figure><p>最后在Resnet中加入全局平均汇聚层，以及全连接输出。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">net = nn.Sequential(b1,b2,b3,b4,b5,</span><br><span class="line">   nn.AdaptiveAvgPool2d((<span class="number">1</span>,<span class="number">1</span>)),</span><br><span class="line">   nn.Flatten(),nn.Linear(<span class="number">512</span>,<span class="number">10</span>))</span><br></pre></td></tr></table></figure><br>将模型每一层输入形状打印出来的方法：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">X = torch.rand(size=(<span class="number">1</span>, <span class="number">1</span>, <span class="number">224</span>, <span class="number">224</span>))</span><br><span class="line"><span class="keyword">for</span> layer <span class="keyword">in</span> net:</span><br><span class="line">    X = layer(X)</span><br><span class="line">    <span class="built_in">print</span>(layer.__class__.__name__,<span class="string">&#x27;output shape:\t&#x27;</span>, X.shape)</span><br></pre></td></tr></table></figure></p><h3 id="练习-22"><a href="#练习-22" class="headerlink" title="练习"></a>练习</h3><ol><li>Inception块与残差块之间的主要区别是什么？在删除了Inception块中的一些路径之后，它们是如何相互关联的？<br> Inception将输入的特征都进行了处理，最后在通道上拼接。resnet的残差块将输入保留不进行运算（保证用1 * 1卷积使尺寸一致），最后直接相加。即使在简化后的Inception块中，多尺度特征提取的概念仍然存在，这是它与纯残差块的主要区别。在实际应用中，可以根据具体任务的需求和网络设计的考虑，灵活地调整Inception块的结构，以达到最佳的性能和效率。同时，也有研究者尝试将Inception块和残差块结合起来，创造出新的网络结构，以利用两者的优势。</li><li>对于更深层次的网络，ResNet引入了“bottleneck”架构来降低模型复杂性。请试着去实现它。<br> 在ResNet中，”bottleneck”架构是为了减少网络中的参数数量和计算复杂性而设计的。这种架构通过引入一个较小的卷积层（通常是1x1卷积）来降低维度，从而在进行较大的卷积操作（如3x3或5x5卷积）之前和之后减少输入和输出的通道数。</li><li>在ResNet的后续版本中，作者将“卷积层、批量规范化层和激活层”架构更改为“批量规范化层、激活层和卷积层”架构。请尝试做这个改进。详见resnet论文中的图1。</li><li>为什么即使函数类是嵌套的，我们仍然要限制增加函数的复杂性呢？<br> 可读性与可维护性，调试难度，性能影响，递归深度，模块化和重用性，测试难度。</li></ol><h2 id="稠密连接网络"><a href="#稠密连接网络" class="headerlink" title="稠密连接网络"></a>稠密连接网络</h2><h3 id="从resnet到densenet"><a href="#从resnet到densenet" class="headerlink" title="从resnet到densenet"></a>从resnet到densenet</h3><p>联系泰勒展开式，将一个函数分解成越来越高阶的项，同样将resnet展开为：</p><script type="math/tex; mode=display">f(x) = x+g(x)</script><p>一个简单线性项和一个复杂的非线性项，想将f(x)拓展为超过两部分的信息？<br>DenseNet与ResNet的区别在于，Densenet输出是连接，而不是简单相加。<br>![[Pasted image 20240403195132.png]]</p><h3 id="稠密块体"><a href="#稠密块体" class="headerlink" title="稠密块体"></a>稠密块体</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">conv_block</span>(<span class="params">input_channels, num_channels</span>):</span><br><span class="line">    <span class="keyword">return</span> nn.Sequential(</span><br><span class="line">        nn.BatchNorm2d(input_channels), nn.ReLU(),</span><br><span class="line">        nn.Conv2d(input_channels, num_channels, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">DenseBlock</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, num_convs, input_channels, num_channels</span>):</span><br><span class="line">        <span class="built_in">super</span>(DenseBlock, self).__init__()</span><br><span class="line">        layer = []</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(num_convs):</span><br><span class="line">            layer.append(conv_block(</span><br><span class="line">                num_channels * i + input_channels, num_channels))</span><br><span class="line">        self.net = nn.Sequential(*layer)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, X</span>):</span><br><span class="line">        <span class="keyword">for</span> blk <span class="keyword">in</span> self.net:</span><br><span class="line">            Y = blk(X)</span><br><span class="line">            <span class="comment"># 连接通道维度上每个块的输入和输出</span></span><br><span class="line">            X = torch.cat((X, Y), dim=<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">return</span> X</span><br></pre></td></tr></table></figure><p>ResNet和DenseNet的关键区别在于，DenseNet输出是连接，而不是如ResNet的简单相加。<br>稠密完工罗主要由两部分组成，稠密快，过渡层，前者定义如何连接和输入，而后者控制通道数量，使其不太会复杂。</p><h3 id="练习-23"><a href="#练习-23" class="headerlink" title="练习"></a>练习</h3><ol><li>DenseNet的优点之一是其模型参数比ResNet小。为什么呢？<ul><li>特征重用，每一层输出会作为下一层的输入，意味着网络中的每一层都可以访问到之前所有层的特征图。这种设计显著减少了需要学习的参数数量，因为网络可以重用之前层的特征，而不是每次都从头开始学习新的特征。</li><li>参数共享，可以在多个层之间共享权重。</li><li>densenet的设计允许在不牺牲性能的情况的下对网络进行压缩，通过一处或合并一些层，可以在保持网络性能的同时减少参数量。</li></ul></li><li>DenseNet一个诟病的问题是内存或显存消耗过多。有另一种方法来减少显存消耗吗？需要改变框架么？<br> DenseNet由于其密集连接的特性，在训练时确实会消耗较多的内存或显存。这是因为在DenseNet中，每一层都需要接收前面所有层的特征图作为输入，这导致特征图数量随着层数的增加而呈平方级增长。<ul><li>共享内存分配</li><li>压缩特征图（1x1卷积减少特征图通道数）</li><li>混合精度训练，将一些变量存储为低精度，可以减少显存消耗。<h1 id="循环神经网络"><a href="#循环神经网络" class="headerlink" title="循环神经网络"></a>循环神经网络</h1><h2 id="序列模型"><a href="#序列模型" class="headerlink" title="序列模型"></a>序列模型</h2><h3 id="马尔可夫模型"><a href="#马尔可夫模型" class="headerlink" title="马尔可夫模型"></a>马尔可夫模型</h3>我们使用当前序列前一段时间内的状态进行估计当前状态。<strong>一阶马尔可夫模型（First-Order Markov Model）</strong>：最简单的马尔可夫模型，它假设下一个状态的概率仅依赖于当前状态。一阶马尔可夫模型通常用状态转移矩阵来描述。<h3 id="练习-24"><a href="#练习-24" class="headerlink" title="练习"></a>练习</h3></li></ul></li><li>一位投资者依赖过去的回报来决定购买哪种证券的策略可能会遇到几个问题：<br> 证券回报可能具有时间序列特性，如季节性、趋势和周期性。单纯依赖历史回报可能无法充分捕捉这些特性，而需要更复杂的时间序列分析方法。</li><li>时间是向前推进的因果模型在文本分析中的适用程度有限。文本数据通常是静态的，不具有时间序列数据的动态特性。然而，因果模型可以用于理解文本内容之间的逻辑关系和因果链，例如在法律文件、历史文档或科学论文中分析不同概念和论点之间的联系。</li><li>隐变量自回归模型（如隐马尔可夫模型）可能在以下情况下用于捕捉数据的动力学模型：</li></ol><ul><li><strong>序列数据</strong>：当数据表现为一系列状态，并且状态转换可能依赖于之前的状态时，隐变量自回归模型可以用来捕捉状态转换的动态过程。</li><li><strong>不完全观测</strong>：当数据中的某些状态或变量无法直接观测到，或者观测到的数据存在噪声时，隐变量模型可以通过观测到的数据来推断隐状态的动态变化。</li></ul><h2 id="文本预处理"><a href="#文本预处理" class="headerlink" title="文本预处理"></a>文本预处理</h2><h3 id="步骤"><a href="#步骤" class="headerlink" title="步骤"></a>步骤</h3><ol><li>将文本作为字符串加载到内存中。</li><li>将字符串拆分为词元（如单词和字符）。</li><li>建立一个词表，将拆分的词元映射到数字索引。</li><li>将文本转换为数字索引序列，方便模型操作。<h3 id="数据集读取与词元化"><a href="#数据集读取与词元化" class="headerlink" title="数据集读取与词元化"></a>数据集读取与词元化</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">d2l.DATA_HUB[<span class="string">&#x27;time_machine&#x27;</span>] = (d2l.DATA_URL + <span class="string">&#x27;timemachine.txt&#x27;</span>,                       <span class="string">&#x27;090b5e7e70c295757f55df93cb0a180b9691891a&#x27;</span>)</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">read_time_machine</span>():  <span class="comment">#@save</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;将时间机器数据集加载到文本行的列表中&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(d2l.download(<span class="string">&#x27;time_machine&#x27;</span>), <span class="string">&#x27;r&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">        lines = f.readlines()</span><br><span class="line">    <span class="keyword">return</span> [re.sub(<span class="string">&#x27;[^A-Za-z]+&#x27;</span>, <span class="string">&#x27; &#x27;</span>, line).strip().lower() <span class="keyword">for</span> line <span class="keyword">in</span> lines]</span><br><span class="line"></span><br><span class="line">lines = read_time_machine()</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;# 文本总行数: <span class="subst">&#123;<span class="built_in">len</span>(lines)&#125;</span>&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(lines[<span class="number">0</span>])</span><br><span class="line"><span class="built_in">print</span>(lines[<span class="number">10</span>])</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">tokenize</span>(<span class="params">lines, token=<span class="string">&#x27;word&#x27;</span></span>):  <span class="comment">#@save</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;将文本行拆分为单词或字符词元&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">if</span> token == <span class="string">&#x27;word&#x27;</span>:</span><br><span class="line">        <span class="keyword">return</span> [line.split() <span class="keyword">for</span> line <span class="keyword">in</span> lines]</span><br><span class="line">    <span class="keyword">elif</span> token == <span class="string">&#x27;char&#x27;</span>:</span><br><span class="line">        <span class="keyword">return</span> [<span class="built_in">list</span>(line) <span class="keyword">for</span> line <span class="keyword">in</span> lines]</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;错误：未知词元类型：&#x27;</span> + token)</span><br><span class="line"></span><br><span class="line">tokens = tokenize(lines)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">11</span>):</span><br><span class="line">    <span class="built_in">print</span>(tokens[i])</span><br></pre></td></tr></table></figure><h3 id="词表"><a href="#词表" class="headerlink" title="词表"></a>词表</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Vocab</span>:  <span class="comment">#@save</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;文本词表&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, tokens=<span class="literal">None</span>, min_freq=<span class="number">0</span>, reserved_tokens=<span class="literal">None</span></span>):</span><br><span class="line">        <span class="keyword">if</span> tokens <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            tokens = []</span><br><span class="line">        <span class="keyword">if</span> reserved_tokens <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            reserved_tokens = []</span><br><span class="line">        <span class="comment"># 按出现频率排序</span></span><br><span class="line">        counter = count_corpus(tokens)</span><br><span class="line">        self._token_freqs = <span class="built_in">sorted</span>(counter.items(), key=<span class="keyword">lambda</span> x: x[<span class="number">1</span>],</span><br><span class="line">                                   reverse=<span class="literal">True</span>)</span><br><span class="line">        <span class="comment"># 未知词元的索引为0</span></span><br><span class="line">        self.idx_to_token = [<span class="string">&#x27;&lt;unk&gt;&#x27;</span>] + reserved_tokens</span><br><span class="line">        self.token_to_idx = &#123;token: idx</span><br><span class="line">                             <span class="keyword">for</span> idx, token <span class="keyword">in</span> <span class="built_in">enumerate</span>(self.idx_to_token)&#125;</span><br><span class="line">        <span class="keyword">for</span> token, freq <span class="keyword">in</span> self._token_freqs:</span><br><span class="line">            <span class="keyword">if</span> freq &lt; min_freq:</span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line">            <span class="keyword">if</span> token <span class="keyword">not</span> <span class="keyword">in</span> self.token_to_idx:</span><br><span class="line">                self.idx_to_token.append(token)</span><br><span class="line">                self.token_to_idx[token] = <span class="built_in">len</span>(self.idx_to_token) - <span class="number">1</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(self.idx_to_token)</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__getitem__</span>(<span class="params">self, tokens</span>):</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> <span class="built_in">isinstance</span>(tokens, (<span class="built_in">list</span>, <span class="built_in">tuple</span>)):</span><br><span class="line">            <span class="keyword">return</span> self.token_to_idx.get(tokens, self.unk)</span><br><span class="line">        <span class="keyword">return</span> [self.__getitem__(token) <span class="keyword">for</span> token <span class="keyword">in</span> tokens]</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">to_tokens</span>(<span class="params">self, indices</span>):</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> <span class="built_in">isinstance</span>(indices, (<span class="built_in">list</span>, <span class="built_in">tuple</span>)):</span><br><span class="line">            <span class="keyword">return</span> self.idx_to_token[indices]</span><br><span class="line">        <span class="keyword">return</span> [self.idx_to_token[index] <span class="keyword">for</span> index <span class="keyword">in</span> indices]</span><br><span class="line"><span class="meta">    @property</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">unk</span>(<span class="params">self</span>):  <span class="comment"># 未知词元的索引为0</span></span><br><span class="line">        <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line"><span class="meta">    @property</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">token_freqs</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> self._token_freqs</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">count_corpus</span>(<span class="params">tokens</span>):  <span class="comment">#@save</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;统计词元的频率&quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># 这里的tokens是1D列表或2D列表</span></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">len</span>(tokens) == <span class="number">0</span> <span class="keyword">or</span> <span class="built_in">isinstance</span>(tokens[<span class="number">0</span>], <span class="built_in">list</span>):</span><br><span class="line">        <span class="comment"># 将词元列表展平成一个列表</span></span><br><span class="line">        tokens = [token <span class="keyword">for</span> line <span class="keyword">in</span> tokens <span class="keyword">for</span> token <span class="keyword">in</span> line]</span><br><span class="line">    <span class="keyword">return</span> collections.Counter(tokens)</span><br></pre></td></tr></table></figure>使用时光机器数据集作为语料库来构建此表，然后打印前几个高频词元及其索引。<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">vocab = Vocab(tokens)</span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">list</span>(vocab.token_to_idx.items())[:<span class="number">10</span>])</span><br></pre></td></tr></table></figure></li></ol><h3 id="功能整合"><a href="#功能整合" class="headerlink" title="功能整合"></a>功能整合</h3><p>在使用上述函数时，我们将所有功能打包到<code>load_corpus_time_machine</code>函数中， 该函数返回<code>corpus</code>（词元索引列表）和<code>vocab</code>（时光机器语料库的词表）。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">load_corpus_time_machine</span>(<span class="params">max_tokens=-<span class="number">1</span></span>):  <span class="comment">#@save</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;返回时光机器数据集的词元索引列表和词表&quot;&quot;&quot;</span></span><br><span class="line">    lines = read_time_machine()</span><br><span class="line">    tokens = tokenize(lines, <span class="string">&#x27;char&#x27;</span>)</span><br><span class="line">    vocab = Vocab(tokens)</span><br><span class="line">    <span class="comment"># 因为时光机器数据集中的每个文本行不一定是一个句子或一个段落，</span></span><br><span class="line">    <span class="comment"># 所以将所有文本行展平到一个列表中</span></span><br><span class="line">    corpus = [vocab[token] <span class="keyword">for</span> line <span class="keyword">in</span> tokens <span class="keyword">for</span> token <span class="keyword">in</span> line]</span><br><span class="line">    <span class="keyword">if</span> max_tokens &gt; <span class="number">0</span>:</span><br><span class="line">        corpus = corpus[:max_tokens]</span><br><span class="line">    <span class="keyword">return</span> corpus, vocab</span><br><span class="line"></span><br><span class="line">corpus, vocab = load_corpus_time_machine()</span><br><span class="line"><span class="built_in">len</span>(corpus), <span class="built_in">len</span>(vocab)</span><br></pre></td></tr></table></figure></p><h3 id="练习-25"><a href="#练习-25" class="headerlink" title="练习"></a>练习</h3><ol><li>词元化是一个关键的预处理步骤，它因语言而异。尝试找到另外三种常用的词元化文本的方法。</li></ol><ul><li>BPE（Byte-Pair Encoding） WordPiece SentencePiece</li></ul><ol><li>在本节的实验中，将文本词元为单词和更改<code>Vocab</code>实例的<code>min_freq</code>参数。这对词表大小有何影响？ <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">lines = read_time_machine()</span><br><span class="line">tokens = tokenize(lines, <span class="string">&#x27;word&#x27;</span>)</span><br><span class="line">vocab = Vocab(tokens, min_freq=<span class="number">3</span>)</span><br><span class="line">corpus = [vocab[token] <span class="keyword">for</span> line <span class="keyword">in</span> tokens <span class="keyword">for</span> token <span class="keyword">in</span> line]</span><br><span class="line"><span class="keyword">if</span> -<span class="number">1</span> &gt; <span class="number">0</span>:</span><br><span class="line">    corpus = corpus[:-<span class="number">1</span>]</span><br><span class="line"><span class="built_in">len</span>(corpus), <span class="built_in">len</span>(vocab)</span><br></pre></td></tr></table></figure>词表大小变为(32775, 1420)<h2 id="语言模型和数据集"><a href="#语言模型和数据集" class="headerlink" title="语言模型和数据集"></a>语言模型和数据集</h2>语言模型的目标是估计序列的联合概率<br>只需要抽取一个词元<br>一个理想的语言模型能够基于模型本身生成自然文本。<h3 id="练习-26"><a href="#练习-26" class="headerlink" title="练习"></a>练习</h3></li><li>假设训练数据集中有100,000个单词。一个四元语法需要存储多少个词频和相邻多词频率？<br> (N-3) <em> (N-2) </em> (N-1) <em> N词频和(N-3) </em> (N-2) * (N-1) 相邻多词词频</li><li>我们如何对一系列对话建模？<br> 将每一段对话视为一个序列数据，用RNN处理这些序列。</li><li>一元语法、二元语法和三元语法的齐普夫定律的指数是不一样的，能设法估计么？<br> 齐普夫定律描述了词频与词序之间的幂律关系。对于一元语法、二元语法和三元语法，齐普夫定律的指数可能会有所不同，这取决于语言的特性和语料库的特定性质。估计这些指数通常需要对大量文本数据进行统计分析，通过拟合词频分布的尾部来确定。</li><li>想一想读取长序列数据的其他方法？<br> 使用滑动窗口技术，将长序列分割成多个较短的子序列，或者使用分段技术，将序列分成多个部分分别处理。</li><li>考虑一下我们用于读取长序列的随机偏移量。<ol><li>为什么随机偏移量是个好主意？<br> 它允许模型在每次迭代中从长序列中随机选择一个子序列，从而增加训练过程中的多样性，并可能提高模型的泛化能力。</li><li>它真的会在文档的序列上实现完美的均匀分布吗？<br> 可能不会在文档的序列上实现完美的均匀分布，因为某些区域可能会被更频繁地采样。</li><li>要怎么做才能使分布更均匀？<br> 限制每个序列的重叠区域，或者使用更复杂的采样策略来引导模型关注序列的不同部分。</li></ol></li><li>如果我们希望一个序列样本是一个完整的句子，那么这在小批量抽样中会带来怎样的问题？如何解决？<br> 一个句子可能被分割成两个或多个小批量，从而破坏了句子的完整性。可以使用桶排序，它将具有相似长度的序列分组到同一个小批量中，以保持句子的完整性。</li></ol><h2 id="循环神经网络-1"><a href="#循环神经网络-1" class="headerlink" title="循环神经网络"></a>循环神经网络</h2><h3 id="练习-27"><a href="#练习-27" class="headerlink" title="练习"></a>练习</h3><ol><li>如果我们使用循环神经网络来预测文本序列中的下一个字符，那么任意输出所需的维度是多少？<br> 等于词汇表的大小</li><li>为什么循环神经网络可以基于文本序列中所有先前的词元，在某个时间步表示当前词元的条件概率？<br> 它的设计允许网络在每个实践部接受新的输入并更新其内部状态。这种状态能够捕捉并保留序列中的历史信息，使得网络可以根据先前观察到的词元来预测下一个词元。</li><li>如果基于一个长序列进行反向传播，梯度会发生什么状况？<br> 可能会梯度消失或者梯度爆炸。在反向传播时，梯度需要通过序列的每个时间步进行多次链式求导，如果序列很长，这些连续的乘积操作可能导致梯度非常小或非常大，影响网络学习过程。</li><li>与本节中描述的语言模型相关的问题有哪些？</li></ol><ul><li>如何选择和设计合适的词汇表，以便更好地捕捉语言的特性。</li><li>如何确定最佳的模型架构，包括隐藏层的大小和层数，以及激活函数的选择。</li><li>如何处理未知词汇，以及如何平衡词表的大小和模型的泛化能力。</li></ul><h2 id="循环神经网络的从零开始实现"><a href="#循环神经网络的从零开始实现" class="headerlink" title="循环神经网络的从零开始实现"></a>循环神经网络的从零开始实现</h2><h3 id="独热编码"><a href="#独热编码" class="headerlink" title="独热编码"></a>独热编码</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">F.one_hot(torch.tensor([<span class="number">0</span>,<span class="number">2</span>]),<span class="built_in">len</span>(vocab))</span><br></pre></td></tr></table></figure><p>我们每次采样的小批量数据形状是二维张量： （批量大小，时间步数）。 <code>one_hot</code>函数将这样一个小批量数据转换成三维张量， 张量的最后一个维度等于词表大小（<code>len(vocab)</code>）。 我们经常转换输入的维度，以便获得形状为 （时间步数，批量大小，词表大小）的输出。 这将使我们能够更方便地通过最外层的维度， 一步一步地更新小批量数据的隐状态。</p><h3 id="初始化模型参数"><a href="#初始化模型参数" class="headerlink" title="初始化模型参数"></a>初始化模型参数</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">get_params</span>(<span class="params">vocab_size, num_hiddens, device</span>):</span><br><span class="line">    num_inputs = num_outputs = vocab_size</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">normal</span>(<span class="params">shape</span>):</span><br><span class="line">        <span class="keyword">return</span> torch.randn(size=shape, device=device) * <span class="number">0.01</span></span><br><span class="line">    <span class="comment"># 隐藏层参数</span></span><br><span class="line">    W_xh = normal((num_inputs, num_hiddens))</span><br><span class="line">    W_hh = normal((num_hiddens, num_hiddens))</span><br><span class="line">    b_h = torch.zeros(num_hiddens, device=device)</span><br><span class="line">    <span class="comment"># 输出层参数</span></span><br><span class="line">    W_hq = normal((num_hiddens, num_outputs))</span><br><span class="line">    b_q = torch.zeros(num_outputs, device=device)</span><br><span class="line">    <span class="comment"># 附加梯度</span></span><br><span class="line">    params = [W_xh, W_hh, b_h, W_hq, b_q]</span><br><span class="line">    <span class="keyword">for</span> param <span class="keyword">in</span> params:</span><br><span class="line">        param.requires_grad_(<span class="literal">True</span>)</span><br><span class="line">    <span class="keyword">return</span> params</span><br></pre></td></tr></table></figure><h3 id="网络模型"><a href="#网络模型" class="headerlink" title="网络模型"></a>网络模型</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">init_rnn_state</span>(<span class="params">batch_size, num_hiddens, device</span>):</span><br><span class="line">    <span class="keyword">return</span> (torch.zeros((batch_size, num_hiddens), device=device), )</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">rnn</span>(<span class="params">inputs, state, params</span>):</span><br><span class="line">    <span class="comment"># inputs的形状：(时间步数量，批量大小，词表大小)</span></span><br><span class="line">    W_xh, W_hh, b_h, W_hq, b_q = params</span><br><span class="line">    H, = state</span><br><span class="line">    outputs = []</span><br><span class="line">    <span class="comment"># X的形状：(批量大小，词表大小)</span></span><br><span class="line">    <span class="keyword">for</span> X <span class="keyword">in</span> inputs:</span><br><span class="line">        H = torch.tanh(torch.mm(X, W_xh) + torch.mm(H, W_hh) + b_h)</span><br><span class="line">        Y = torch.mm(H, W_hq) + b_q</span><br><span class="line">        outputs.append(Y)</span><br><span class="line">    <span class="keyword">return</span> torch.cat(outputs, dim=<span class="number">0</span>), (H,)</span><br></pre></td></tr></table></figure><p>rnn函数定义了如何在一个时间步内计算因状态和输出。循环神经网络模型通过inputs最外层的维度实现循环，一边逐时间步更新批量数据的隐状态H，此外，这里使用tanh函数作为激活函数。<br>接下来需要创建一个类来包装这些函数：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">RNNModelScratch</span>: <span class="comment">#@save</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;从零开始实现的循环神经网络模型&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, vocab_size, num_hiddens, device,</span></span><br><span class="line"><span class="params">                 get_params, init_state, forward_fn</span>):</span><br><span class="line">        self.vocab_size, self.num_hiddens = vocab_size, num_hiddens</span><br><span class="line">        self.params = get_params(vocab_size, num_hiddens, device)</span><br><span class="line">        self.init_state, self.forward_fn = init_state, forward_fn</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__call__</span>(<span class="params">self, X, state</span>):</span><br><span class="line">        X = F.one_hot(X.T, self.vocab_size).<span class="built_in">type</span>(torch.float32)</span><br><span class="line">        <span class="keyword">return</span> self.forward_fn(X, state, self.params)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">begin_state</span>(<span class="params">self, batch_size, device</span>):</span><br><span class="line">        <span class="keyword">return</span> self.init_state(batch_size, self.num_hiddens, device)</span><br></pre></td></tr></table></figure></p><h3 id="训练"><a href="#训练" class="headerlink" title="训练"></a>训练</h3><ol><li>序列数据的不同采样方法（随机采样和顺序分区）将导致隐状态初始化的差异。</li><li>我们在更新模型参数之前裁剪梯度。 这样的操作的目的是，即使训练过程中某个点上发生了梯度爆炸，也能保证模型不会发散。</li><li>我们用困惑度来评价模型。所述， 这样的度量确保了不同长度的序列具有可比性。<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#@save</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">train_epoch_ch8</span>(<span class="params">net, train_iter, loss, updater, device, use_random_iter</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;训练网络一个迭代周期（定义见第8章）&quot;&quot;&quot;</span></span><br><span class="line">    state, timer = <span class="literal">None</span>, d2l.Timer()</span><br><span class="line">    metric = d2l.Accumulator(<span class="number">2</span>)  <span class="comment"># 训练损失之和,词元数量</span></span><br><span class="line">    <span class="keyword">for</span> X, Y <span class="keyword">in</span> train_iter:</span><br><span class="line">        <span class="keyword">if</span> state <span class="keyword">is</span> <span class="literal">None</span> <span class="keyword">or</span> use_random_iter:</span><br><span class="line">            <span class="comment"># 在第一次迭代或使用随机抽样时初始化state</span></span><br><span class="line">            state = net.begin_state(batch_size=X.shape[<span class="number">0</span>], device=device)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">isinstance</span>(net, nn.Module) <span class="keyword">and</span> <span class="keyword">not</span> <span class="built_in">isinstance</span>(state, <span class="built_in">tuple</span>):</span><br><span class="line">                <span class="comment"># state对于nn.GRU是个张量</span></span><br><span class="line">                state.detach_()</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="comment"># state对于nn.LSTM或对于我们从零开始实现的模型是个张量</span></span><br><span class="line">                <span class="keyword">for</span> s <span class="keyword">in</span> state:</span><br><span class="line">                    s.detach_()</span><br><span class="line">        y = Y.T.reshape(-<span class="number">1</span>)</span><br><span class="line">        X, y = X.to(device), y.to(device)</span><br><span class="line">        y_hat, state = net(X, state)</span><br><span class="line">        l = loss(y_hat, y.long()).mean()</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">isinstance</span>(updater, torch.optim.Optimizer):</span><br><span class="line">            updater.zero_grad()</span><br><span class="line">            l.backward()</span><br><span class="line">            grad_clipping(net, <span class="number">1</span>)</span><br><span class="line">            updater.step()</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            l.backward()</span><br><span class="line">            grad_clipping(net, <span class="number">1</span>)</span><br><span class="line">            updater(batch_size=<span class="number">1</span>)</span><br><span class="line">        metric.add(l * y.numel(), y.numel())</span><br><span class="line">    <span class="keyword">return</span> math.exp(metric[<span class="number">0</span>] / metric[<span class="number">1</span>]), metric[<span class="number">1</span>] / timer.stop()</span><br></pre></td></tr></table></figure></li></ol><h3 id="训练和预测"><a href="#训练和预测" class="headerlink" title="训练和预测"></a>训练和预测</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#@save</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">train_ch8</span>(<span class="params">net, train_iter, vocab, lr, num_epochs, device,</span></span><br><span class="line"><span class="params">              use_random_iter=<span class="literal">False</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;训练模型（定义见第8章）&quot;&quot;&quot;</span></span><br><span class="line">    loss = nn.CrossEntropyLoss()</span><br><span class="line">    animator = d2l.Animator(xlabel=<span class="string">&#x27;epoch&#x27;</span>, ylabel=<span class="string">&#x27;perplexity&#x27;</span>,</span><br><span class="line">                            legend=[<span class="string">&#x27;train&#x27;</span>], xlim=[<span class="number">10</span>, num_epochs])</span><br><span class="line">    <span class="comment"># 初始化</span></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">isinstance</span>(net, nn.Module):</span><br><span class="line">        updater = torch.optim.SGD(net.parameters(), lr)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        updater = <span class="keyword">lambda</span> batch_size: d2l.sgd(net.params, lr, batch_size)</span><br><span class="line">    predict = <span class="keyword">lambda</span> prefix: predict_ch8(prefix, <span class="number">50</span>, net, vocab, device)</span><br><span class="line">    <span class="comment"># 训练和预测</span></span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epochs):</span><br><span class="line">        ppl, speed = train_epoch_ch8(</span><br><span class="line">            net, train_iter, loss, updater, device, use_random_iter)</span><br><span class="line">        <span class="keyword">if</span> (epoch + <span class="number">1</span>) % <span class="number">10</span> == <span class="number">0</span>:</span><br><span class="line">            <span class="built_in">print</span>(predict(<span class="string">&#x27;time traveller&#x27;</span>))</span><br><span class="line">            animator.add(epoch + <span class="number">1</span>, [ppl])</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;困惑度 <span class="subst">&#123;ppl:<span class="number">.1</span>f&#125;</span>, <span class="subst">&#123;speed:<span class="number">.1</span>f&#125;</span> 词元/秒 <span class="subst">&#123;<span class="built_in">str</span>(device)&#125;</span>&#x27;</span>)</span><br><span class="line">    <span class="built_in">print</span>(predict(<span class="string">&#x27;time traveller&#x27;</span>))</span><br><span class="line">    <span class="built_in">print</span>(predict(<span class="string">&#x27;traveller&#x27;</span>))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">num_epochs, lr = <span class="number">500</span>, <span class="number">1</span></span><br><span class="line">train_ch8(net, train_iter, vocab, lr, num_epochs, d2l.try_gpu())</span><br></pre></td></tr></table></figure><h3 id="练习-28"><a href="#练习-28" class="headerlink" title="练习"></a>练习</h3><ol><li>尝试说明独热编码等价于为每个对象选择不同的嵌入表示。<br> 在文本处理中，每个词元（如单词或字符）都会被分配一个唯一的索引，独热编码就是根据这个索引创建一个与词汇表大小相同的向量，其中对应索引位置的元素为1，其余为0。这种表示方法等价于为每个对象选择一个不同的嵌入向量，其中嵌入向量是预先定义好的，而不是通过学习得到的。</li><li>在不裁剪梯度的情况下运行本节中的代码会发生什么？<br> 可能会导致数值不稳定，如梯度爆炸，这会影响模型的训练过程和最终性能。</li><li>更改顺序划分，使其不会从计算图中分离隐状态。运行时间会有变化吗？困惑度呢？<br> 可能会影响模型的训练效率和稳定性。这种改变可能会使隐状态在每次迭代中保持连续，从而有助于梯度的传播和模型的学习。</li><li>用ReLU替换本节中使用的激活函数，并重复本节中的实验。我们还需要梯度裁剪吗？为什么？<br> 可能会影响模型的性能。ReLU激活函数在正区间内保持线性，而在负区间内输出为0。这种特性使得ReLU在正向传播时能够保持梯度不衰减，有助于缓解梯度消失问题。在某些情况下，使用ReLU可以减少对梯度裁剪的需求，因为它相对于其他激活函数（如sigmoid或tanh）在正区间内具有恒定的梯度。</li></ol><h3 id="循环神经网络的简介实现"><a href="#循环神经网络的简介实现" class="headerlink" title="循环神经网络的简介实现"></a>循环神经网络的简介实现</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.nn <span class="keyword">import</span> functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br><span class="line"></span><br><span class="line">batch_size, num_steps = <span class="number">32</span>, <span class="number">35</span></span><br><span class="line">train_iter, vocab = d2l.load_data_time_machine(batch_size, num_steps)</span><br></pre></td></tr></table></figure><h3 id="模型定义"><a href="#模型定义" class="headerlink" title="模型定义"></a>模型定义</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">num_hiddens = <span class="number">256</span></span><br><span class="line">rnn_layer = nn.RNN(<span class="built_in">len</span>(vocab), num_hiddens)</span><br><span class="line"><span class="comment"># 我们使用张量来初始化隐状态，它的形状是（隐藏层数，批量大小，隐藏单元数）。</span></span><br><span class="line">state = torch.zeros((<span class="number">1</span>, batch_size, num_hiddens))</span><br><span class="line">X = torch.rand(size=(num_steps, batch_size, <span class="built_in">len</span>(vocab)))</span><br><span class="line">Y, state_new = rnn_layer(X, state)</span><br></pre></td></tr></table></figure><h3 id="定义一个RNN类"><a href="#定义一个RNN类" class="headerlink" title="定义一个RNN类"></a>定义一个RNN类</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#@save</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">RNNModel</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;循环神经网络模型&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, rnn_layer, vocab_size, **kwargs</span>):</span><br><span class="line">        <span class="built_in">super</span>(RNNModel, self).__init__(**kwargs)</span><br><span class="line">        self.rnn = rnn_layer</span><br><span class="line">        self.vocab_size = vocab_size</span><br><span class="line">        self.num_hiddens = self.rnn.hidden_size</span><br><span class="line">        <span class="comment"># 如果RNN是双向的（之后将介绍），num_directions应该是2，否则应该是1</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> self.rnn.bidirectional:</span><br><span class="line">            self.num_directions = <span class="number">1</span></span><br><span class="line">            self.linear = nn.Linear(self.num_hiddens, self.vocab_size)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.num_directions = <span class="number">2</span></span><br><span class="line">            self.linear = nn.Linear(self.num_hiddens * <span class="number">2</span>, self.vocab_size)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, inputs, state</span>):</span><br><span class="line">        X = F.one_hot(inputs.T.long(), self.vocab_size)</span><br><span class="line">        X = X.to(torch.float32)</span><br><span class="line">        Y, state = self.rnn(X, state)</span><br><span class="line">        <span class="comment"># 全连接层首先将Y的形状改为(时间步数*批量大小,隐藏单元数)</span></span><br><span class="line">        <span class="comment"># 它的输出形状是(时间步数*批量大小,词表大小)。</span></span><br><span class="line">        output = self.linear(Y.reshape((-<span class="number">1</span>, Y.shape[-<span class="number">1</span>])))</span><br><span class="line">        <span class="keyword">return</span> output, state</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">begin_state</span>(<span class="params">self, device, batch_size=<span class="number">1</span></span>):</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> <span class="built_in">isinstance</span>(self.rnn, nn.LSTM):</span><br><span class="line">            <span class="comment"># nn.GRU以张量作为隐状态</span></span><br><span class="line">            <span class="keyword">return</span>  torch.zeros((self.num_directions * self.rnn.num_layers,batch_size, self.num_hiddens),</span><br><span class="line">                                device=device)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="comment"># nn.LSTM以元组作为隐状态</span></span><br><span class="line">            <span class="keyword">return</span> (torch.zeros((</span><br><span class="line">                self.num_directions * self.rnn.num_layers,</span><br><span class="line">                batch_size, self.num_hiddens),device=device),</span><br><span class="line">                    torch.zeros((</span><br><span class="line">                        self.num_directions *self.rnn.num_layers,batch_size, self.num_hiddens), device=device))</span><br></pre></td></tr></table></figure><h3 id="训练与预测"><a href="#训练与预测" class="headerlink" title="训练与预测"></a>训练与预测</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">device = d2l.try_gpu()</span><br><span class="line">net = RNNModel(rnn_layer, vocab_size=<span class="built_in">len</span>(vocab))</span><br><span class="line">net = net.to(device)</span><br><span class="line">d2l.predict_ch8(<span class="string">&#x27;time traveller&#x27;</span>, <span class="number">10</span>, net, vocab, device)</span><br></pre></td></tr></table></figure><h3 id="练习-29"><a href="#练习-29" class="headerlink" title="练习"></a>练习</h3><ol><li>尝试使用高级API，能使循环神经网络模型过拟合吗？<br> 尤其是当数据集相对较小或者模型过于复杂时。高级API通常提供了更多的模型配置选项和优化技术，如正则化、dropout等，这些技术可以帮助减少过拟合的风险。然而，如果不正确使用这些技术，或者在训练过程中没有适当的验证和调整，模型仍然可能会在训练数据上过度拟合，导致泛化能力下降。</li><li>如果在循环神经网络模型中增加隐藏层的数量会发生什么？能使模型正常工作吗？<br> 可以增加模型的复杂度和学习能力，这在处理复杂任务时可能是有益的。然而，增加隐藏层的数量也可能导致几个问题：首先，它可能会增加模型训练的难度，因为更多的参数需要被优化；其次，它可能会增加过拟合的风险，因为模型可能会学习到数据中的噪声而不是潜在的模式。为了使增加隐藏层的模型正常工作，需要仔细调整超参数，可能还需要更多的训练数据和更复杂的正则化技术。</li></ol><h1 id="现代循环神经网络"><a href="#现代循环神经网络" class="headerlink" title="现代循环神经网络"></a>现代循环神经网络</h1><h2 id="门控循环单元（GRU）"><a href="#门控循环单元（GRU）" class="headerlink" title="门控循环单元（GRU）"></a>门控循环单元（GRU）</h2><h3 id="练习-30"><a href="#练习-30" class="headerlink" title="练习"></a>练习</h3><ol><li>假设我们只想使用时间步t′的输入来预测时间步t&gt;t′的输出。对于每个时间步，重置门和更新门的最佳值是什么？</li></ol><ul><li>重置门：rt​ 应该接近0，这表示我们不希望将过去的隐藏状态信息融入到当前的候选隐藏状态中。</li><li>更新门：zt​ 应该接近1，这表示我们希望保留大部分的当前输入信息，而不是过去的隐藏状态。</li></ul><ol><li>调整和分析超参数对运行时间、困惑度和输出顺序的影响。</li></ol><ul><li>学习率：较高的学习率可能导致模型快速收敛，但也可能导致在最优解附近的震荡或发散。较低的学习率则可能导致模型收敛速度缓慢。</li><li>隐藏层大小：较大的隐藏层可以捕捉更复杂的特征，但可能导致过拟合和更长的训练时间。较小的隐藏层可能无法捕捉所有必要的信息，导致欠拟合。</li><li>批次大小：较大的批次可以提供更稳定的梯度估计，但需要更多的内存，并且可能需要更多的迭代来训练模型。较小的批次可能需要更多的迭代，但每次迭代的计算量较小。</li><li>迭代次数：更多的迭代次数可以给模型更多的时间来学习数据，但也可能导致过拟合和不必要的计算。较少的迭代次数可能导致模型未能充分学习。</li></ul><ol><li>比较<code>rnn.RNN</code>和<code>rnn.GRU</code>的不同实现对运行时间、困惑度和输出字符串的影响。</li></ol><ul><li>运行时间：GRU通常具有更少的参数和更简单的结构，这可能导致在某些情况下训练速度更快。</li><li>困惑度：GRU由于其门控机制，通常能够更好地捕捉长距离依赖关系，这可能导致较低的困惑度和更好的模型性能。</li><li>输出字符串：GRU的输出可能更加流畅和连贯，因为它能够更有效地避免梯度消失问题，从而在长序列中保持信息的传递。</li></ul><ol><li>如果仅仅实现门控循环单元的一部分，例如，只有一个重置门或一个更新门会怎样？<br> 没有更新门，模型可能无法有效地决定何时应该更新其隐藏状态；没有重置门，模型可能无法决定何时应该忘记过去的信息。这可能导致模型无法捕捉到序列中的重要动态，从而影响其性能和预测能力。<h2 id="长短期记忆网络（LSTM）"><a href="#长短期记忆网络（LSTM）" class="headerlink" title="长短期记忆网络（LSTM）"></a>长短期记忆网络（LSTM）</h2>LSTM的每个单元包含以下几个关键的组成部分：</li><li><strong>遗忘门（Forget Gate）</strong>：决定哪些信息应该被丢弃或保留。它通过一个sigmoid函数来输出一个介于0到1之间的值，0表示完全遗忘，而1表示完全保留。</li><li><strong>输入门（Input Gate）</strong>：决定哪些新的信息应该被添加到细胞状态中。它由两部分组成：一个sigmoid层决定哪些值我们将要更新，和一个tanh层创建一个新的候选值向量，这些值将会被加入到状态中。</li><li><strong>细胞状态（Cell State）</strong>：是LSTM的核心，它在整个序列中传递相关的信息。细胞状态的更新是通过遗忘门和输入门的组合来实现的。</li><li><strong>输出门（Output Gate）</strong>：基于当前的细胞状态和输入，决定最终的输出。输出门的输出通过tanh函数处理，并将结果与sigmoid门的输出相乘，以决定最终输出的活跃度。<br>LSTM的这些门控机制使得网络能够有选择性地保留或遗忘信息，从而有效地处理长期依赖关系。这使得LSTM在许多序列数据任务中，如语言模型、机器翻译、语音识别等领域，都取得了显著的成功。<br>![[Pasted image 20240406190204.png]]<h3 id="练习-31"><a href="#练习-31" class="headerlink" title="练习"></a>练习</h3></li><li>调整和分析超参数对运行时间、困惑度和输出顺序的影响。<br> 增加隐藏层的神经元数量可能会导致模型的训练时间增加，因为需要更多的时间来计算和更新更多的权重。同时，这也可能影响模型的困惑度，即模型对测试数据的不确定性的度量。如果模型过于复杂，可能会导致过拟合，从而在训练数据上表现良好但在测试数据上表现不佳，从而增加困惑度。</li><li>如何更改模型以生成适当的单词，而不是字符序列？<br> 可以通过更改模型的输出层和训练数据来实现。<br> 首先，需要将文本数据预处理为单词级别的表示，而不是字符级别的表示。这意味着需要构建一个单词到整数的映射，并在训练数据中将每个单词转换为相应的整数序列。然后，模型的输出层应该设计为预测下一个单词的概率分布，而不是下一个字符。这通常通过在RNN的顶部添加一个全连接层来实现，该层的输出维度等于词汇表的大小。在训练过程中，使用单词级别的标签（即正确的下一个单词）作为目标，而不是字符级别的标签。</li><li>在给定隐藏层维度的情况下，比较门控循环单元、长短期记忆网络和常规循环神经网络的计算成本。要特别注意训练和推断成本。<br> 门控循环单元（GRU）和长短期记忆网络（LSTM）都是为了解决常规循环神经网络（RNN）在处理长序列时遇到的梯度消失问题而设计的。在隐藏层维度给定的情况下，LSTM和GRU通常比常规RNN具有更多的参数和更复杂的结构，因为它们引入了门控机制来控制信息的流动。这可能会导致它们的训练成本高于常规RNN，因为需要更多的计算资源来更新和优化这些额外的参数。然而，这种增加的计算成本可能会通过减少训练时间（因为模型能够更快地学习长距离依赖关系）和提高模型性能来补偿。在推断阶段，LSTM和GRU可能会比常规RNN慢，因为门控机制需要额外的计算来决定信息的保留和遗忘。</li><li>既然候选记忆元通过使用tanh函数来确保值范围在(−1,1)之间，那么为什么隐状态需要再次使用tanh函数来确保输出值范围在(−1,1)之间呢？<br> 在LSTM中，候选记忆元是通过tanh函数处理的，确保其值范围在[-1, 1]之间。然而，LSTM的隐状态是记忆元的一个加权和，其中包括旧的隐状态和新的候选记忆元。隐状态需要通过tanh函数处理，以确保其值范围在[0, 1]之间，这是因为隐状态在LSTM中充当着网络的“记忆”，并且需要反映为一个概率分布，表示信息的重要性和应该被保留的程度。tanh函数的输出范围与LSTM的设计目标相匹配，即保留重要的信息并遗忘不重要的信息。</li><li>实现一个能够基于时间序列进行预测而不是基于字符序列进行预测的长短期记忆网络模型。<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"><span class="comment"># x_train的形状应该是（样本数量，时间步长，特征数量）</span></span><br><span class="line"><span class="comment"># y_train的形状应该是（样本数量，输出长度）</span></span><br><span class="line"><span class="comment"># 定义LSTM模型</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">LSTMModel</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, input_size, hidden_size, num_layers, output_size</span>):</span><br><span class="line">        <span class="built_in">super</span>(LSTMModel, self).__init__()</span><br><span class="line">        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=<span class="literal">True</span>)</span><br><span class="line">        self.fc = nn.Linear(hidden_size, output_size)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        lstm_out, (hidden, cell) = self.lstm(x)</span><br><span class="line">        <span class="comment"># 只使用最后一个时间步的输出</span></span><br><span class="line">        output = self.fc(lstm_out[:, -<span class="number">1</span>, :])</span><br><span class="line">        <span class="keyword">return</span> output</span><br><span class="line"><span class="comment"># 设定超参数</span></span><br><span class="line">input_size = <span class="number">4</span>  <span class="comment"># 特征数量</span></span><br><span class="line">hidden_size = <span class="number">50</span>  <span class="comment"># LSTM隐藏层大小</span></span><br><span class="line">num_layers = <span class="number">1</span>  <span class="comment"># LSTM层数</span></span><br><span class="line">output_size = <span class="number">1</span>  <span class="comment"># 输出长度，对于回归问题通常为1</span></span><br><span class="line">learning_rate = <span class="number">0.01</span></span><br><span class="line">batch_size = <span class="number">32</span></span><br><span class="line">num_epochs = <span class="number">100</span></span><br><span class="line"><span class="comment"># 创建模型实例</span></span><br><span class="line">model = LSTMModel(input_size, hidden_size, num_layers, output_size)</span><br><span class="line"><span class="comment"># 损失函数和优化器</span></span><br><span class="line">criterion = nn.MSELoss()</span><br><span class="line">optimizer = optim.Adam(model.parameters(), lr=learning_rate)</span><br><span class="line"><span class="comment"># 训练模型</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epochs):</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, <span class="built_in">len</span>(x_train), batch_size):</span><br><span class="line">        batch_x = x_train[i:i+batch_size]</span><br><span class="line">        batch_y = y_train[i:i+batch_size]</span><br><span class="line">        inputs = torch.tensor(batch_x, dtype=torch.float32)</span><br><span class="line">        targets = torch.tensor(batch_y, dtype=torch.float32)</span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        outputs = model(inputs)</span><br><span class="line">        loss = criterion(outputs, targets)</span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line">    <span class="keyword">if</span> (epoch+<span class="number">1</span>) % <span class="number">10</span> == <span class="number">0</span>:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&#x27;Epoch [<span class="subst">&#123;epoch+<span class="number">1</span>&#125;</span>/<span class="subst">&#123;num_epochs&#125;</span>], Loss: <span class="subst">&#123;loss.item()&#125;</span>&#x27;</span>)</span><br><span class="line"><span class="comment"># 测试模型</span></span><br><span class="line"><span class="comment"># 假设x_test是你要进行预测的时间序列数据</span></span><br><span class="line">x_test = np.random.rand(<span class="number">1</span>, <span class="number">1</span>, <span class="number">4</span>)</span><br><span class="line">inputs = torch.tensor(x_test, dtype=torch.float32)</span><br><span class="line">predicted = model(inputs)</span><br><span class="line"><span class="built_in">print</span>(predicted)</span><br></pre></td></tr></table></figure></li></ol><h2 id="深度循环神经网络"><a href="#深度循环神经网络" class="headerlink" title="深度循环神经网络"></a>深度循环神经网络</h2><ol><li>尝试从零开始实现两层循环神经网络。<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">TwoLayerLSTM</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, input_size, hidden_size, num_layers, output_size</span>):</span><br><span class="line">        <span class="built_in">super</span>(TwoLayerLSTM, self).__init__()</span><br><span class="line">        self.hidden_size = hidden_size</span><br><span class="line">        self.num_layers = num_layers</span><br><span class="line">        self.lstm_layers = nn.ModuleList([nn.LSTM(input_size, hidden_size, num_layers, batch_first=<span class="literal">True</span>) <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">2</span>)])</span><br><span class="line">        self.output_layer = nn.Linear(hidden_size, output_size)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        lstm_out, (hidden, cell) = self.lstm_layers[<span class="number">0</span>](x)</span><br><span class="line">        lstm_out, (hidden, cell) = self.lstm_layers[<span class="number">1</span>](lstm_out)</span><br><span class="line">        output = self.output_layer(lstm_out[:, -<span class="number">1</span>, :])</span><br><span class="line">        <span class="keyword">return</span> output, (hidden, cell)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 假设输入数据的维度为10，隐藏层大小为20，输出大小为1</span></span><br><span class="line">model = TwoLayerLSTM(input_size=<span class="number">10</span>, hidden_size=<span class="number">20</span>, num_layers=<span class="number">2</span>, output_size=<span class="number">1</span>)</span><br></pre></td></tr></table></figure></li><li>在本节训练模型中，比较使用门控循环单元替换长短期记忆网络后模型的精确度和训练速度。<br> 在实际应用中，GRU通常比LSTM有更快的训练速度，因为GRU的门控机制相对简单，参数数量较少。然而，LSTM通常在处理长期依赖关系方面表现得更好，因为它具有更复杂的门控机制。在比较两者的精确度和训练速度时，需要根据具体任务和数据集进行实验。一般来说，如果任务对长期依赖关系的要求不高，GRU可能是一个更快、更高效的选择。如果任务需要捕捉复杂的长期依赖关系，LSTM可能是更好的选择。</li><li>如果增加训练数据，能够将困惑度降到多低？<br> 增加训练数据通常有助于提高模型的性能，包括降低困惑度。困惑度是衡量模型对测试数据的不确定性的指标，较低的困惑度意味着模型对数据的预测更准确。理论上，随着训练数据的增加，模型可以学习到更多的模式和依赖关系，从而提高其对测试数据的预测能力。然而，降低困惑度的幅度也受到数据质量、模型容量和训练策略等因素的影响。</li><li>在为文本建模时，是否可以将不同作者的源数据合并？有何优劣呢？<br> 可以增加数据的多样性和丰富性，有助于提高模型的泛化能力。然而，不同作者的写作风格和用词习惯可能存在显著差异，这可能会影响模型的训练效果。</li></ol><h2 id="双向循环神经网络"><a href="#双向循环神经网络" class="headerlink" title="双向循环神经网络"></a>双向循环神经网络</h2><h3 id="练习-32"><a href="#练习-32" class="headerlink" title="练习"></a>练习</h3><ol><li>设计一个具有多个隐藏层的双向循环神经网络。<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MultiLayerBiLSTM</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, input_size, hidden_size, num_layers, output_size</span>):</span><br><span class="line">        <span class="built_in">super</span>(MultiLayerBiLSTM, self).__init__()</span><br><span class="line">        self.hidden_size = hidden_size</span><br><span class="line">        self.num_layers = num_layers</span><br><span class="line">        self.bi_lstm = nn.ModuleList([</span><br><span class="line">            nn.LSTM(input_size, hidden_size, num_layers, batch_first=<span class="literal">True</span>, bidirectional=<span class="literal">True</span>)</span><br><span class="line">            <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(num_layers)</span><br><span class="line">        ])</span><br><span class="line">        self.output_layer = nn.Linear(hidden_size * <span class="number">2</span>, output_size)  <span class="comment"># 双向输出需要合并</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        outputs = []</span><br><span class="line">        <span class="keyword">for</span> lstm <span class="keyword">in</span> self.bi_lstm:</span><br><span class="line">            lstm_out, (hidden, cell) = lstm(x)</span><br><span class="line">            outputs.append(lstm_out)</span><br><span class="line">            x = lstm_out</span><br><span class="line">        <span class="comment"># 合并所有隐藏层的输出</span></span><br><span class="line">        combined_output = torch.cat(outputs, dim=-<span class="number">1</span>)</span><br><span class="line">        output = self.output_layer(combined_output[:, -<span class="number">1</span>, :])</span><br><span class="line">        <span class="keyword">return</span> output</span><br><span class="line"></span><br><span class="line"><span class="comment"># 假设输入数据的维度为10，隐藏层大小为20，输出大小为1</span></span><br><span class="line">model = MultiLayerBiLSTM(input_size=<span class="number">10</span>, hidden_size=<span class="number">20</span>, num_layers=<span class="number">2</span>, output_size=<span class="number">1</span>)</span><br></pre></td></tr></table></figure></li><li>在自然语言中一词多义很常见。例如，“bank”一词在不同的上下文“i went to the bank to deposit cash”和“i went to the bank to sit down”中有不同的含义。如何设计一个神经网络模型，使其在给定上下文序列和单词的情况下，返回该单词在此上下文中的向量表示？哪种类型的神经网络架构更适合处理一词多义？<br> 可以使用编码器-解码器（Encoder-Decoder）架构，其中编码器负责生成上下文的表示，解码器则利用这个表示来生成目标单词的向量。<br> 模型设计：<ul><li><strong>上下文编码器</strong>：使用Bi-LSTM或Transformer架构来编码输入句子的上下文信息。这种双向结构能够捕捉到每个词前后的依赖关系，从而为每个词生成一个包含上下文信息的向量表示。</li><li><strong>注意力机制</strong>：在解码器中引入注意力机制，它可以动态地聚焦于编码器输出的上下文表示中最相关的部分。这样，模型可以根据当前处理的单词来调整其对上下文的关注点。</li><li><strong>解码器</strong>：解码器可以是另一个LSTM或GRU层，它使用注意力加权的上下文向量来生成目标单词的表示。</li></ul></li></ol><h2 id="机器翻译与数据集"><a href="#机器翻译与数据集" class="headerlink" title="机器翻译与数据集"></a>机器翻译与数据集</h2><h3 id="练习-33"><a href="#练习-33" class="headerlink" title="练习"></a>练习</h3><ol><li>在<code>load_data_nmt</code>函数中尝试不同的<code>num_examples</code>参数值。这对源语言和目标语言的词表大小有何影响？<br> 在<code>load_data_nmt</code>函数中尝试不同的<code>num_examples</code>参数值会影响源语言和目标语言的词表大小。<code>num_examples</code>参数决定了从数据集中抽取多少个样本来构建词表。如果<code>num_examples</code>的值较小，那么词表可能会不够全面，因为它只考虑了较少的样本。这可能导致一些单词没有被包含在词表中，特别是那些较少出现的单词。相反，如果<code>num_examples</code>的值较大，词表将更加全面，因为它考虑了更多的样本，从而可能包含更多的单词。然而，一个非常大的<code>num_examples</code>值也可能导致词表过于庞大，包含许多在特定任务中并不重要的单词。</li><li>某些语言（例如中文和日语）的文本没有单词边界指示符（例如空格）。对于这种情况，单词级词元化仍然是个好主意吗？为什么？<br> 子词级词元化可以帮助模型更好地理解文本的结构，同时避免了将单词错误地分割的问题。此外，使用基于词典或基于统计的分词方法可以有效地处理这些语言的文本，因为这些方法能够识别出有意义的词汇单元，而不是简单地基于字符进行分割。这样，模型可以更准确地捕捉到语言的特征，并提高处理效果。</li></ol><h2 id="编码器-解码器结构"><a href="#编码器-解码器结构" class="headerlink" title="编码器-解码器结构"></a>编码器-解码器结构</h2><p>机器翻译是序列转换模型的一个核心问题， 其输入和输出都是长度可变的序列。 为了处理这种类型的输入和输出， 我们可以设计一个包含两个主要组件的架构： 第一个组件是一个<em>编码器</em>（encoder）： 它接受一个长度可变的序列作为输入， 并将其转换为具有固定形状的编码状态。 第二个组件是<em>解码器</em>（decoder）： 它将固定形状的编码状态映射到长度可变的序列。 这被称为编码器-解码器（encoder-decoder）架构。</p><h3 id="编码器"><a href="#编码器" class="headerlink" title="编码器"></a>编码器</h3><p>在编码器接口中，我们只指定长度可变的序列作为编码器的输入<code>X</code>。 任何继承这个<code>Encoder</code>基类的模型将完成代码实现。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="comment">#@save</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Encoder</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;编码器-解码器架构的基本编码器接口&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, **kwargs</span>):</span><br><span class="line">        <span class="built_in">super</span>(Encoder, self).__init__(**kwargs)</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, X, *args</span>):</span><br><span class="line">        <span class="keyword">raise</span> NotImplementedError</span><br></pre></td></tr></table></figure><br>解码器<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#@save</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Decoder</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;编码器-解码器架构的基本解码器接口&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, **kwargs</span>):</span><br><span class="line">        <span class="built_in">super</span>(Decoder, self).__init__(**kwargs)</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">init_state</span>(<span class="params">self, enc_outputs, *args</span>):</span><br><span class="line">        <span class="keyword">raise</span> NotImplementedError</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, X, state</span>):</span><br><span class="line">        <span class="keyword">raise</span> NotImplementedError</span><br></pre></td></tr></table></figure></p><h3 id="合并编码器和解码器"><a href="#合并编码器和解码器" class="headerlink" title="合并编码器和解码器"></a>合并编码器和解码器</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#@save</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">EncoderDecoder</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;编码器-解码器架构的基类&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, encoder, decoder, **kwargs</span>):</span><br><span class="line">        <span class="built_in">super</span>(EncoderDecoder, self).__init__(**kwargs)</span><br><span class="line">        self.encoder = encoder</span><br><span class="line">        self.decoder = decoder</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, enc_X, dec_X, *args</span>):</span><br><span class="line">        enc_outputs = self.encoder(enc_X, *args)</span><br><span class="line">        dec_state = self.decoder.init_state(enc_outputs, *args)</span><br><span class="line">        <span class="keyword">return</span> self.decoder(dec_X, dec_state)</span><br></pre></td></tr></table></figure><h3 id="练习-34"><a href="#练习-34" class="headerlink" title="练习"></a>练习</h3><ol><li>假设我们使用神经网络来实现“编码器－解码器”架构，那么编码器和解码器必须是同一类型的神经网络吗？<br> 不一定是同一类型的网络</li><li>除了机器翻译，还有其它可以适用于”编码器－解码器“架构的应用吗？<br> 语音识别，图像描述，视频内容理解，聊天对话模型</li></ol><h2 id="序列到序列学习（seq2seq）"><a href="#序列到序列学习（seq2seq）" class="headerlink" title="序列到序列学习（seq2seq）"></a>序列到序列学习（seq2seq）</h2><p>遵循编码器－解码器架构的设计原则， 循环神经网络编码器使用长度可变的序列作为输入， 将其转换为固定形状的隐状态。 换言之，输入序列的信息被<em>编码</em>到循环神经网络编码器的隐状态中。 为了连续生成输出序列的词元， 独立的循环神经网络解码器是基于输入序列的编码信息 和输出序列已经看见的或者生成的词元来预测下一个词元。</p><h3 id="练习-35"><a href="#练习-35" class="headerlink" title="练习"></a>练习</h3><ol><li>重新运行实验并在计算损失时不使用遮蔽，可以观察到什么结果？为什么会有这个结果？<br> 模型可能会在预测时错误地将注意力放在填充的位置上。这会导致模型学习到错误的依赖关系，因为它把填充的部分也当作了有效的输入。结果通常是模型性能下降，因为它不能正确地区分真实的序列数据和用于保持序列长度一致性的填充数据。遮蔽的目的是告诉模型哪些位置是真实的数据，哪些是填充数据，从而确保模型只在有效的数据上进行学习。</li><li>如果编码器和解码器的层数或者隐藏单元数不同，那么如何初始化解码器的隐状态？<br> 解码器的隐状态通常可以从编码器的最后一个时间步的隐状态进行初始化。这样做是因为编码器的最终状态被认为包含了整个输入序列的上下文信息，它可以作为解码器的初始上下文。</li><li>在训练中，如果用前一时间步的预测输入到解码器来代替强制教学，对性能有何影响？<br> 这可能会导致模型性能下降。teacher forcing是一种训练策略，它迫使模型在每个时间步使用真实的目标词作为输入，即使这些目标词在推理时可能不可用。如果移除这个策略，模型可能会生成不准确的预测，因为它不再依赖于真实的目标序列进行学习，而是依赖于自己的预测。这可能导致模型陷入错误累积的循环，从而降低生成序列的准确性。</li><li>有没有其他方法来设计解码器的输出层？</li></ol><ul><li>注意力机制：通过注意力机制，输出层可以聚焦于输入序列中的相关部分来生成每个输出。</li><li>层归一化：在输出层之前使用层归一化可以帮助稳定训练过程。</li><li>残差连接：引入残差连接可以帮助信息在网络中更有效地流动，防止梯度消失。</li><li>不同的激活函数：根据任务的不同，可以尝试使用不同的激活函数，如tanh或ReLU，来改善模型的性能。</li></ul><h2 id="束搜索"><a href="#束搜索" class="headerlink" title="束搜索"></a>束搜索</h2><h3 id="贪心搜索"><a href="#贪心搜索" class="headerlink" title="贪心搜索"></a>贪心搜索</h3><p>它在每一步都选择当前看起来最优的选择，而不考虑长远的后果。这种方法简单且易于实现，但在某些情况下可能无法找到全局最优解，因为它可能在早期步骤中就锁定了次优的路径。<br>在机器翻译中，贪心搜索可能会在每个时间步选择最有可能的单词作为翻译的一部分，直到生成完整的句子。然而，这种方法可能会导致输出的连贯性和准确性问题，因为它没有考虑整个句子的全局最优性。<br>在训练过程中，贪心搜索可以用于解码器的输出，其中模型在每个时间步生成一个输出，而不是等待整个序列生成完成。这种方法的优点是速度快，但可能导致生成的序列质量不高，因为它没有利用后续时间步的信息来优化当前的输出。</p><h3 id="穷举搜索"><a href="#穷举搜索" class="headerlink" title="穷举搜索"></a>穷举搜索</h3><p>如果目标是获得最优序列， 我们可以考虑使用<em>穷举搜索</em>（exhaustive search）： 穷举地列举所有可能的输出序列及其条件概率， 然后计算输出条件概率最高的一个。</p><h3 id="束搜索-1"><a href="#束搜索-1" class="headerlink" title="束搜索"></a>束搜索</h3><p><em>束搜索</em>（beam search）是贪心搜索的一个改进版本。 它有一个超参数，名为束宽（beam size）k。 在时间步1，我们选择具有最高条件概率的k个词元。 这k个词元将分别是k个候选输出序列的第一个词元。 在随后的每个时间步，基于上一时间步的k个候选输出序列， 我们将继续从k|y个可能的选择中 挑出具有最高条件概率的k个候选输出序列。<br>![[Pasted image 20240406202313.png]]</p><h3 id="练习-36"><a href="#练习-36" class="headerlink" title="练习"></a>练习</h3><ol><li>我们可以把穷举搜索看作一种特殊的束搜索吗？为什么？<br> 穷举搜索可以被看作是束宽为1的特殊束搜索。在穷举搜索中，搜索算法会考虑所有可能的候选解，直到找到最优解或满足某些条件为止。相比之下，束搜索在每一步只保留一定数量的最佳候选解（即束宽），并在此基础上继续搜索。束宽为1时，束搜索在每一步只保留一个候选解，这与穷举搜索中考虑所有可能解的方式非常相似。</li><li>在 <a href="https://zh-v2.d2l.ai/chapter_recurrent-modern/seq2seq.html#sec-seq2seq">9.7节</a>的机器翻译问题中应用束搜索。 束宽是如何影响预测的速度和结果的？<br> 束宽决定了在每一步保留的候选解的数量：<ul><li>较小的束宽（如1）可以加快搜索速度，因为需要评估的候选解数量较少。但这也可能导致搜索过程错过一些优质的候选解，从而影响翻译质量。</li><li>较大的束宽可以增加找到高质量翻译的概率，因为它考虑了更多的候选解。然而，这也意味着需要更多的计算资源和时间来评估这些候选解，从而减慢搜索速度。 因此，选择合适的束宽需要在搜索质量和效率之间做出权衡。<h1 id="注意力机制"><a href="#注意力机制" class="headerlink" title="注意力机制"></a>注意力机制</h1>“是否包含自主性提示”将注意力机制与全连接层或汇聚层区别开来。 在注意力机制的背景下，自主性提示被称为查询（query）。 给定任何查询，注意力机制通过注意力汇聚（attention pooling） 将选择引导至<em>感官输入</em>（sensory inputs，例如中间特征表示）。 在注意力机制中，这些感官输入被称为值（value）。 更通俗的解释，每个值都与一个键（key）配对， 这可以想象为感官输入的非自主提示。<h3 id="练习-37"><a href="#练习-37" class="headerlink" title="练习"></a>练习</h3></li></ul></li><li>在机器翻译中通过解码序列词元时，其自主性提示可能是什么？非自主性提示和感官输入又是什么？<br> 在机器翻译的上下文中，自主性提示通常指的是模型在生成翻译时依赖于其先前生成的输出，而不是外部输入。例如，在解码过程中，模型可能会根据已经生成的词序列来决定下一个最合适的词。这种提示体现了模型在生成翻译时的自主性和连贯性，因为它完全依赖于内部状态和已生成的序列。</li><li>随机生成一个10×10矩阵并使用<code>softmax</code>运算来确保每行都是有效的概率分布，然后可视化输出注意力权重。<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="comment"># 随机生成一个3x5的矩阵，代表注意力权重</span></span><br><span class="line">attention_weights = np.random.rand(<span class="number">3</span>, <span class="number">5</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用softmax运算来归一化每行，确保它们是有效的概率分布</span></span><br><span class="line">attention_distribution = np.exp(attention_weights) / np.<span class="built_in">sum</span>(np.exp(attention_weights), axis=<span class="number">1</span>, keepdims=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 可视化注意力权重</span></span><br><span class="line">plt.figure(figsize=(<span class="number">10</span>, <span class="number">5</span>))</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(attention_distribution.shape[<span class="number">0</span>]):</span><br><span class="line">    plt.plot(<span class="built_in">range</span>(<span class="number">1</span>, attention_distribution.shape[<span class="number">1</span>] + <span class="number">1</span>), attention_distribution[i, :], label=<span class="string">f&#x27;Decoder Position <span class="subst">&#123;i+<span class="number">1</span>&#125;</span>&#x27;</span>)</span><br><span class="line">plt.xlabel(<span class="string">&#x27;Position&#x27;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;Attention Weight&#x27;</span>)</span><br><span class="line">plt.title(<span class="string">&#x27;Attention Weights Distribution&#x27;</span>)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><h2 id="注意力汇聚：Nadaraya-Watson核回归"><a href="#注意力汇聚：Nadaraya-Watson核回归" class="headerlink" title="注意力汇聚：Nadaraya-Watson核回归"></a>注意力汇聚：Nadaraya-Watson核回归</h2><h3 id="生成数据集"><a href="#生成数据集" class="headerlink" title="生成数据集"></a>生成数据集</h3>给定的成对的输入数据集，根据某非线性函数生成一个人工数据集，加入噪声项。生成50个训练样本和50个测试样本。<h3 id="平均汇聚"><a href="#平均汇聚" class="headerlink" title="平均汇聚"></a>平均汇聚</h3>先使用最简单的估计器来解决回归问题。 基于平均汇聚来计算所有训练样本输出值的平均值<h3 id="非参数注意力汇聚"><a href="#非参数注意力汇聚" class="headerlink" title="非参数注意力汇聚"></a>非参数注意力汇聚</h3>显然，平均汇聚忽略了输入xi。 于是Nadaraya 和 Watson提出了一个更好的想法，根据输入的位置对输出yi进行加权。<script type="math/tex; mode=display">f(x)=\sum_{i=1}^{n}\frac{K(x-x_i}{ {\textstyle \sum_{j=1}^{n}}K(x-x_j) }y_i</script>其中KI是核（kernel）<h3 id="带参数注意力汇聚"><a href="#带参数注意力汇聚" class="headerlink" title="带参数注意力汇聚"></a>带参数注意力汇聚</h3>非参数的Nadaraya-Watson核回归具有<em>一致性</em>（consistency）的优点： 如果有足够的数据，此模型会收敛到最优结果。 尽管如此，我们还是可以轻松地将可学习的参数集成到注意力汇聚中。<h3 id="批量矩阵乘法"><a href="#批量矩阵乘法" class="headerlink" title="批量矩阵乘法"></a>批量矩阵乘法</h3>为了更有效地计算小批量数据的注意力， 我们可以利用深度学习开发框架中提供的批量矩阵乘法。<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">X = torch.ones((<span class="number">2</span>, <span class="number">1</span>, <span class="number">4</span>))</span><br><span class="line">Y = torch.ones((<span class="number">2</span>, <span class="number">4</span>, <span class="number">6</span>))</span><br><span class="line">torch.bmm(X, Y).shape</span><br></pre></td></tr></table></figure>在注意力机制的背景中，我们可以使用小批量矩阵乘法来计算小批量数据中的加权平均值。<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">weights = torch.ones((<span class="number">2</span>, <span class="number">10</span>)) * <span class="number">0.1</span></span><br><span class="line">values = torch.arange(<span class="number">20.0</span>).reshape((<span class="number">2</span>, <span class="number">10</span>))</span><br><span class="line">torch.bmm(weights.unsqueeze(<span class="number">1</span>), values.unsqueeze(-<span class="number">1</span>))</span><br></pre></td></tr></table></figure><h3 id="练习-38"><a href="#练习-38" class="headerlink" title="练习"></a>练习</h3></li><li>增加训练数据的样本数量，能否得到更好的非参数的Nadaraya-Watson核回归模型？<br> 增加训练数据的样本数量通常能够提升非参数的Nadaraya-Watson核回归模型的性能。非参数方法不对数据的基础结构做任何先验假设，因此它们能够从更多的数据中学习并捕捉到更复杂的模式。</li><li>在带参数的注意力汇聚的实验中学习得到的参数w的价值是什么？为什么在可视化注意力权重时，它会使加权区域更加尖锐？<br> 在带参数的注意力汇聚中，学习到的参数可以捕捉输入数据中的重要特征和结构。这些参数有助于模型更好地理解哪些部分的输入对于预测任务最为关键。注意力权重的可视化通常会显示出一个加权区域，这个区域突出了输入数据中与预测目标最相关的部分。当注意力权重集中在一个较小的、尖锐的区域时，这意味着模型已经学习到专注于输入数据中的特定部分，从而提高了预测的准确性和解释性。</li><li>如何将超参数添加到非参数的Nadaraya-Watson核回归中以实现更好地预测结果？<br> 可以考虑引入正则化项或者调整核函数的带宽。正则化可以帮助防止过拟合，而带宽参数则控制了模型对数据局部波动的平滑程度。通过调整这些超参数，可以优化模型以适应特定的预测任务。</li><li>为本节的核回归设计一个新的带参数的注意力汇聚模型。训练这个新模型并可视化其注意力权重。<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">ParameterizedAttentionModel</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, input_dim, attention_dim</span>):</span><br><span class="line">        <span class="built_in">super</span>(ParameterizedAttentionModel, self).__init__()</span><br><span class="line">        self.query LinearLayer(input_dim, attention_dim)</span><br><span class="line">        self.key LinearLayer(input_dim, attention_dim)</span><br><span class="line">        self.value LinearLayer(input_dim, attention_dim)</span><br><span class="line">        self.output LinearLayer(attention_dim, <span class="number">1</span>)</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, inputs</span>):</span><br><span class="line">        queries = self.query(inputs)</span><br><span class="line">        keys = self.key(inputs)</span><br><span class="line">        values = self.value(inputs)</span><br><span class="line">        attention_scores = torch.matmul(queries, keys.transpose(-<span class="number">2</span>, -<span class="number">1</span>))</span><br><span class="line">        attention_weights = F.softmax(attention_scores, dim=-<span class="number">1</span>)</span><br><span class="line">        attended_values = torch.matmul(attention_weights, values)</span><br><span class="line">        output = self.output(attended_values)</span><br><span class="line">        <span class="keyword">return</span> output</span><br><span class="line"><span class="comment"># 假设输入维度为10，注意力维度为5</span></span><br><span class="line">model = ParameterizedAttentionModel(input_dim=<span class="number">10</span>, attention_dim=<span class="number">5</span>)</span><br></pre></td></tr></table></figure><h2 id="注意力评分函数"><a href="#注意力评分函数" class="headerlink" title="注意力评分函数"></a>注意力评分函数</h2><a href="https://zh-v2.d2l.ai/chapter_attention-mechanisms/nadaraya-waston.html#equation-eq-nadaraya-watson-gaussian">(10.2.6)</a>中的 高斯核指数部分可以视为<em>注意力评分函数</em>（attention scoring function）， 简称评分函数（scoring function）， 然后把这个函数的输出结果输入到softmax函数中进行运算。 通过上述步骤，将得到与键对应的值的概率分布（即注意力权重）。 最后，注意力汇聚的输出就是基于这些注意力权重的值的加权和。<br>![[Pasted image 20240408043455.png]]<h3 id="掩蔽softmax操作"><a href="#掩蔽softmax操作" class="headerlink" title="掩蔽softmax操作"></a>掩蔽softmax操作</h3>softmax操作用于输出一个概率分布作为注意力权重。 在某些情况下，并非所有的值都应该被纳入到注意力汇聚中。<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#@save</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">masked_softmax</span>(<span class="params">X, valid_lens</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;通过在最后一个轴上掩蔽元素来执行softmax操作&quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># X:3D张量，valid_lens:1D或2D张量</span></span><br><span class="line">    <span class="keyword">if</span> valid_lens <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">        <span class="keyword">return</span> nn.functional.softmax(X, dim=-<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        shape = X.shape</span><br><span class="line">        <span class="keyword">if</span> valid_lens.dim() == <span class="number">1</span>:</span><br><span class="line">            valid_lens = torch.repeat_interleave(valid_lens, shape[<span class="number">1</span>])</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            valid_lens = valid_lens.reshape(-<span class="number">1</span>)</span><br><span class="line">        <span class="comment"># 最后一轴上被掩蔽的元素使用一个非常大的负值替换，从而其softmax输出为0</span></span><br><span class="line">        X = d2l.sequence_mask(X.reshape(-<span class="number">1</span>, shape[-<span class="number">1</span>]), valid_lens,</span><br><span class="line">                              value=-<span class="number">1e6</span>)</span><br><span class="line">        <span class="keyword">return</span> nn.functional.softmax(X.reshape(shape), dim=-<span class="number">1</span>)</span><br></pre></td></tr></table></figure>为了演示此函数是如何工作的， 考虑由两个2×4矩阵表示的样本， 这两个样本的有效长度分别为2和3。 经过掩蔽softmax操作，超出有效长度的值都被掩蔽为0。<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">masked_softmax(torch.rand(<span class="number">2</span>, <span class="number">2</span>, <span class="number">4</span>), torch.tensor([<span class="number">2</span>, <span class="number">3</span>]))</span><br></pre></td></tr></table></figure><h3 id="加性注意力"><a href="#加性注意力" class="headerlink" title="加性注意力"></a>加性注意力</h3>一般来说，当查询和键是不同长度的矢量时，可以使用加性注意力作为评分函数。<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#@save</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">AdditiveAttention</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;加性注意力&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, key_size, query_size, num_hiddens, dropout, **kwargs</span>):</span><br><span class="line">        <span class="built_in">super</span>(AdditiveAttention, self).__init__(**kwargs)</span><br><span class="line">        self.W_k = nn.Linear(key_size, num_hiddens, bias=<span class="literal">False</span>)</span><br><span class="line">        self.W_q = nn.Linear(query_size, num_hiddens, bias=<span class="literal">False</span>)</span><br><span class="line">        self.w_v = nn.Linear(num_hiddens, <span class="number">1</span>, bias=<span class="literal">False</span>)</span><br><span class="line">        self.dropout = nn.Dropout(dropout)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, queries, keys, values, valid_lens</span>):</span><br><span class="line">        queries, keys = self.W_q(queries), self.W_k(keys)</span><br><span class="line">        <span class="comment"># 在维度扩展后，</span></span><br><span class="line">        <span class="comment"># queries的形状：(batch_size，查询的个数，1，num_hidden)</span></span><br><span class="line">        <span class="comment"># key的形状：(batch_size，1，“键－值”对的个数，num_hiddens)</span></span><br><span class="line">        <span class="comment"># 使用广播方式进行求和</span></span><br><span class="line">        features = queries.unsqueeze(<span class="number">2</span>) + keys.unsqueeze(<span class="number">1</span>)</span><br><span class="line">        features = torch.tanh(features)</span><br><span class="line">        <span class="comment"># self.w_v仅有一个输出，因此从形状中移除最后那个维度。</span></span><br><span class="line">        <span class="comment"># scores的形状：(batch_size，查询的个数，“键-值”对的个数)</span></span><br><span class="line">        scores = self.w_v(features).squeeze(-<span class="number">1</span>)</span><br><span class="line">        self.attention_weights = masked_softmax(scores, valid_lens)</span><br><span class="line">        <span class="comment"># values的形状：(batch_size，“键－值”对的个数，值的维度)</span></span><br><span class="line">        <span class="keyword">return</span> torch.bmm(self.dropout(self.attention_weights), values)</span><br></pre></td></tr></table></figure>用一个小例子来演示上面的<code>AdditiveAttention</code>类， 其中查询、键和值的形状为（批量大小，步数或词元序列长度，特征大小）， 实际输出为(2,1,20)、(2,10,2)和(2,10,4)。 注意力汇聚输出的形状为（批量大小，查询的步数，值的维度）。<h3 id="练习-39"><a href="#练习-39" class="headerlink" title="练习"></a>练习</h3></li><li>修改小例子中的键，并且可视化注意力权重。可加性注意力和缩放的“点－积”注意力是否仍然产生相同的结果？为什么？<br> 我们首先需要了解注意力权重是如何计算的。在典型的注意力机制中，如“点-积”注意力，权重是通过查询（Q）和键（K）的点积来计算的，然后通常会除以一个缩放因子，最后应用softmax函数来获得概率分布。如果我们要修改键，我们需要确保修改后的键仍然能够与查询进行有效的交互，以便计算出有意义的注意力权重。可视化注意力权重通常涉及到将权重矩阵以图形的形式展示出来，这样可以直观地看到模型在处理输入序列时对不同位置的关注度。</li><li>只使用矩阵乘法，能否为具有不同矢量长度的查询和键设计新的评分函数？<br> 只使用矩阵乘法可能不足以处理具有不同矢量长度的查询和键。在标准的注意力机制中，查询和键的长度通常是相同的，以便进行点积操作。如果查询和键的长度不同，可能需要引入额外的步骤来调整它们的长度，或者设计一个能够处理不同长度输入的评分函数。例如，可以通过嵌入层将查询和键映射到相同长度的空间，或者设计一个基于对齐或匹配的评分函数，而不是简单的点积。</li><li>当查询和键具有相同的矢量长度时，矢量求和作为评分函数是否比“点－积”更好？为什么？<br> 当查询和键具有相同的矢量长度时，矢量求和作为评分函数可能不如“点-积”注意力有效。原因在于点-积能够更好地捕捉查询和键之间的相似性，因为它考虑了查询和键的每个维度之间的相互作用。而矢量求和可能会忽略这些维度之间的相互作用，导致无法有效地捕捉查询和键之间的关系。<h2 id="Bahdanau-注意力"><a href="#Bahdanau-注意力" class="headerlink" title="Bahdanau 注意力"></a>Bahdanau 注意力</h2><h3 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h3>![[Pasted image 20240408043946.png]]<h3 id="定义注意力解码器"><a href="#定义注意力解码器" class="headerlink" title="定义注意力解码器"></a>定义注意力解码器</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#@save</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">AttentionDecoder</span>(d2l.Decoder):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;带有注意力机制解码器的基本接口&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, **kwargs</span>):</span><br><span class="line">        <span class="built_in">super</span>(AttentionDecoder, self).__init__(**kwargs)</span><br><span class="line"></span><br><span class="line"><span class="meta">    @property</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">attention_weights</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">raise</span> NotImplementedError</span><br></pre></td></tr></table></figure>接下来，让我们在接下来的<code>Seq2SeqAttentionDecoder</code>类中 实现带有Bahdanau注意力的循环神经网络解码器。 首先，初始化解码器的状态，需要下面的输入：</li><li>编码器在所有时间步的最终层隐状态，将作为注意力的键和值；</li><li>上一时间步的编码器全层隐状态，将作为初始化解码器的隐状态；</li><li>编码器有效长度（排除在注意力池中填充词元）。<br>在每个解码时间步骤中，解码器上一个时间步的最终层隐状态将用作查询。 因此，注意力输出和输入嵌入都连结为循环神经网络解码器的输入。<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Seq2SeqAttentionDecoder</span>(<span class="title class_ inherited__">AttentionDecoder</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, vocab_size, embed_size, num_hiddens, num_layers,</span></span><br><span class="line"><span class="params">                 dropout=<span class="number">0</span>, **kwargs</span>):</span><br><span class="line">        <span class="built_in">super</span>(Seq2SeqAttentionDecoder, self).__init__(**kwargs)</span><br><span class="line">        self.attention = d2l.AdditiveAttention(</span><br><span class="line">            num_hiddens, num_hiddens, num_hiddens, dropout)</span><br><span class="line">        self.embedding = nn.Embedding(vocab_size, embed_size)</span><br><span class="line">        self.rnn = nn.GRU(</span><br><span class="line">            embed_size + num_hiddens, num_hiddens, num_layers,</span><br><span class="line">            dropout=dropout)</span><br><span class="line">        self.dense = nn.Linear(num_hiddens, vocab_size)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">init_state</span>(<span class="params">self, enc_outputs, enc_valid_lens, *args</span>):</span><br><span class="line">        <span class="comment"># outputs的形状为(batch_size，num_steps，num_hiddens).</span></span><br><span class="line">        <span class="comment"># hidden_state的形状为(num_layers，batch_size，num_hiddens)</span></span><br><span class="line">        outputs, hidden_state = enc_outputs</span><br><span class="line">        <span class="keyword">return</span> (outputs.permute(<span class="number">1</span>, <span class="number">0</span>, <span class="number">2</span>), hidden_state, enc_valid_lens)</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, X, state</span>):</span><br><span class="line">        <span class="comment"># enc_outputs的形状为(batch_size,num_steps,num_hiddens).</span></span><br><span class="line">        <span class="comment"># hidden_state的形状为(num_layers,batch_size,</span></span><br><span class="line">        <span class="comment"># num_hiddens)</span></span><br><span class="line">        enc_outputs, hidden_state, enc_valid_lens = state</span><br><span class="line">        <span class="comment"># 输出X的形状为(num_steps,batch_size,embed_size)</span></span><br><span class="line">        X = self.embedding(X).permute(<span class="number">1</span>, <span class="number">0</span>, <span class="number">2</span>)</span><br><span class="line">        outputs, self._attention_weights = [], []</span><br><span class="line">        <span class="keyword">for</span> x <span class="keyword">in</span> X:</span><br><span class="line">            <span class="comment"># query的形状为(batch_size,1,num_hiddens)</span></span><br><span class="line">            query = torch.unsqueeze(hidden_state[-<span class="number">1</span>], dim=<span class="number">1</span>)</span><br><span class="line">            <span class="comment"># context的形状为(batch_size,1,num_hiddens)</span></span><br><span class="line">            context = self.attention(</span><br><span class="line">                query, enc_outputs, enc_outputs, enc_valid_lens)</span><br><span class="line">            <span class="comment"># 在特征维度上连结</span></span><br><span class="line">            x = torch.cat((context, torch.unsqueeze(x, dim=<span class="number">1</span>)), dim=-<span class="number">1</span>)</span><br><span class="line">            <span class="comment"># 将x变形为(1,batch_size,embed_size+num_hiddens)</span></span><br><span class="line">            out, hidden_state = self.rnn(x.permute(<span class="number">1</span>, <span class="number">0</span>, <span class="number">2</span>), hidden_state)</span><br><span class="line">            outputs.append(out)</span><br><span class="line">            self._attention_weights.append(self.attention.attention_weights)</span><br><span class="line">        <span class="comment"># 全连接层变换后，outputs的形状为</span></span><br><span class="line">        <span class="comment"># (num_steps,batch_size,vocab_size)</span></span><br><span class="line">        outputs = self.dense(torch.cat(outputs, dim=<span class="number">0</span>))</span><br><span class="line">        <span class="keyword">return</span> outputs.permute(<span class="number">1</span>, <span class="number">0</span>, <span class="number">2</span>), [enc_outputs, hidden_state,</span><br><span class="line">                                          enc_valid_lens]</span><br><span class="line"><span class="meta">    @property</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">attention_weights</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> self._attention_weights</span><br></pre></td></tr></table></figure><h2 id="多头注意力"><a href="#多头注意力" class="headerlink" title="多头注意力"></a>多头注意力</h2>多头注意力（Multi-head Attention）是一种用于增强神经网络模型对序列数据（如文本或时间序列）建模能力的注意力机制。它是 Transformer 模型中的关键组件之一。</li></ol><p>在传统的注意力机制中，模型通过计算查询（query）、键（key）和值（value）之间的关联来为每个查询选择相关的值。而多头注意力通过引入多组查询-键-值投影矩阵，从而使模型能够在不同的表示空间中学习关注不同方面的信息。每个头产生的注意力权重矩阵被独立地计算，然后这些矩阵被拼接在一起并经过另一个线性变换，最终得到多头注意力的输出。</p><p>多头注意力的主要优势在于它可以让模型在不同的表示子空间中学习到不同的语义信息，从而提高模型对复杂关系的建模能力。在 Transformer 中，多头注意力被用来同时捕捉不同位置的关系和不同层次的语义信息，使得模型能够更好地处理长距离依赖性。</p><h3 id="实现"><a href="#实现" class="headerlink" title="实现"></a>实现</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#@save</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MultiHeadAttention</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;多头注意力&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, key_size, query_size, value_size, num_hiddens,</span></span><br><span class="line"><span class="params">                 num_heads, dropout, bias=<span class="literal">False</span>, **kwargs</span>):</span><br><span class="line">        <span class="built_in">super</span>(MultiHeadAttention, self).__init__(**kwargs)</span><br><span class="line">        self.num_heads = num_heads</span><br><span class="line">        self.attention = d2l.DotProductAttention(dropout)</span><br><span class="line">        self.W_q = nn.Linear(query_size, num_hiddens, bias=bias)</span><br><span class="line">        self.W_k = nn.Linear(key_size, num_hiddens, bias=bias)</span><br><span class="line">        self.W_v = nn.Linear(value_size, num_hiddens, bias=bias)</span><br><span class="line">        self.W_o = nn.Linear(num_hiddens, num_hiddens, bias=bias)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, queries, keys, values, valid_lens</span>):</span><br><span class="line">        <span class="comment"># queries，keys，values的形状:</span></span><br><span class="line">        <span class="comment"># (batch_size，查询或者“键－值”对的个数，num_hiddens)</span></span><br><span class="line">        <span class="comment"># valid_lens　的形状:</span></span><br><span class="line">        <span class="comment"># (batch_size，)或(batch_size，查询的个数)</span></span><br><span class="line">        <span class="comment"># 经过变换后，输出的queries，keys，values　的形状:</span></span><br><span class="line">        <span class="comment"># (batch_size*num_heads，查询或者“键－值”对的个数，</span></span><br><span class="line">        <span class="comment"># num_hiddens/num_heads)</span></span><br><span class="line">        queries = transpose_qkv(self.W_q(queries), self.num_heads)</span><br><span class="line">        keys = transpose_qkv(self.W_k(keys), self.num_heads)</span><br><span class="line">        values = transpose_qkv(self.W_v(values), self.num_heads)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> valid_lens <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="comment"># 在轴0，将第一项（标量或者矢量）复制num_heads次，</span></span><br><span class="line">            <span class="comment"># 然后如此复制第二项，然后诸如此类。</span></span><br><span class="line">            valid_lens = torch.repeat_interleave(</span><br><span class="line">                valid_lens, repeats=self.num_heads, dim=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># output的形状:(batch_size*num_heads，查询的个数，</span></span><br><span class="line">        <span class="comment"># num_hiddens/num_heads)</span></span><br><span class="line">        output = self.attention(queries, keys, values, valid_lens)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># output_concat的形状:(batch_size，查询的个数，num_hiddens)</span></span><br><span class="line">        output_concat = transpose_output(output, self.num_heads)</span><br><span class="line">        <span class="keyword">return</span> self.W_o(output_concat)</span><br></pre></td></tr></table></figure><p>同时定义transpose_qkv,转置函数<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#@save</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">transpose_qkv</span>(<span class="params">X, num_heads</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;为了多注意力头的并行计算而变换形状&quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># 输入X的形状:(batch_size，查询或者“键－值”对的个数，num_hiddens)</span></span><br><span class="line">    <span class="comment"># 输出X的形状:(batch_size，查询或者“键－值”对的个数，num_heads，</span></span><br><span class="line">    <span class="comment"># num_hiddens/num_heads)</span></span><br><span class="line">    X = X.reshape(X.shape[<span class="number">0</span>], X.shape[<span class="number">1</span>], num_heads, -<span class="number">1</span>)</span><br><span class="line">    <span class="comment"># 输出X的形状:(batch_size，num_heads，查询或者“键－值”对的个数,</span></span><br><span class="line">    <span class="comment"># num_hiddens/num_heads)</span></span><br><span class="line">    X = X.permute(<span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">3</span>)</span><br><span class="line">    <span class="comment"># 最终输出的形状:(batch_size*num_heads,查询或者“键－值”对的个数,</span></span><br><span class="line">    <span class="comment"># num_hiddens/num_heads)</span></span><br><span class="line">    <span class="keyword">return</span> X.reshape(-<span class="number">1</span>, X.shape[<span class="number">2</span>], X.shape[<span class="number">3</span>])</span><br><span class="line"><span class="comment">#@save</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">transpose_output</span>(<span class="params">X, num_heads</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;逆转transpose_qkv函数的操作&quot;&quot;&quot;</span></span><br><span class="line">    X = X.reshape(-<span class="number">1</span>, num_heads, X.shape[<span class="number">1</span>], X.shape[<span class="number">2</span>])</span><br><span class="line">    X = X.permute(<span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">3</span>)</span><br><span class="line">    <span class="keyword">return</span> X.reshape(X.shape[<span class="number">0</span>], X.shape[<span class="number">1</span>], -<span class="number">1</span>)</span><br></pre></td></tr></table></figure></p><h3 id="练习-40"><a href="#练习-40" class="headerlink" title="练习"></a>练习</h3><ol><li><p>分别可视化这个实验中的多个头的注意力权重。</p></li><li><p>假设有一个完成训练的基于多头注意力的模型，现在希望修剪最不重要的注意力头以提高预测速度。如何设计实验来衡量注意力头的重要性呢？<br> 为了衡量注意力头的重要性并设计实验进行修剪，我们可以采用以下方法：<br> 首先，我们需要定义一个性能指标，如准确率、F1分数或模型的损失值，来评估模型在不同任务上的表现。<br> 逐个或逐步关闭（修剪）注意力头，并观察性能指标的变化。每次关闭一个或多个头后，重新训练模型，并记录性能变化。<br> 分析性能变化与修剪的头之间的关系。如果关闭某些头后性能下降不明显，这可能表明这些头不是很重要。相反，如果性能显著下降，则表明这些头对模型的预测至关重要。<br> 根据重要性分析的结果，迭代地调整模型结构，移除不重要的头，并重新训练模型。这个过程可能需要多次迭代，直到找到一个性能和效率之间的最佳平衡点。</p></li></ol><h2 id="自注意力与位置编码"><a href="#自注意力与位置编码" class="headerlink" title="自注意力与位置编码"></a>自注意力与位置编码</h2><p>想象一下，有了注意力机制之后，我们将词元序列输入注意力池化中， 以便同一组词元同时充当查询、键和值。 具体来说，每个查询都会关注所有的键－值对并生成一个注意力输出。 由于查询、键和值来自同一组输入，因此被称为 <em>自注意力</em>（self-attention）</p><h3 id="位置编码"><a href="#位置编码" class="headerlink" title="位置编码"></a>位置编码</h3><p>在处理词元序列时，循环神经网络是逐个的重复地处理词元的， 而自注意力则因为并行计算而放弃了顺序操作。 为了使用序列的顺序信息，通过在输入表示中添加 <em>位置编码</em>（positional encoding）来注入绝对的或相对的位置信息。 位置编码可以通过学习得到也可以直接固定得到。基于正弦函数和余弦函数的固定位置编码：<br>假设输入表示$X∈R^{n<em>d}$ 包含一个序列中n个词元的d维嵌入表示。 位置编码使用相同形状的位置嵌入矩阵 $P∈R^{n</em>d}$输出X+P， 矩阵第i行、第2j列和2j+1列上的元素为：</p><script type="math/tex; mode=display">p_{i,2j}=sin(\frac{i}{10000^{2j/d}})</script><script type="math/tex; mode=display">p_{i,2j+1}=cos(\frac{i}{10000^{2j/d}})</script><p>在位置嵌入矩阵P中， 行代表词元在序列中的位置，列代表位置编码的不同维度。 从下面的例子中可以看到位置嵌入矩阵的第6列和第7列的频率高于第8列和第9列。 第6列和第7列之间的偏移量（第8列和第9列相同）是由于正弦函数和余弦函数的交替。</p><h3 id="练习-41"><a href="#练习-41" class="headerlink" title="练习"></a>练习</h3><ol><li>假设设计一个深度架构，通过堆叠基于位置编码的自注意力层来表示序列。可能会存在什么问题？<br> 位置编码的固定性，难以捕捉序列中的长期依赖关系，计算复杂度增加，过拟合风险。</li><li>请设计一种可学习的位置编码方法。<br>  在可学习的位置编码中，位置编码不再是固定的，而是作为模型参数的一部分。这意味着位置编码可以通过训练过程进行优化。在上述代码示例中，<code>self.positional_embedding</code>是一个可学习的参数，其形状为<code>[max_position, num_features]</code>。这个矩阵存储了从位置0到<code>max_position</code>的每个位置的编码。<br> 位置编码的自适应性： 由于位置编码是可学习的，模型可以根据数据中的模式和序列的特点自适应地调整位置编码。这使得模型能够更好地处理各种长度的序列，并且能够捕捉到序列中位置信息的复杂性。<h2 id="Transformer"><a href="#Transformer" class="headerlink" title="Transformer"></a>Transformer</h2>![[Pasted image 20240408050453.png]]</li></ol><h3 id="基于位置的前馈网络"><a href="#基于位置的前馈网络" class="headerlink" title="基于位置的前馈网络"></a>基于位置的前馈网络</h3><p>基于位置的前馈网络对序列中的所有位置的表示进行变换时使用的是同一个多层感知机（MLP）。输入<code>X</code>的形状（批量大小，时间步数或序列长度，隐单元数或特征维度）将被一个两层的感知机转换成形状为（批量大小，时间步数，<code>ffn_num_outputs</code>）的输出张量。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#@save</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">PositionWiseFFN</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;基于位置的前馈网络&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, ffn_num_input, ffn_num_hiddens, ffn_num_outputs,</span></span><br><span class="line"><span class="params">                 **kwargs</span>):</span><br><span class="line">        <span class="built_in">super</span>(PositionWiseFFN, self).__init__(**kwargs)</span><br><span class="line">        self.dense1 = nn.Linear(ffn_num_input, ffn_num_hiddens)</span><br><span class="line">        self.relu = nn.ReLU()</span><br><span class="line">        self.dense2 = nn.Linear(ffn_num_hiddens, ffn_num_outputs)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, X</span>):</span><br><span class="line">        <span class="keyword">return</span> self.dense2(self.relu(self.dense1(X)))</span><br></pre></td></tr></table></figure><br>用同一个多层感知机对所有位置上的输入进行变换，当这些位置的输入相同时，输出也是相同的。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">ffn = PositionWiseFFN(<span class="number">4</span>, <span class="number">4</span>, <span class="number">8</span>)</span><br><span class="line">ffn.<span class="built_in">eval</span>()</span><br><span class="line">ffn(torch.ones((<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>)))[<span class="number">0</span>]</span><br></pre></td></tr></table></figure></p><h3 id="残差连接和层规范化"><a href="#残差连接和层规范化" class="headerlink" title="残差连接和层规范化"></a>残差连接和层规范化</h3><p>以下代码对比不同维度的层规范化和批量规范化的效果。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">ln = nn.LayerNorm(<span class="number">2</span>)</span><br><span class="line">bn = nn.BatchNorm1d(<span class="number">2</span>)</span><br><span class="line">X = torch.tensor([[<span class="number">1</span>, <span class="number">2</span>], [<span class="number">2</span>, <span class="number">3</span>]], dtype=torch.float32)</span><br><span class="line"><span class="comment"># 在训练模式下计算X的均值和方差</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;layer norm:&#x27;</span>, ln(X), <span class="string">&#x27;\nbatch norm:&#x27;</span>, bn(X))</span><br></pre></td></tr></table></figure><br>现在可以使用残差连接和层规范化来实现<code>AddNorm</code>类。暂退法也被作为正则化方法使用。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#@save</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">AddNorm</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;残差连接后进行层规范化&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, normalized_shape, dropout, **kwargs</span>):</span><br><span class="line">        <span class="built_in">super</span>(AddNorm, self).__init__(**kwargs)</span><br><span class="line">        self.dropout = nn.Dropout(dropout)</span><br><span class="line">        self.ln = nn.LayerNorm(normalized_shape)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, X, Y</span>):</span><br><span class="line">        <span class="keyword">return</span> self.ln(self.dropout(Y) + X)</span><br></pre></td></tr></table></figure><br>残差连接要求两个输入的形状相同，以便加法操作后输出张量的形状相同。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">add_norm = AddNorm([<span class="number">3</span>, <span class="number">4</span>], <span class="number">0.5</span>)</span><br><span class="line">add_norm.<span class="built_in">eval</span>()</span><br><span class="line">add_norm(torch.ones((<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>)), torch.ones((<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>))).shape</span><br></pre></td></tr></table></figure></p><h3 id="编码器-1"><a href="#编码器-1" class="headerlink" title="编码器"></a>编码器</h3><p>有了组成Transformer编码器的基础组件，现在可以先实现编码器中的一个层。下面的<code>EncoderBlock</code>类包含两个子层：多头自注意力和基于位置的前馈网络，这两个子层都使用了残差连接和紧随的层规范化。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#@save</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">EncoderBlock</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Transformer编码器块&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, key_size, query_size, value_size, num_hiddens,</span></span><br><span class="line"><span class="params">                 norm_shape, ffn_num_input, ffn_num_hiddens, num_heads,</span></span><br><span class="line"><span class="params">                 dropout, use_bias=<span class="literal">False</span>, **kwargs</span>):</span><br><span class="line">        <span class="built_in">super</span>(EncoderBlock, self).__init__(**kwargs)</span><br><span class="line">        self.attention = d2l.MultiHeadAttention(</span><br><span class="line">            key_size, query_size, value_size, num_hiddens, num_heads, dropout,</span><br><span class="line">            use_bias)</span><br><span class="line">        self.addnorm1 = AddNorm(norm_shape, dropout)</span><br><span class="line">        self.ffn = PositionWiseFFN(</span><br><span class="line">            ffn_num_input, ffn_num_hiddens, num_hiddens)</span><br><span class="line">        self.addnorm2 = AddNorm(norm_shape, dropout)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, X, valid_lens</span>):</span><br><span class="line">        Y = self.addnorm1(X, self.attention(X, X, X, valid_lens))</span><br><span class="line">        <span class="keyword">return</span> self.addnorm2(Y, self.ffn(Y))</span><br></pre></td></tr></table></figure><br>验证Transformer编码器中任何层都不会改变骑输入的形状：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">X = torch.ones((<span class="number">2</span>, <span class="number">100</span>, <span class="number">24</span>))</span><br><span class="line">valid_lens = torch.tensor([<span class="number">3</span>, <span class="number">2</span>])</span><br><span class="line">encoder_blk = EncoderBlock(<span class="number">24</span>, <span class="number">24</span>, <span class="number">24</span>, <span class="number">24</span>, [<span class="number">100</span>, <span class="number">24</span>], <span class="number">24</span>, <span class="number">48</span>, <span class="number">8</span>, <span class="number">0.5</span>)</span><br><span class="line">encoder_blk.<span class="built_in">eval</span>()</span><br><span class="line">encoder_blk(X, valid_lens).shape</span><br><span class="line"><span class="comment"># torch.Size([2, 100, 24])</span></span><br></pre></td></tr></table></figure><br>下面实现的Transformer编码器的代码中，堆叠了<code>num_layers</code>个<code>EncoderBlock</code>类的实例。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#@save</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">TransformerEncoder</span>(d2l.Encoder):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Transformer编码器&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, vocab_size, key_size, query_size, value_size,</span></span><br><span class="line"><span class="params">                 num_hiddens, norm_shape, ffn_num_input, ffn_num_hiddens,</span></span><br><span class="line"><span class="params">                 num_heads, num_layers, dropout, use_bias=<span class="literal">False</span>, **kwargs</span>):</span><br><span class="line">        <span class="built_in">super</span>(TransformerEncoder, self).__init__(**kwargs)</span><br><span class="line">        self.num_hiddens = num_hiddens</span><br><span class="line">        self.embedding = nn.Embedding(vocab_size, num_hiddens)</span><br><span class="line">        self.pos_encoding = d2l.PositionalEncoding(num_hiddens, dropout)</span><br><span class="line">        self.blks = nn.Sequential()</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(num_layers):</span><br><span class="line">            self.blks.add_module(<span class="string">&quot;block&quot;</span>+<span class="built_in">str</span>(i),</span><br><span class="line">                EncoderBlock(key_size, query_size, value_size, num_hiddens,</span><br><span class="line">                             norm_shape, ffn_num_input, ffn_num_hiddens,</span><br><span class="line">                             num_heads, dropout, use_bias))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, X, valid_lens, *args</span>):</span><br><span class="line">        <span class="comment"># 因为位置编码值在-1和1之间，</span></span><br><span class="line">        <span class="comment"># 因此嵌入值乘以嵌入维度的平方根进行缩放，</span></span><br><span class="line">        <span class="comment"># 然后再与位置编码相加。</span></span><br><span class="line">        X = self.pos_encoding(self.embedding(X) * math.sqrt(self.num_hiddens))</span><br><span class="line">        self.attention_weights = [<span class="literal">None</span>] * <span class="built_in">len</span>(self.blks)</span><br><span class="line">        <span class="keyword">for</span> i, blk <span class="keyword">in</span> <span class="built_in">enumerate</span>(self.blks):</span><br><span class="line">            X = blk(X, valid_lens)</span><br><span class="line">            self.attention_weights[</span><br><span class="line">                i] = blk.attention.attention.attention_weights</span><br><span class="line">        <span class="keyword">return</span> X</span><br></pre></td></tr></table></figure><br>下面我们指定了超参数来创建一个两层的Transformer编码器。 Transformer编码器输出的形状是（批量大小，时间步数目，<code>num_hiddens</code>）。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">encoder = TransformerEncoder(</span><br><span class="line">    <span class="number">200</span>, <span class="number">24</span>, <span class="number">24</span>, <span class="number">24</span>, <span class="number">24</span>, [<span class="number">100</span>, <span class="number">24</span>], <span class="number">24</span>, <span class="number">48</span>, <span class="number">8</span>, <span class="number">2</span>, <span class="number">0.5</span>)</span><br><span class="line">encoder.<span class="built_in">eval</span>()</span><br><span class="line">encoder(torch.ones((<span class="number">2</span>, <span class="number">100</span>), dtype=torch.long), valid_lens).shape</span><br><span class="line"><span class="comment"># torch.Size([2, 100, 24])</span></span><br></pre></td></tr></table></figure></p><h3 id="解码器"><a href="#解码器" class="headerlink" title="解码器"></a>解码器</h3><p>Transformer解码器也是由多个相同的层组成。在<code>DecoderBlock</code>类中实现的每个层包含了三个子层：解码器自注意力、“编码器-解码器”注意力和基于位置的前馈网络。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">DecoderBlock</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;解码器中第i个块&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, key_size, query_size, value_size, num_hiddens,</span></span><br><span class="line"><span class="params">                 norm_shape, ffn_num_input, ffn_num_hiddens, num_heads,</span></span><br><span class="line"><span class="params">                 dropout, i, **kwargs</span>):</span><br><span class="line">        <span class="built_in">super</span>(DecoderBlock, self).__init__(**kwargs)</span><br><span class="line">        self.i = i</span><br><span class="line">        self.attention1 = d2l.MultiHeadAttention(</span><br><span class="line">            key_size, query_size, value_size, num_hiddens, num_heads, dropout)</span><br><span class="line">        self.addnorm1 = AddNorm(norm_shape, dropout)</span><br><span class="line">        self.attention2 = d2l.MultiHeadAttention(</span><br><span class="line">            key_size, query_size, value_size, num_hiddens, num_heads, dropout)</span><br><span class="line">        self.addnorm2 = AddNorm(norm_shape, dropout)</span><br><span class="line">        self.ffn = PositionWiseFFN(ffn_num_input, ffn_num_hiddens,</span><br><span class="line">                                   num_hiddens)</span><br><span class="line">        self.addnorm3 = AddNorm(norm_shape, dropout)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, X, state</span>):</span><br><span class="line">        enc_outputs, enc_valid_lens = state[<span class="number">0</span>], state[<span class="number">1</span>]</span><br><span class="line">        <span class="comment"># 训练阶段，输出序列的所有词元都在同一时间处理，</span></span><br><span class="line">        <span class="comment"># 因此state[2][self.i]初始化为None。</span></span><br><span class="line">        <span class="comment"># 预测阶段，输出序列是通过词元一个接着一个解码的，</span></span><br><span class="line">        <span class="comment"># 因此state[2][self.i]包含着直到当前时间步第i个块解码的输出表示</span></span><br><span class="line">        <span class="keyword">if</span> state[<span class="number">2</span>][self.i] <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            key_values = X</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            key_values = torch.cat((state[<span class="number">2</span>][self.i], X), axis=<span class="number">1</span>)</span><br><span class="line">        state[<span class="number">2</span>][self.i] = key_values</span><br><span class="line">        <span class="keyword">if</span> self.training:</span><br><span class="line">            batch_size, num_steps, _ = X.shape</span><br><span class="line">            <span class="comment"># dec_valid_lens的开头:(batch_size,num_steps),</span></span><br><span class="line">            <span class="comment"># 其中每一行是[1,2,...,num_steps]</span></span><br><span class="line">            dec_valid_lens = torch.arange(</span><br><span class="line">                <span class="number">1</span>, num_steps + <span class="number">1</span>, device=X.device).repeat(batch_size, <span class="number">1</span>)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            dec_valid_lens = <span class="literal">None</span></span><br><span class="line">        <span class="comment"># 自注意力</span></span><br><span class="line">        X2 = self.attention1(X, key_values, key_values, dec_valid_lens)</span><br><span class="line">        Y = self.addnorm1(X, X2)</span><br><span class="line">        <span class="comment"># 编码器－解码器注意力。</span></span><br><span class="line">        <span class="comment"># enc_outputs的开头:(batch_size,num_steps,num_hiddens)</span></span><br><span class="line">        Y2 = self.attention2(Y, enc_outputs, enc_outputs, enc_valid_lens)</span><br><span class="line">        Z = self.addnorm2(Y, Y2)</span><br><span class="line">        <span class="keyword">return</span> self.addnorm3(Z, self.ffn(Z)), state</span><br></pre></td></tr></table></figure><br>为了便于在“编码器－解码器”注意力中进行缩放点积计算和残差连接中进行加法计算，编码器和解码器的特征维度都是<code>num_hiddens</code>。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">decoder_blk = DecoderBlock(<span class="number">24</span>, <span class="number">24</span>, <span class="number">24</span>, <span class="number">24</span>, [<span class="number">100</span>, <span class="number">24</span>], <span class="number">24</span>, <span class="number">48</span>, <span class="number">8</span>, <span class="number">0.5</span>, <span class="number">0</span>)</span><br><span class="line">decoder_blk.<span class="built_in">eval</span>()</span><br><span class="line">X = torch.ones((<span class="number">2</span>, <span class="number">100</span>, <span class="number">24</span>))</span><br><span class="line">state = [encoder_blk(X, valid_lens), valid_lens, [<span class="literal">None</span>]]</span><br><span class="line">decoder_blk(X, state)[<span class="number">0</span>].shape</span><br><span class="line"><span class="comment"># torch.Size([2, 100, 24])</span></span><br></pre></td></tr></table></figure><br>将多个block拼接成decoder<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">TransformerDecoder</span>(d2l.AttentionDecoder):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, vocab_size, key_size, query_size, value_size,</span></span><br><span class="line"><span class="params">                 num_hiddens, norm_shape, ffn_num_input, ffn_num_hiddens,</span></span><br><span class="line"><span class="params">                 num_heads, num_layers, dropout, **kwargs</span>):</span><br><span class="line">        <span class="built_in">super</span>(TransformerDecoder, self).__init__(**kwargs)</span><br><span class="line">        self.num_hiddens = num_hiddens</span><br><span class="line">        self.num_layers = num_layers</span><br><span class="line">        self.embedding = nn.Embedding(vocab_size, num_hiddens)</span><br><span class="line">        self.pos_encoding = d2l.PositionalEncoding(num_hiddens, dropout)</span><br><span class="line">        self.blks = nn.Sequential()</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(num_layers):</span><br><span class="line">            self.blks.add_module(<span class="string">&quot;block&quot;</span>+<span class="built_in">str</span>(i),</span><br><span class="line">                DecoderBlock(key_size, query_size, value_size, num_hiddens,</span><br><span class="line">                             norm_shape, ffn_num_input, ffn_num_hiddens,</span><br><span class="line">                             num_heads, dropout, i))</span><br><span class="line">        self.dense = nn.Linear(num_hiddens, vocab_size)</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">init_state</span>(<span class="params">self, enc_outputs, enc_valid_lens, *args</span>):</span><br><span class="line">        <span class="keyword">return</span> [enc_outputs, enc_valid_lens, [<span class="literal">None</span>] * self.num_layers]</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, X, state</span>):</span><br><span class="line">        X = self.pos_encoding(self.embedding(X) * math.sqrt(self.num_hiddens))</span><br><span class="line">        self._attention_weights = [[<span class="literal">None</span>] * <span class="built_in">len</span>(self.blks) <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span> (<span class="number">2</span>)]</span><br><span class="line">        <span class="keyword">for</span> i, blk <span class="keyword">in</span> <span class="built_in">enumerate</span>(self.blks):</span><br><span class="line">            X, state = blk(X, state)</span><br><span class="line">            <span class="comment"># 解码器自注意力权重</span></span><br><span class="line">            self._attention_weights[<span class="number">0</span>][</span><br><span class="line">                i] = blk.attention1.attention.attention_weights</span><br><span class="line">            <span class="comment"># “编码器－解码器”自注意力权重</span></span><br><span class="line">            self._attention_weights[<span class="number">1</span>][</span><br><span class="line">                i] = blk.attention2.attention.attention_weights</span><br><span class="line">        <span class="keyword">return</span> self.dense(X), state</span><br><span class="line"><span class="meta">    @property</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">attention_weights</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> self._attention_weights</span><br></pre></td></tr></table></figure></p><h3 id="练习-42"><a href="#练习-42" class="headerlink" title="练习"></a>练习</h3><ol><li>在实验中训练更深的Transformer将如何影响训练速度和翻译效果？<br> 可能会增加训练时间和计算资源的需求。随着层数的增加，模型的参数数量也会增加，这可能导致梯度下降过程中的优化变得更加复杂和困难。同时，更多的层意味着更多的注意力计算，这会增加内存消耗和计算时间。</li><li>在Transformer中使用加性注意力取代缩放点积注意力是不是个好办法？为什么？<br> 加性注意力通过一个加性而非点积的方式来计算注意力分数。这种方法可能有助于模型捕捉不同类型的依赖关系，因为它提供了一种不同的机制来组合查询和键的信息。</li><li>对于语言模型，应该使用Transformer的编码器还是解码器，或者两者都用？如何设计？<br> 对于语言模型，通常使用Transformer的解码器部分，因为解码器能够处理序列数据并生成下一个词元的预测。然而，在某些情况下，也可以结合编码器和解码器，例如在机器翻译任务中。编码器可以处理源语言文本，而解码器则生成目标语言的翻译。</li><li>如果输入序列很长，Transformer会面临什么挑战？为什么？<br> 首先，长序列会增加自注意力层的计算复杂度，因为每个时间步都需要与序列中的所有其他时间步进行交互。这会导致显著的计算和内存开销。其次，长序列可能导致梯度消失或爆炸问题，影响模型的训练稳定性和效率。</li><li>如何提高Transformer的计算速度和内存使用效率？提示：可以参考论文 (<a href="https://zh-v2.d2l.ai/chapter_references/zreferences.html#id166" title="Tay, Y., Dehghani, M., Bahri, D., &amp; Metzler, D. (2020). Efficient transformers: a survey. arXiv preprint arXiv:2009.06732.">Tay <em>et al.</em>, 2020</a>)。<br> 可以使用量化来减少模型参数的位宽，从而减少计算和存储需求。还可以使用模型剪枝技术去除不重要的权重，简化模型结构。或使用局部注意力或稀疏注意力。</li><li>如果不使用卷积神经网络，如何设计基于Transformer模型的图像分类任务？提示：可以参考Vision Transformer<br> 可以设计Vision Transformer（ViT）来进行图像分类任务。ViT将图像分割成一系列的小块（patches），然后将这些小块线性嵌入到一个序列中，就像处理文本序列一样。然后，使用标准的Transformer架构来处理这个序列，包括自注意力层和前馈网络层。ViT能够捕捉图像中的全局依赖关系，并且在多个图像分类任务上取得了良好的性能。设计时，可以调整块的大小、序列的长度以及Transformer层的配置来适应不同的图像和任务需求。</li></ol><h1 id="优化算法"><a href="#优化算法" class="headerlink" title="优化算法"></a>优化算法</h1><h2 id="优化和深度学习"><a href="#优化和深度学习" class="headerlink" title="优化和深度学习"></a>优化和深度学习</h2><h3 id="优化的目标"><a href="#优化的目标" class="headerlink" title="优化的目标"></a>优化的目标</h3><p>主要关注最小化目标，由于优化算法发目标函数通常是基于训练数据集发损失函数，因此优化发目标时减少训练误差。但深度学习的目标是减少泛化误差。为了实现后者，除了使用优化算法来减少训练误差之外，还要注意过拟合。</p><h3 id="深度学习中的优化挑战"><a href="#深度学习中的优化挑战" class="headerlink" title="深度学习中的优化挑战"></a>深度学习中的优化挑战</h3><p>对于任何目标函数f(x)，如果在x处对应的f(x)值小于在x附近任意其他点的f(x)值，那么f(x)可能是局部最小值。如果f(x)在x处的值是整个域中目标函数的最小值，那么f(x)是全局最小值。</p><h3 id="鞍点"><a href="#鞍点" class="headerlink" title="鞍点"></a>鞍点</h3><p>除了局部最小值之外，鞍点是梯度消失的另一个原因。<em>鞍点</em>（saddle point）是指函数的所有梯度都消失但既不是全局最小值也不是局部最小值的任何位置。考虑这个函数$f(x)=x^3$。它的一阶和二阶导数在x=0时消失。这时优化可能会停止，尽管它不是最小值。</p><h3 id="练习-43"><a href="#练习-43" class="headerlink" title="练习"></a>练习</h3><ol><li>考虑一个简单的MLP，它有一个隐藏层，比如，隐藏层中维度为d和一个输出。证明对于任何局部最小值，至少有d’个等效方案。<br> 我们至少可以构造 d 个等效方案，每个方案对应于隐藏层权重的一种调整。但由于输出层的权重和偏置也可以调整，实际上存在无限多个等效方案。这里的 d′ 可以理解为 d 个隐藏单元提供的基础对称性，而实际的等效方案数量是无限的。</li><li>你能想到深度学习优化还涉及哪些其他挑战？<br> 非凸优化问题，训练与验证的偏差，计算资源限制，超参数调整</li><li>假设你想在（真实的）鞍上平衡一个（真实的）球。<br> 为什么这很难？<br> <strong>不稳定的平衡点</strong>：鞍点是一个既不是最高点也不是最低点的位置，球在这一点上容易受到扰动而滚动到其他位置，类似于优化过程中的鞍点，模型参数的微小变化可能导致性能显著下降。<br>  <strong>难以识别</strong>：鞍点可能在参数空间中不明显，难以被识别和区分，特别是在高维空间中。<br> <strong>局部最小值的干扰</strong>：鞍点附近可能存在局部最小值，优化算法可能会被这些局部最小值吸引，从而陷入鞍点并难以逃脱。<br> 能利用这种效应来优化算法吗？<br> <strong>使用动量</strong>：动量可以帮助模型在参数空间中保持速度，避免在鞍点附近停滞不前。<br> <strong>学习率调整策略</strong>：通过调整学习率，可以在鞍点附近进行更细致的搜索，有助于模型越过鞍点继续向全局最小值前进。<br> <strong>正则化技术</strong>：正则化可以帮助模型学习更平滑的权重配置，减少在鞍点附近震荡的可能性。<h2 id="凸性"><a href="#凸性" class="headerlink" title="凸性"></a>凸性</h2><h3 id="凸集"><a href="#凸集" class="headerlink" title="凸集"></a>凸集</h3>对于任何$a,b\in X$连接a和b的线段也位于X中，则向量空间中的集合X是凸的。<h3 id="凸函数"><a href="#凸函数" class="headerlink" title="凸函数"></a>凸函数</h3>给定一个凸集X，对于所有x，x‘属于X，和所有$\lambda \in [0,1]$，函数f是凸的，我们可以得到：<script type="math/tex; mode=display">\lambda f(x)+(1-\lambda )f(x')>=f(\lambda x+(1-\lambda)x')</script>余弦函数是非凸的，抛物线和指数函数是凸的。<h3 id="詹森不等式"><a href="#詹森不等式" class="headerlink" title="詹森不等式"></a>詹森不等式</h3><script type="math/tex; mode=display">\sum_{i}^{}a_if(x_i)>=f(\sum_{i}^{}a_ix_i)and E_X[f(X)]>=f(E_X[X])</script>凸函数的期望不小于期望的凸函数，其中后者通常是一个更简单的表达式。<br>应用：用一个较简单的表达式约束一个较复杂的表达式。<h3 id="性质"><a href="#性质" class="headerlink" title="性质"></a>性质</h3>局部极小值是全局极小值<br>凸函数的下水平集是凸的<h3 id="拉格朗日函数"><a href="#拉格朗日函数" class="headerlink" title="拉格朗日函数"></a>拉格朗日函数</h3>通常，求解一个有约束的优化问题是困难的，解决这个问题的一种方法来自物理中相当简单的直觉。 想象一个球在一个盒子里，球会滚到最低的地方，重力将与盒子两侧对球施加的力平衡。 简而言之，目标函数（即重力）的梯度将被约束函数的梯度所抵消（由于墙壁的“推回”作用，需要保持在盒子内）。 请注意，任何不起作用的约束（即球不接触壁）都将无法对球施加任何力。<br>这里我们简略拉格朗日函数L的推导，上述推理可以通过以下鞍点优化问题来表示：<script type="math/tex; mode=display">L(x,a1,a2,...,an)=f(x)+\sum_{i=1}^{n}aici(x) where ai>=0</script><h3 id="练习-44"><a href="#练习-44" class="headerlink" title="练习"></a>练习</h3></li><li>假设我们想要通过绘制集合内点之间的所有直线并检查这些直线是否包含来验证集合的凸性。i.证明只检查边界上的点是充分的。ii.证明只检查集合的顶点是充分的。<br> i. 证明只检查边界上的点是充分的：<br> 集合的凸性意味着对于集合中的任意两点，这两点之间的线段都完全位于集合内。要验证一个集合是否为凸集，我们可以从集合内任选两点，并检查这两点之间的线段是否完全在集合内。边界上的点是集合中最接近“边缘”的点，如果集合是凸的，那么边界上的任意两点之间的线段都将完全在集合内，因为任何偏离这条线段的点都将在集合外。因此，如果边界上任意两点之间的线段都在集合内，那么集合内任意两点之间的线段也必然在集合内，因为它们是由边界上的点“夹”在中间的。所以，只检查边界上的点足以验证集合的凸性。<br> ii. 证明只检查集合的顶点是充分的：<br> 顶点是集合边界上的特定点，它们是集合边界的“角”或最尖锐的部分。在凸集的定义中，如果集合是凸的，那么通过集合中任意两点的线段都将完全位于集合内。顶点是确定集合形状的关键点，因为它们定义了集合边界的转向。<br> 如果只检查顶点，我们可以观察顶点之间的连接线段是否完全在集合内。如果所有顶点之间的线段都在集合内，那么可以推断出集合是凸的，因为这些线段覆盖了集合的所有边界。此外，任何不在这些顶点线段上的点都位于这些线段之间，因此也必然在集合内。因此，只检查顶点足以验证集合的凸性。</li></ol><h2 id="梯度下降"><a href="#梯度下降" class="headerlink" title="梯度下降"></a>梯度下降</h2><h2 id="随机梯度下降"><a href="#随机梯度下降" class="headerlink" title="随机梯度下降"></a>随机梯度下降</h2><h2 id="小批量随机梯度下降"><a href="#小批量随机梯度下降" class="headerlink" title="小批量随机梯度下降"></a>小批量随机梯度下降</h2><h2 id="动量法"><a href="#动量法" class="headerlink" title="动量法"></a>动量法</h2><p>本节将探讨更有效的优化算法，尤其是针对实验中常见的某些类型的优化问题。<br>它旨在帮助加速梯度下降算法在相关方向上的收敛，并抑制在不相关方向上的震荡。动量法通过累积过去梯度的指数衰减平均来实现这一目标。</p><h3 id="从零开始实现-1"><a href="#从零开始实现-1" class="headerlink" title="从零开始实现"></a>从零开始实现</h3><p>相比于小批量随机梯度下降，动量方法需要维护一组辅助变量，即速度。 它与梯度以及优化问题的变量具有相同的形状。 在下面的实现中，我们称这些变量为<code>states</code>。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">init_momentum_states</span>(<span class="params">feature_dim</span>):</span><br><span class="line">    v_w = torch.zeros((feature_dim, <span class="number">1</span>))</span><br><span class="line">    v_b = torch.zeros(<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">return</span> (v_w, v_b)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">sgd_momentum</span>(<span class="params">params, states, hyperparams</span>):</span><br><span class="line">    <span class="keyword">for</span> p, v <span class="keyword">in</span> <span class="built_in">zip</span>(params, states):</span><br><span class="line">        <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">            v[:] = hyperparams[<span class="string">&#x27;momentum&#x27;</span>] * v + p.grad</span><br><span class="line">            p[:] -= hyperparams[<span class="string">&#x27;lr&#x27;</span>] * v</span><br><span class="line">        p.grad.data.zero_()</span><br></pre></td></tr></table></figure><br>在实验中运作：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">train_momentum</span>(<span class="params">lr, momentum, num_epochs=<span class="number">2</span></span>):</span><br><span class="line">    d2l.train_ch11(sgd_momentum, init_momentum_states(feature_dim),</span><br><span class="line">                   &#123;<span class="string">&#x27;lr&#x27;</span>: lr, <span class="string">&#x27;momentum&#x27;</span>: momentum&#125;, data_iter,</span><br><span class="line">                   feature_dim, num_epochs)</span><br><span class="line"></span><br><span class="line">data_iter, feature_dim = d2l.get_data_ch11(batch_size=<span class="number">10</span>)</span><br><span class="line">train_momentum(<span class="number">0.02</span>, <span class="number">0.5</span>)</span><br></pre></td></tr></table></figure><br>![[Pasted image 20240408230734.png]]</p><h3 id="简介实现-1"><a href="#简介实现-1" class="headerlink" title="简介实现"></a>简介实现</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">trainer = torch.optim.SGD</span><br><span class="line">d2l.train_concise_ch11(trainer, &#123;<span class="string">&#x27;lr&#x27;</span>: <span class="number">0.005</span>, <span class="string">&#x27;momentum&#x27;</span>: <span class="number">0.9</span>&#125;, data_iter)</span><br></pre></td></tr></table></figure><h3 id="练习-45"><a href="#练习-45" class="headerlink" title="练习"></a>练习</h3><ol><li>当我们执行带动量法的随机梯度下降时会有什么变化？当我们使用带动量法的小批量随机梯度下降时会发生什么？试验参数如何？<br> 带动量法的随机梯度下降（SGD）和小批量随机梯度下降（Mini-batch SGD）是两种常见的优化算法，它们在标准SGD的基础上引入了动量项，以加速训练过程并提高收敛性。下面是这两种方法的特点和变化：</li></ol><ul><li><strong>带动量法的随机梯度下降（SGD with Momentum）</strong>：<ul><li>在每次迭代中，动量法不仅考虑当前梯度，还考虑之前梯度的加权平均，这有助于平滑梯度更新路径。</li><li>动量项 vt​ 根据当前梯度和之前动量的加权平均进行更新，然后用于更新参数。</li><li>这种方法可以减少SGD在优化过程中的震荡，特别是在面对噪声或非平稳目标函数时。</li><li>动量法的SGD通常需要调整额外的超参数，即动量系数 γ，以及学习率 α。</li></ul></li><li><strong>带动量法的小批量随机梯度下降（Momentum SGD with Mini-batches）</strong>：<ul><li>在这种方法中，每次迭代使用一个小批量数据来计算梯度，然后结合动量项来更新参数。</li><li>小批量方法可以提供更稳定的梯度估计，并且可以更有效地利用并行计算资源，如GPU。</li><li>动量项同样有助于平滑梯度更新，减少震荡，并加速训练过程。</li><li>使用小批量数据时，动量项的更新可能会受到批量大小的影响，因此可能需要根据批量大小调整动量系数 γ 和学习率 α。<h2 id="AdaGrad算法"><a href="#AdaGrad算法" class="headerlink" title="AdaGrad算法"></a>AdaGrad算法</h2>AdaGrad的核心思想是根据每个参数的历史梯度信息来调整其学习率，使得模型在训练过程中能够针对不同的参数采取不同的更新策略。<br>AdaGrad算法的主要特点如下：</li></ul></li></ul><ol><li><strong>参数特定的学习率</strong>：AdaGrad会为模型中的每个参数分配一个单独的学习率，这样不同的参数可以以不同的速度进行更新。</li><li><strong>累积梯度平方和</strong>：对于每个参数，AdaGrad会累积其梯度的平方和，这有助于捕捉到每个参数的更新频率和幅度。</li><li><strong>自适应调整</strong>：随着训练的进行，每个参数的学习率会根据其累积的梯度信息自动调整。如果一个参数的梯度在训练过程中变化很大，其学习率会降低；反之，如果梯度变化较小，学习率会增加。</li><li><strong>适用性</strong>：AdaGrad特别适合处理稀疏数据集，因为它可以针对数据中出现频率不同的特征采取不同的更新策略。<h3 id="简介实现-2"><a href="#简介实现-2" class="headerlink" title="简介实现"></a>简介实现</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">trainer = torch.optim.Adagrad</span><br><span class="line">d2l.train_concise_ch11(trainer, &#123;<span class="string">&#x27;lr&#x27;</span>: <span class="number">0.1</span>&#125;, data_iter)</span><br></pre></td></tr></table></figure><h3 id="练习-46"><a href="#练习-46" class="headerlink" title="练习"></a>练习</h3></li><li>要如何修改AdaGrad算法，才能使其在学习率方面的衰减不那么激进？<br> 引入衰减因子，使用学习率预热，动态调整学习率<h2 id="RMSProp算法"><a href="#RMSProp算法" class="headerlink" title="RMSProp算法"></a>RMSProp算法</h2>![[Pasted image 20240408231934.png]]<h3 id="简介实现-3"><a href="#简介实现-3" class="headerlink" title="简介实现"></a>简介实现</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">trainer = torch.optim.RMSprop</span><br><span class="line">d2l.train_concise_ch11(trainer, &#123;<span class="string">&#x27;lr&#x27;</span>: <span class="number">0.01</span>, <span class="string">&#x27;alpha&#x27;</span>: <span class="number">0.9</span>&#125;,</span><br><span class="line">                       data_iter)</span><br></pre></td></tr></table></figure><h2 id="Adadelta"><a href="#Adadelta" class="headerlink" title="Adadelta"></a>Adadelta</h2>Adadelta是AdaGrad的另一种变体， 主要区别在于前者减少了学习率适应坐标的数量。 此外，广义上Adadelta被称为没有学习率，因为它使用变化量本身作为未来变化的校准。 <h3 id="简介实现-4"><a href="#简介实现-4" class="headerlink" title="简介实现"></a>简介实现</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">trainer = torch.optim.Adadelta</span><br><span class="line">d2l.train_concise_ch11(trainer, &#123;<span class="string">&#x27;rho&#x27;</span>: <span class="number">0.9</span>&#125;, data_iter)</span><br></pre></td></tr></table></figure><h3 id="练习-47"><a href="#练习-47" class="headerlink" title="练习"></a>练习</h3></li><li>将Adadelta的收敛行为与AdaGrad和RMSProp进行比较。<br> Adadelta算法是对AdaGrad算法的改进，旨在解决AdaGrad学习率单调递减可能导致过早收敛的问题。<br> AdaGrad通过累积梯度平方来自适应学习率，但可能导致学习率过早下降；RMSProp通过指数加权移动平均来调整学习率，改善了AdaGrad的这一缺点；而Adadelta则进一步改进了自适应学习率机制，通过两个指数加权移动平均来同时控制学习和更新的幅度，使得算法在训练过程中更加稳定和有效。<h2 id="Adam算法"><a href="#Adam算法" class="headerlink" title="Adam算法"></a>Adam算法</h2>Adam（Adaptive Moment Estimation）算法是一种用于优化神经网络的随机梯度下降算法的变种，它结合了动量法（momentum）和自适应学习率的思想。<br>Adam算法的核心思想是根据梯度的一阶矩估计（mean）和二阶矩估计（uncentered variance）来动态调整每个参数的学习率。具体而言，Adam算法会维护两个指数加权移动平均变量，分别表示梯度的一阶矩估计和二阶矩估计。这两个变量分别用来校正梯度的偏差和动态调整学习率。<br>算法步骤如下：</li><li>初始化参数θ，一阶矩估计变量m和二阶矩估计变量v。</li><li>在每个时间步t：<ul><li>计算当前时间步的梯度gt。</li><li>更新一阶矩估计变量m和二阶矩估计变量v：<figure class="highlight scss"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">mt = β<span class="number">1</span> * mt-<span class="number">1</span> + (<span class="number">1</span> - β<span class="number">1</span>) * gt</span><br><span class="line">vt = β<span class="number">2</span> * vt-<span class="number">1</span> + (<span class="number">1</span> - β<span class="number">2</span>) * gt^<span class="number">2</span></span><br></pre></td></tr></table></figure></li></ul></li></ol><ul><li>其中，β1和β2分别是控制一阶矩估计和二阶矩估计的指数衰减率。</li><li>根据一阶矩估计的偏差修正mt和vt：<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">m̂t = mt / (<span class="number">1</span> - β<span class="number">1</span>^t)</span><br><span class="line">v̂t = vt / (<span class="number">1</span> - β<span class="number">2</span>^t)</span><br></pre></td></tr></table></figure>更新参数θ：<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">θt+<span class="number">1</span> = θt - α * m̂t / (sqrt(v̂t) + ε)</span><br></pre></td></tr></table></figure><h3 id="练习-48"><a href="#练习-48" class="headerlink" title="练习"></a>练习</h3></li></ul><ol><li>试着重写动量和二次矩更新，从而使其不需要偏差校正。</li><li>收敛时为什么需要降低学习率$\eta$？<br> 当模型接近收敛时，学习率的降低有助于使模型在最优解附近更加稳定地收敛。在开始阶段，较大的学习率可以帮助模型更快地接近最优解，但随着训练的进行，学习率的减小可以使模型在最优解周围更精细地调整参数，避免在最优解附近波动。因此，逐渐降低学习率可以提高模型的收敛速度和稳定性。<h2 id="学习率调度器"><a href="#学习率调度器" class="headerlink" title="学习率调度器"></a>学习率调度器</h2></li></ol><ul><li>首先，学习率的大小很重要。如果它太大，优化就会发散；如果它太小，训练就会需要过长时间，或者我们最终只能得到次优的结果。直观地说，这是最不敏感与最敏感方向的变化量的比率。</li><li>其次，衰减速率同样很重要。如果学习率持续过高，我们可能最终会在最小值附近弹跳，从而无法达到最优解。</li><li>在训练期间逐步降低学习率可以提高准确性，并且减少模型的过拟合。</li><li>在实验中，每当进展趋于稳定时就降低学习率，这是很有效的。从本质上说，这可以确保我们有效地收敛到一个适当的解，也只有这样才能通过降低学习率来减小参数的固有方差。</li><li>余弦调度器在某些计算机视觉问题中很受欢迎。</li><li>优化之前的预热期可以防止发散。</li><li>优化在深度学习中有多种用途。对于同样的训练误差而言，选择不同的优化算法和学习率调度，除了最大限度地减少训练时间，可以导致测试集上不同的泛化和过拟合量。<h3 id="练习-49"><a href="#练习-49" class="headerlink" title="练习"></a>练习</h3></li></ul><ol><li>如果改变学习率下降的指数，收敛性会如何改变？在实验中方便起见，使用<code>PolyScheduler</code>。<br> 如果改变学习率下降的指数，收敛性会受到影响。较大的指数会导致学习率下降得更快，可能会导致模型在训练过程中跳过最优解附近的区域，从而影响最终的收敛性。较小的指数会导致学习率下降得更慢，可能需要更多的训练时间才能达到最优解。因此，选择合适的学习率下降指数是重要的，通常需要根据具体问题和模型来调整。</li><li>将余弦调度器应用于大型计算机视觉问题，例如训练ImageNet数据集。与其他调度器相比，它如何影响性能？<br> 将余弦调度器应用于大型计算机视觉问题（如训练ImageNet数据集）可以带来一些好处。余弦调度器在训练初期使用较大的学习率，有助于快速收敛到一个比较好的解，然后在训练后期逐渐降低学习率，以更精细地调整参数并提高模型的泛化能力。与其他调度器相比，余弦调度器可以在一定程度上提高模型的性能和泛化能力。</li><li>预热应该持续多长时间？<br> 预热的持续时间应该根据具体情况来确定。预热的目的是在训练开始时使用较大的学习率，有助于快速收敛到一个比较好的解。预热时间不宜过长，通常在几个epoch内即可。预热时间过长可能会导致模型在训练初期过度调整参数，影响最终的收敛性能。</li><li>可以试着把优化和采样联系起来吗？首先，在随机梯度朗之万动力学上使用的结果。<br> 优化和采样可以通过随机梯度朗之万动力学（SGRLD）来联系。SGRLD是一种融合了随机梯度下降和朗之万动力学的优化算法，它将梯度下降的更新规则与朗之万动力学的随机性相结合，可以更好地处理带噪声的优化问题。在SGRLD中，采样被用来引入随机性，以避免陷入局部最优解，并且可以更好地探索参数空间。<h1 id="计算性能"><a href="#计算性能" class="headerlink" title="计算性能"></a>计算性能</h1>python是一种解释性语言，按顺序执行函数体的操作。通过对e = add(a,b)求值，将结果存储为变量和e。<br>尽管命令式编程很方便，但可能效率不高，一方面因为python会单独执行这三个函数的调用，而没有考虑add函数在fancy_func中倍重复调用。如果在一个GPU上执行这些命令，那么python解释器产生的凯西奥可能会非常大。此外，它需要保存e和f的值，指导函数中所有语句都执行完毕，这是因为程序不知道在执行语句e = add（a,b)和f = add(c,d)之后，其他部分是否会使用变量e和f。<h3 id="符号式编程"><a href="#符号式编程" class="headerlink" title="符号式编程"></a>符号式编程</h3>代码只有在完全定义了过程之后才执行计算：</li><li>定义计算流程</li><li>将流程编译成可执行的程序</li><li>给定输入，调用编译好的程序执行</li></ol><ul><li>命令式编程使得新模型的设计变得容易，因为可以依据控制流编写代码，并拥有相对成熟的Python软件生态。</li><li>符号式编程要求我们先定义并且编译程序，然后再执行程序，其好处是提高了计算性能。<h2 id="异步计算"><a href="#异步计算" class="headerlink" title="异步计算"></a>异步计算</h2></li><li><p>深度学习框架可以将Python前端的控制与后端的执行解耦，使得命令可以快速地异步插入后端、并行执行。</p></li><li><p>异步产生了一个相当灵活的前端，但请注意：过度填充任务队列可能会导致内存消耗过多。建议对每个小批量进行同步，以保持前端和后端大致同步。</p></li><li><p>芯片供应商提供了复杂的性能分析工具，以获得对深度学习效率更精确的洞察。</p><h3 id="自动并行"><a href="#自动并行" class="headerlink" title="自动并行"></a>自动并行</h3><h3 id="基于GPU的并行计算"><a href="#基于GPU的并行计算" class="headerlink" title="基于GPU的并行计算"></a>基于GPU的并行计算</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">devices = d2l.try_all_gpus()</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">run</span>(<span class="params">x</span>):</span><br><span class="line">    <span class="keyword">return</span> [x.mm(x) <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">50</span>)]</span><br><span class="line"></span><br><span class="line">x_gpu1 = torch.rand(size=(<span class="number">4000</span>, <span class="number">4000</span>), device=devices[<span class="number">0</span>])</span><br><span class="line">x_gpu2 = torch.rand(size=(<span class="number">4000</span>, <span class="number">4000</span>), device=devices[<span class="number">1</span>])</span><br></pre></td></tr></table></figure><h3 id="练习-50"><a href="#练习-50" class="headerlink" title="练习"></a>练习</h3></li></ul><ol><li>设计一个实验，在CPU和GPU这两种设备上使用并行计算和通信。<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建随机矩阵</span></span><br><span class="line">size = <span class="number">1000</span></span><br><span class="line">A = np.random.rand(size, size)</span><br><span class="line">B = np.random.rand(size, size)</span><br><span class="line"></span><br><span class="line"><span class="comment"># CPU上的串行矩阵乘法</span></span><br><span class="line">start_time = time.time()</span><br><span class="line">C_cpu_serial = np.dot(A, B)</span><br><span class="line">cpu_serial_time = time.time() - start_time</span><br><span class="line"></span><br><span class="line"><span class="comment"># GPU上的并行矩阵乘法</span></span><br><span class="line">A_gpu = torch.tensor(A, dtype=torch.float32).cuda()</span><br><span class="line">B_gpu = torch.tensor(B, dtype=torch.float32).cuda()</span><br><span class="line">start_time = time.time()</span><br><span class="line">C_gpu_parallel = torch.mm(A_gpu, B_gpu)</span><br><span class="line">gpu_parallel_time = time.time() - start_time</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将结果从GPU复制回CPU</span></span><br><span class="line">C_gpu_parallel = C_gpu_parallel.cpu().numpy()</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;CPU串行计算时间：<span class="subst">&#123;cpu_serial_time&#125;</span>秒&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;GPU并行计算时间：<span class="subst">&#123;gpu_parallel_time&#125;</span>秒&quot;</span>)</span><br></pre></td></tr></table></figure><h2 id="多GPU训练"><a href="#多GPU训练" class="headerlink" title="多GPU训练"></a>多GPU训练</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br><span class="line"><span class="comment">#@save</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">resnet18</span>(<span class="params">num_classes, in_channels=<span class="number">1</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;稍加修改的ResNet-18模型&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">resnet_block</span>(<span class="params">in_channels, out_channels, num_residuals,</span></span><br><span class="line"><span class="params">                     first_block=<span class="literal">False</span></span>):</span><br><span class="line">        blk = []</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(num_residuals):</span><br><span class="line">            <span class="keyword">if</span> i == <span class="number">0</span> <span class="keyword">and</span> <span class="keyword">not</span> first_block:</span><br><span class="line">                blk.append(d2l.Residual(in_channels, out_channels,</span><br><span class="line">                                        use_1x1conv=<span class="literal">True</span>, strides=<span class="number">2</span>))</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                blk.append(d2l.Residual(out_channels, out_channels))</span><br><span class="line">        <span class="keyword">return</span> nn.Sequential(*blk)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 该模型使用了更小的卷积核、步长和填充，而且删除了最大汇聚层</span></span><br><span class="line">    net = nn.Sequential(</span><br><span class="line">        nn.Conv2d(in_channels, <span class="number">64</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>),</span><br><span class="line">        nn.BatchNorm2d(<span class="number">64</span>),</span><br><span class="line">        nn.ReLU())</span><br><span class="line">    net.add_module(<span class="string">&quot;resnet_block1&quot;</span>, resnet_block(</span><br><span class="line">        <span class="number">64</span>, <span class="number">64</span>, <span class="number">2</span>, first_block=<span class="literal">True</span>))</span><br><span class="line">    net.add_module(<span class="string">&quot;resnet_block2&quot;</span>, resnet_block(<span class="number">64</span>, <span class="number">128</span>, <span class="number">2</span>))</span><br><span class="line">    net.add_module(<span class="string">&quot;resnet_block3&quot;</span>, resnet_block(<span class="number">128</span>, <span class="number">256</span>, <span class="number">2</span>))</span><br><span class="line">    net.add_module(<span class="string">&quot;resnet_block4&quot;</span>, resnet_block(<span class="number">256</span>, <span class="number">512</span>, <span class="number">2</span>))</span><br><span class="line">    net.add_module(<span class="string">&quot;global_avg_pool&quot;</span>, nn.AdaptiveAvgPool2d((<span class="number">1</span>,<span class="number">1</span>)))</span><br><span class="line">    net.add_module(<span class="string">&quot;fc&quot;</span>, nn.Sequential(nn.Flatten(),</span><br><span class="line">                                       nn.Linear(<span class="number">512</span>, num_classes)))</span><br><span class="line">    <span class="keyword">return</span> net</span><br><span class="line"></span><br><span class="line">net = resnet18(<span class="number">10</span>)</span><br><span class="line"><span class="comment"># 获取GPU列表</span></span><br><span class="line">devices = d2l.try_all_gpus()</span><br><span class="line"><span class="comment"># 我们将在训练代码实现中初始化网络</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">train</span>(<span class="params">net, num_gpus, batch_size, lr</span>):</span><br><span class="line">    train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)</span><br><span class="line">    devices = [d2l.try_gpu(i) <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(num_gpus)]</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">init_weights</span>(<span class="params">m</span>):</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">type</span>(m) <span class="keyword">in</span> [nn.Linear, nn.Conv2d]:</span><br><span class="line">            nn.init.normal_(m.weight, std=<span class="number">0.01</span>)</span><br><span class="line">    net.apply(init_weights)</span><br><span class="line">    <span class="comment"># 在多个GPU上设置模型</span></span><br><span class="line">    net = nn.DataParallel(net, device_ids=devices)</span><br><span class="line">    trainer = torch.optim.SGD(net.parameters(), lr)</span><br><span class="line">    loss = nn.CrossEntropyLoss()</span><br><span class="line">    timer, num_epochs = d2l.Timer(), <span class="number">10</span></span><br><span class="line">    animator = d2l.Animator(<span class="string">&#x27;epoch&#x27;</span>, <span class="string">&#x27;test acc&#x27;</span>, xlim=[<span class="number">1</span>, num_epochs])</span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epochs):</span><br><span class="line">        net.train()</span><br><span class="line">        timer.start()</span><br><span class="line">        <span class="keyword">for</span> X, y <span class="keyword">in</span> train_iter:</span><br><span class="line">            trainer.zero_grad()</span><br><span class="line">            X, y = X.to(devices[<span class="number">0</span>]), y.to(devices[<span class="number">0</span>])</span><br><span class="line">            l = loss(net(X), y)</span><br><span class="line">            l.backward()</span><br><span class="line">            trainer.step()</span><br><span class="line">        timer.stop()</span><br><span class="line">        animator.add(epoch + <span class="number">1</span>, (d2l.evaluate_accuracy_gpu(net, test_iter),))</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;测试精度：<span class="subst">&#123;animator.Y[<span class="number">0</span>][-<span class="number">1</span>]:<span class="number">.2</span>f&#125;</span>，<span class="subst">&#123;timer.avg():<span class="number">.1</span>f&#125;</span>秒/轮，&#x27;</span></span><br><span class="line">          <span class="string">f&#x27;在<span class="subst">&#123;<span class="built_in">str</span>(devices)&#125;</span>&#x27;</span>)</span><br><span class="line"><span class="comment"># 使用1个</span></span><br><span class="line">train(net, num_gpus=<span class="number">1</span>, batch_size=<span class="number">256</span>, lr=<span class="number">0.1</span>)</span><br><span class="line"><span class="comment"># 使用2个</span></span><br><span class="line">train(net, num_gpus=<span class="number">2</span>, batch_size=<span class="number">512</span>, lr=<span class="number">0.2</span>)</span><br></pre></td></tr></table></figure><h2 id="参数服务器"><a href="#参数服务器" class="headerlink" title="参数服务器"></a>参数服务器</h2>当我们从一个GPU迁移到多个GPU时，以及再迁移到包含多个GPU的多个服务器时（可能所有服务器的分布跨越了多个机架和多个网络交换机），分布式并行训练算法也需要变得更加复杂。</li></ol><ul><li>同步需要高度适应特定的网络基础设施和服务器内的连接，这种适应会严重影响同步所需的时间。</li><li>环同步对于p3和DGX-2服务器是最佳的，而对于其他服务器则未必。</li><li>当添加多个参数服务器以增加带宽时，分层同步策略可以工作的很好。<h3 id="练习-51"><a href="#练习-51" class="headerlink" title="练习"></a>练习</h3></li></ul><ol><li>请尝试进一步提高环同步的性能吗。（提示：可以双向发送消息。）<br> 可以尝试使用双向发送消息。在传统的环同步中，每个进程在每个阶段只能发送或接收消息，而双向发送消息可以使得进程在每个阶段既可以发送也可以接收消息，从而提高了通信的效率。这样可以减少通信的次数，加快算法的收敛速度。</li><li>在计算仍在进行中，可否允许执行异步通信？它将如何影响性能？<br> 允许在计算仍在进行中执行异步通信可以提高性能，因为它可以使得计算和通信重叠进行，减少了计算和通信之间的等待时间。但是，需要注意的是，在使用异步通信时需要确保通信操作不会影响计算的正确性。</li><li>怎样处理在长时间运行的计算过程中丢失了一台服务器这种问题？尝试设计一种容错机制来避免重启计算这种解决方案？<br> 可以通过设计容错机制来避免重启计算。一种常见的容错机制是使用检查点和恢复技术，定期保存计算状态到持久存储器中，以便在发生故障时能够重新启动计算。另一种方法是使用冗余计算节点，在计算过程中同时在多个节点上执行相同的计算任务，当某个节点发生故障时，可以从其他节点恢复计算。<h1 id="计算机视觉"><a href="#计算机视觉" class="headerlink" title="计算机视觉"></a>计算机视觉</h1></li></ol><h2 id="图像增广"><a href="#图像增广" class="headerlink" title="图像增广"></a>图像增广</h2><p>我们可以以不同的方式裁剪图像，使感兴趣的对象出现在不同的位置，减少模型对于对象出现位置的依赖。 我们还可以调整亮度、颜色等因素来降低模型对颜色的敏感度。</p><h3 id="常用的方法"><a href="#常用的方法" class="headerlink" title="常用的方法"></a>常用的方法</h3><p>翻转和裁剪<br>改变颜色：亮度，对比度，饱和度，色调。</p><h3 id="使用图像增广进行训练"><a href="#使用图像增广进行训练" class="headerlink" title="使用图像增广进行训练"></a>使用图像增广进行训练</h3><h3 id="练习-52"><a href="#练习-52" class="headerlink" title="练习"></a>练习</h3><ol><li>在不使用图像增广的情况下训练模型：<code>train_with_data_aug(no_aug, no_aug)</code>。比较使用和不使用图像增广的训练结果和测试精度。这个对比实验能支持图像增广可以减轻过拟合的论点吗？为什么？<br> 对比实验的结果可以支持图像增广可以减轻过拟合的论点。在实验中，使用图像增广的模型通常会在训练集上表现更好，同时在测试集上也能取得更好的泛化性能，即使在没有使用图像增广的情况下，模型可能会出现过拟合的现象。<br> 图像增广可以减轻过拟合的原因在于它可以增加训练数据的多样性，从而使得模型更加鲁棒。通过对训练图像进行随机变换，图像增广可以生成更多样化的训练样本，使得模型不容易记住训练集中的特定样本，从而降低过拟合的风险。</li><li>在基于CIFAR-10数据集的模型训练中结合多种不同的图像增广方法。它能提高测试准确性吗？<br> 在基于CIFAR-10数据集的模型训练中结合多种不同的图像增广方法可以提高测试准确性。通过使用多种不同的图像增广方法，可以进一步增加训练数据的多样性，使得模型更加鲁棒，从而提高模型在测试集上的泛化能力。</li><li>参阅深度学习框架的在线文档。它还提供了哪些其他的图像增广方法？<br> 平移（仿射变换），随即擦除，尺度变换<h2 id="微调"><a href="#微调" class="headerlink" title="微调"></a>微调</h2>使用迁移学习，从元数据学到的知识迁移到目标数据集。尽管Imagenet上数据集大多数与意思无关，但在此数据集上训练的模型可能会提取更通用的图像特征，这有助于识别边缘、纹理、形状和对象组合。<br>微调包含以下步骤：</li><li>在源数据集（例如ImageNet数据集）上预训练神经网络模型，即源模型。</li><li>创建一个新的神经网络模型，即目标模型。这将复制源模型上的所有模型设计及其参数（输出层除外）。我们假定这些模型参数包含从源数据集中学到的知识，这些知识也将适用于目标数据集。我们还假设源模型的输出层与源数据集的标签密切相关；因此不在目标模型中使用该层。</li><li>向目标模型添加输出层，其输出数是目标数据集中的类别数。然后随机初始化该层的模型参数。</li><li>在目标数据集（如椅子数据集）上训练目标模型。输出层将从头开始进行训练，而所有其他层的参数将根据源模型的参数进行微调。<h2 id="目标检测和边界框"><a href="#目标检测和边界框" class="headerlink" title="目标检测和边界框"></a>目标检测和边界框</h2><h3 id="边界框"><a href="#边界框" class="headerlink" title="边界框"></a>边界框</h3>在目标检测中，我们通常使用边界框来描述对象的空间位置。边界框是举行的，由鞠总左上角和右下角的xy坐标来决定另一种是边界框中心坐标和框的宽度和高度。<br>可以设计函数将两种表示方法进行转换，并在图像中可视化。<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">x = torch.tensor([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>])</span><br><span class="line">y = torch.tensor([<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>])</span><br><span class="line"><span class="comment"># 在新维度上堆叠张量</span></span><br><span class="line">stacked = torch.stack([x, y])</span><br><span class="line"><span class="comment"># 输出: tensor([[1, 2, 3],</span></span><br><span class="line"><span class="comment">#                [4, 5, 6]])</span></span><br><span class="line">stacked = torch.stack([x, y], dim=<span class="number">1</span>) </span><br><span class="line"><span class="built_in">print</span>(stacked) </span><br><span class="line"><span class="comment"># 输出: tensor([[1, 4], </span></span><br><span class="line"><span class="comment"># [2, 5], </span></span><br><span class="line"><span class="comment"># [3, 6]])</span></span><br></pre></td></tr></table></figure></li><li><code>torch.stack</code>会在新维度上堆叠张量，而<code>torch.cat</code>会在现有维度上拼接张量。</li><li><code>torch.stack</code>要求所有要堆叠的张量具有相同的形状，而<code>torch.cat</code>要求除了沿着指定维度之外的其他维度具有相同的形状。<h2 id="锚框"><a href="#锚框" class="headerlink" title="锚框"></a>锚框</h2>目标检测算法通常会在输入图像中采样大量的区域，然后判断这些区域中是否包含我们感兴趣的目标，并调整区域边界从而更准确地预测目标的真实边界框（ground-truth bounding box）。 不同的模型使用的区域采样方法可能不同。 这里我们介绍其中的一种方法：以每个像素为中心，生成多个缩放比和宽高比（aspect ratio）不同的边界框。 这些边界框被称为<em>锚框</em>（anchor box）。<h3 id="生成多个锚框"><a href="#生成多个锚框" class="headerlink" title="生成多个锚框"></a>生成多个锚框</h3>要生成多个不同形状的锚框，让我们设置许多缩放比（scale）取值s1,…,sn和许多宽高比（aspect ratio）取值r1,…,rm。 当使用这些比例和长宽比的所有组合以每个像素为中心时，输入图像将总共有wℎnm个锚框。 尽管这些锚框可能会覆盖所有真实边界框，但计算复杂性很容易过高。 在实践中，我们只考虑包含s1或r1的组合：<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#@save</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">multibox_prior</span>(<span class="params">data, sizes, ratios</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;生成以每个像素为中心具有不同形状的锚框&quot;&quot;&quot;</span></span><br><span class="line">    in_height, in_width = data.shape[-<span class="number">2</span>:]</span><br><span class="line">    device, num_sizes, num_ratios = data.device, <span class="built_in">len</span>(sizes), <span class="built_in">len</span>(ratios)</span><br><span class="line">    boxes_per_pixel = (num_sizes + num_ratios - <span class="number">1</span>)</span><br><span class="line">    size_tensor = torch.tensor(sizes, device=device)</span><br><span class="line">    ratio_tensor = torch.tensor(ratios, device=device)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 为了将锚点移动到像素的中心，需要设置偏移量。</span></span><br><span class="line">    <span class="comment"># 因为一个像素的高为1且宽为1，我们选择偏移我们的中心0.5</span></span><br><span class="line">    offset_h, offset_w = <span class="number">0.5</span>, <span class="number">0.5</span></span><br><span class="line">    steps_h = <span class="number">1.0</span> / in_height  <span class="comment"># 在y轴上缩放步长</span></span><br><span class="line">    steps_w = <span class="number">1.0</span> / in_width  <span class="comment"># 在x轴上缩放步长</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 生成锚框的所有中心点</span></span><br><span class="line">    center_h = (torch.arange(in_height, device=device) + offset_h) * steps_h</span><br><span class="line">    center_w = (torch.arange(in_width, device=device) + offset_w) * steps_w</span><br><span class="line">    shift_y, shift_x = torch.meshgrid(center_h, center_w, indexing=<span class="string">&#x27;ij&#x27;</span>)</span><br><span class="line">    shift_y, shift_x = shift_y.reshape(-<span class="number">1</span>), shift_x.reshape(-<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 生成“boxes_per_pixel”个高和宽，</span></span><br><span class="line">    <span class="comment"># 之后用于创建锚框的四角坐标(xmin,xmax,ymin,ymax)</span></span><br><span class="line">    w = torch.cat((size_tensor * torch.sqrt(ratio_tensor[<span class="number">0</span>]),</span><br><span class="line">                   sizes[<span class="number">0</span>] * torch.sqrt(ratio_tensor[<span class="number">1</span>:])))\</span><br><span class="line">                   * in_height / in_width  <span class="comment"># 处理矩形输入</span></span><br><span class="line">    h = torch.cat((size_tensor / torch.sqrt(ratio_tensor[<span class="number">0</span>]),</span><br><span class="line">                   sizes[<span class="number">0</span>] / torch.sqrt(ratio_tensor[<span class="number">1</span>:])))</span><br><span class="line">    <span class="comment"># 除以2来获得半高和半宽</span></span><br><span class="line">    anchor_manipulations = torch.stack((-w, -h, w, h)).T.repeat(</span><br><span class="line">                                        in_height * in_width, <span class="number">1</span>) / <span class="number">2</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 每个中心点都将有“boxes_per_pixel”个锚框，</span></span><br><span class="line">    <span class="comment"># 所以生成含所有锚框中心的网格，重复了“boxes_per_pixel”次</span></span><br><span class="line">    out_grid = torch.stack([shift_x, shift_y, shift_x, shift_y],</span><br><span class="line">                dim=<span class="number">1</span>).repeat_interleave(boxes_per_pixel, dim=<span class="number">0</span>)</span><br><span class="line">    output = out_grid + anchor_manipulations</span><br><span class="line">    <span class="keyword">return</span> output.unsqueeze(<span class="number">0</span>)</span><br></pre></td></tr></table></figure><h3 id="交并比（IoU）"><a href="#交并比（IoU）" class="headerlink" title="交并比（IoU）"></a>交并比（IoU）</h3>相交面积除以相并面积<h3 id="练习-53"><a href="#练习-53" class="headerlink" title="练习"></a>练习</h3></li><li>在multibox_prior函数中更改sizes和ratios的值。生成的锚框有什么变化？<br> 在<code>multibox_prior</code>函数中更改<code>sizes</code>和<code>ratios</code>的值会改变生成的锚框的大小和长宽比。<code>sizes</code>控制锚框的大小，<code>ratios</code>控制锚框的长宽比。更改这些参数会导致生成的锚框在大小和形状上有所变化。</li><li>构建并可视化两个IoU为0.5的边界框。它们是怎样重叠的？ <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.patches <span class="keyword">as</span> patches</span><br><span class="line"><span class="comment"># 创建两个边界框的坐标</span></span><br><span class="line">bbox1 = [<span class="number">0.3</span>, <span class="number">0.3</span>, <span class="number">0.6</span>, <span class="number">0.6</span>]  <span class="comment"># (xmin, ymin, xmax, ymax)</span></span><br><span class="line">bbox2 = [<span class="number">0.5</span>, <span class="number">0.5</span>, <span class="number">0.8</span>, <span class="number">0.8</span>]</span><br><span class="line"><span class="comment"># 计算两个边界框的重叠部分</span></span><br><span class="line">overlap_xmin = <span class="built_in">max</span>(bbox1[<span class="number">0</span>], bbox2[<span class="number">0</span>])</span><br><span class="line">overlap_ymin = <span class="built_in">max</span>(bbox1[<span class="number">1</span>], bbox2[<span class="number">1</span>])</span><br><span class="line">overlap_xmax = <span class="built_in">min</span>(bbox1[<span class="number">2</span>], bbox2[<span class="number">2</span>])</span><br><span class="line">overlap_ymax = <span class="built_in">min</span>(bbox1[<span class="number">3</span>], bbox2[<span class="number">3</span>])</span><br><span class="line"><span class="comment"># 计算重叠部分的面积</span></span><br><span class="line">overlap_area = <span class="built_in">max</span>(<span class="number">0</span>, overlap_xmax - overlap_xmin) * <span class="built_in">max</span>(<span class="number">0</span>, overlap_ymax - overlap_ymin)</span><br><span class="line"><span class="comment"># 计算两个边界框的面积</span></span><br><span class="line">area1 = (bbox1[<span class="number">2</span>] - bbox1[<span class="number">0</span>]) * (bbox1[<span class="number">3</span>] - bbox1[<span class="number">1</span>])</span><br><span class="line">area2 = (bbox2[<span class="number">2</span>] - bbox2[<span class="number">0</span>]) * (bbox2[<span class="number">3</span>] - bbox2[<span class="number">1</span>])</span><br><span class="line"><span class="comment"># 计算IoU</span></span><br><span class="line">iou = overlap_area / (area1 + area2 - overlap_area)</span><br><span class="line"><span class="comment"># 可视化边界框</span></span><br><span class="line">fig, ax = plt.subplots()</span><br><span class="line">ax.add_patch(patches.Rectangle((bbox1[<span class="number">0</span>], bbox1[<span class="number">1</span>]), bbox1[<span class="number">2</span>] - bbox1[<span class="number">0</span>], bbox1[<span class="number">3</span>] - bbox1[<span class="number">1</span>], fill=<span class="literal">False</span>))</span><br><span class="line">ax.add_patch(patches.Rectangle((bbox2[<span class="number">0</span>], bbox2[<span class="number">1</span>]), bbox2[<span class="number">2</span>] - bbox2[<span class="number">0</span>], bbox2[<span class="number">3</span>] - bbox2[<span class="number">1</span>], fill=<span class="literal">False</span>))</span><br><span class="line">plt.xlim(<span class="number">0</span>, <span class="number">1</span>)</span><br><span class="line">plt.ylim(<span class="number">0</span>, <span class="number">1</span>)</span><br><span class="line">plt.gca().invert_yaxis()  <span class="comment"># 反转y轴，使得原点在左上角</span></span><br><span class="line">plt.show()</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;IoU: <span class="subst">&#123;iou&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure></li><li>非极大值抑制是一种贪心算法，它通过移除来抑制预测的边界框。是否存在一种可能，被移除的一些框实际上是有用的？<br> 对于非极大值抑制（NMS），确实存在一些情况下被移除的边界框实际上是有用的。为了柔和地抑制，可以使用Soft-NMS算法，该算法通过减少重叠边界框的得分而不是完全删除它们来实现更平滑的抑制。Soft-NMS的关键思想是根据重叠的程度逐渐降低边界框的得分，而不是直接将其移除。</li><li>如何修改这个算法来柔和地抑制？可以参考Soft-NMS (Bodla et al., 2017)。 如果非手动，非最大限度的抑制可以被学习吗？<br> 非手动、非最大限度的抑制可以被学习，这通常通过训练一个边界框回归器来实现。边界框回归器的目标是预测每个边界框的位置和大小，从而使得最终的检测结果更加准确。这种方法可以在训练过程中学习到更有效的抑制策略，而不是简单地使用固定的阈值或规则来选择最佳边界框。<h2 id="多尺度目标检测"><a href="#多尺度目标检测" class="headerlink" title="多尺度目标检测"></a>多尺度目标检测</h2>如果为每个像素都生成的锚框，我们最终可能会得到太多需要计算的锚框。<br>减少图像上的锚框数量并不困难。 比如，我们可以在输入图像中均匀采样一小部分像素，并以它们为中心生成锚框。 此外，在不同尺度下，我们可以生成不同数量和不同大小的锚框。 直观地说，比起较大的目标，较小的目标在图像上出现的可能性更多样。 例如，1×1、1×2和2×2的目标可以分别以4、2和1种可能的方式出现在2×2图像上。 因此，当使用较小的锚框检测较小的物体时，我们可以采样更多的区域，而对于较大的物体，我们可以采样较少的区域。</li></ol><ul><li>在多个尺度下，我们可以生成不同尺寸的锚框来检测不同尺寸的目标。</li><li>通过定义特征图的形状，我们可以决定任何图像上均匀采样的锚框的中心。</li><li>我们使用输入图像在某个感受野区域内的信息，来预测输入图像上与该区域位置相近的锚框类别和偏移量。</li><li>我们可以通过深入学习，在多个层次上的图像分层表示进行多尺度目标检测。<h3 id="练习-54"><a href="#练习-54" class="headerlink" title="练习"></a>练习</h3></li></ul><ol><li>给定形状为1×c×ℎ×w的特征图变量，其中c、ℎ和w分别是特征图的通道数、高度和宽度。怎样才能将这个变量转换为锚框类别和偏移量？输出的形状是什么？<br> 使用一个1×1的卷积层将特征图变量转换为形状为1×(4k+c)×h×w的特征图。这个卷积层的输出通道数应为4k+c。将这个特征图展平为形状为1×((4k+c)×h×w)的向量。<br> 使用一个全连接层将展平后的向量转换为形状为1×((4k+1)×h×w)的向量。这个全连接层的输出大小应为(4k+1)×h×w，其中4k是偏移量的数量，1是类别预测的数量。<br> 将全连接层的输出重新整形为形状为1×(4k+1)×h×w的特征图，其中4k+1是每个位置的类别和偏移量的总数。<br> 最终输出的形状是1×(4k+1)×h×w，其中1表示批次大小为1。这个特征图包含每个位置的锚框类别和偏移量预测。<h2 id="目标检测数据集"><a href="#目标检测数据集" class="headerlink" title="目标检测数据集"></a>目标检测数据集</h2><h2 id="单发多框检测"><a href="#单发多框检测" class="headerlink" title="单发多框检测"></a>单发多框检测</h2>![[Pasted image 20240409202220.png]]<h2 id="区域卷积神经网络"><a href="#区域卷积神经网络" class="headerlink" title="区域卷积神经网络"></a>区域卷积神经网络</h2>区域卷积神经网络（R-CNN）系列是一系列用于目标检测的深度学习模型，主要包括R-CNN、Fast R-CNN、Faster R-CNN和Mask R-CNN等。这些模型在目标检测领域取得了重大突破，成为了目标检测任务中的经典模型。</li><li>R-CNN（Region-based Convolutional Neural Network）：R-CNN是最早的一种区域卷积神经网络模型，它通过选择性搜索（Selective Search）算法提取候选区域，并对每个候选区域进行卷积神经网络的特征提取和目标分类，从而实现目标检测。但是，R-CNN的速度较慢，因为它需要对每个候选区域分别进行卷积运算。</li><li>Fast R-CNN：Fast R-CNN对R-CNN进行了改进，提出了候选区域池化（RoI Pooling）层，将整个图像的特征图输入到卷积神经网络中，然后通过RoI Pooling层将每个候选区域映射到特征图上，并提取固定大小的特征。这样可以避免对每个候选区域都进行卷积运算，提高了速度和效率。</li><li>Faster R-CNN：Faster R-CNN在Fast R-CNN的基础上进一步改进，引入了区域提议网络（Region Proposal Network，RPN），用于生成候选区域。RPN通过在特征图上滑动一个小窗口来生成候选区域，并利用分类分支和回归分支对候选区域进行分类和精细化定位。这样可以将目标检测任务分为两个阶段：生成候选区域和目标分类定位，从而进一步提高了速度和效率。</li><li>Mask R-CNN：Mask R-CNN在Faster R-CNN的基础上添加了一个额外的分支用于实例分割。在目标检测的基础上，Mask R-CNN可以生成每个检测到的目标的精确掩码，从而实现目标的精确分割。<br>R-CNN系列模型的发展，使得目标检测在准确率和效率上取得了巨大的提升，成为了目标检测领域的重要里程碑。<h3 id="练习-55"><a href="#练习-55" class="headerlink" title="练习"></a>练习</h3></li><li><p>我们能否将目标检测视为回归问题（例如预测边界框和类别的概率）？可以参考YOLO模型 (<a href="https://zh-v2.d2l.ai/chapter_references/zreferences.html#id135" title="Redmon, J., Divvala, S., Girshick, R., &amp; Farhadi, A. (2016). You only look once: unified, real-time object detection. Proceedings of the IEEE conference on computer vision and pattern recognition (pp. 779–788).">Redmon <em>et al.</em>, 2016</a>)的设计。<br> 目标检测可以被视为回归问题，其中目标是预测边界框的位置和大小，以及每个边界框内物体类别的概率。YOLO模型（You Only Look Once）是一个将目标检测视为回归问题的经典模型，它在单个神经网络中同时预测多个边界框的位置和类别概率。</p><p> 与YOLO相比，单发多框检测（Single Shot MultiBox Detection，SSD）也是一种将目标检测视为回归问题的方法，但它采用了不同的设计思路。主要区别包括：</p><ol><li><p>网络结构：YOLO采用全卷积网络结构，将整个图像作为输入并直接输出预测边界框和类别概率的特征图。而SSD在特征图上应用一系列卷积和池化操作，然后在不同层次上预测不同尺度和长宽比的边界框。</p></li><li><p>多尺度特征：SSD利用不同层次的特征图来检测不同尺度的物体，从而提高了检测的准确性。而YOLO将所有预测都放在单个特征图上，可能会导致对小物体的检测效果不佳。</p></li><li><p>预测方式：YOLO使用单个全连接层来预测边界框和类别概率，而SSD在不同层次上使用卷积层来预测，这样可以提高模型对不同尺度物体的检测能力。</p></li></ol></li></ol><h2 id="语义分割和数据集"><a href="#语义分割和数据集" class="headerlink" title="语义分割和数据集"></a>语义分割和数据集</h2><p>计算机视觉领域还有2个与语义分割相似的重要问题，即<em>图像分割</em>（image segmentation）和<em>实例分割</em>（instance segmentation）。 我们在这里将它们同语义分割简单区分一下。</p><ul><li>图像分割：将图像划分为若干组成区域，这类问题的方法通常利用图像中像素之间的相关性。它在训练时不需要有关图像像素的标签信息，在预测时也无法保证分割出的区域具有我们希望得到的语义。</li><li>实例分割也叫同时检测并分割（simultaneous detection and segmentation），它研究如何识别图像中各个目标实例的像素级区域。与语义分割不同，实例分割不仅需要区分语义，还要区分不同的目标实例。例如，如果图像中有两条狗，则实例分割需要区分像素属于的两条狗中的哪一条。<h3 id="数据集"><a href="#数据集" class="headerlink" title="数据集"></a>数据集</h3>Pascal VOC2012 语义分割数据集<h3 id="练习-56"><a href="#练习-56" class="headerlink" title="练习"></a>练习</h3></li></ul><ol><li>如何在自动驾驶和医疗图像诊断中应用语义分割？还能想到其他领域的应用吗？<br> 语义分割可以帮助自动驾驶系统理解道路上不同区域的含义，如车道线、行人、车辆和路标等。这对于决策制定和车辆行驶路径规划非常重要。<br> 语义分割可以帮助医生准确地识别和分割出不同的组织结构或病变区域，如肿瘤、器官等，从而提高诊断的准确性和效率。<br> 农业，建筑与城市规划，环境保护</li><li>回想一下 <a href="https://zh-v2.d2l.ai/chapter_computer-vision/image-augmentation.html#sec-image-augmentation">13.1节</a>中对数据增强的描述。图像分类中使用的哪种图像增强方法是难以用于语义分割的？<br> 随机裁剪和随机翻转，对于语义分割来说并不适用。因为这些方法改变了图像的空间信息和像素分布，可能会导致语义分割结果不准确。<h2 id="转置卷积"><a href="#转置卷积" class="headerlink" title="转置卷积"></a>转置卷积</h2>用于逆转下采样导致的空间尺寸减少<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">tconv = nn.ConvTranspose2d(<span class="number">1</span>, <span class="number">1</span>, kernel_size=<span class="number">2</span>, padding=<span class="number">1</span>, bias=<span class="literal">False</span>)</span><br><span class="line">tconv.weight.data = K</span><br><span class="line">tconv(X)</span><br></pre></td></tr></table></figure>与常规卷积不同，在转置卷积中，填充被应用于的输出（常规卷积将填充应用于输入）。 例如，当将高和宽两侧的填充数指定为1时，转置卷积的输出中将删除第一和最后的行与列。<h3 id="练习-57"><a href="#练习-57" class="headerlink" title="练习"></a>练习</h3></li><li>在 <a href="https://zh-v2.d2l.ai/chapter_computer-vision/transposed-conv.html#subsec-connection-to-mat-transposition">13.10.3节</a>中，卷积输入<code>X</code>和转置的卷积输出<code>Z</code>具有相同的形状。他们的数值也相同吗？为什么？<br> 不相同</li><li>使用矩阵乘法来实现卷积是否有效率？为什么？<br> 使用矩阵乘法来实现卷积在某些情况下可能是有效率的，但在一般情况下通常不是最优的选择。这是因为卷积操作通常涉及大量的参数和稀疏权重，而矩阵乘法则会将这些稀疏性转换为密集计算，导致计算量增加。<h2 id="全卷积网络"><a href="#全卷积网络" class="headerlink" title="全卷积网络"></a>全卷积网络</h2>如 <a href="https://zh-v2.d2l.ai/chapter_computer-vision/semantic-segmentation-and-dataset.html#sec-semantic-segmentation">13.9节</a>中所介绍的那样，语义分割是对图像中的每个像素分类。 <em>全卷积网络</em>（fully convolutional network，FCN）采用卷积神经网络实现了从图像像素到像素类别的变换 (<a href="https://zh-v2.d2l.ai/chapter_references/zreferences.html#id100" title="Long, J., Shelhamer, E., &amp; Darrell, T. (2015). Fully convolutional networks for semantic segmentation. Proceedings of the IEEE conference on computer vision and pattern recognition (pp. 3431–3440).">Long <em>et al.</em>, 2015</a>)。 与我们之前在图像分类或目标检测部分介绍的卷积神经网络不同，全卷积网络将中间层特征图的高和宽变换回输入图像的尺寸：这是通过在 <a href="https://zh-v2.d2l.ai/chapter_computer-vision/transposed-conv.html#sec-transposed-conv">13.10节</a>中引入的<em>转置卷积</em>（transposed convolution）实现的。 因此，输出的类别预测与输入图像在像素级别上具有一一对应关系：通道维的输出即该位置对应像素的类别预测。<h3 id="模型-1"><a href="#模型-1" class="headerlink" title="模型"></a>模型</h3>全卷积最基本的设计：<br>全卷积网络先使用卷积神经网络抽取图像特征，然后通过1×1卷积层将通道数变换为类别个数。<h3 id="练习-58"><a href="#练习-58" class="headerlink" title="练习"></a>练习</h3></li><li>如果将转置卷积层改用Xavier随机初始化，结果有什么变化？<br> 通过使用Xavier随机初始化，转置卷积层的权重将以一种更合理的方式初始化，有助于加速模型的收敛速度，并可能提高模型的性能。具体效果取决于网络的结构、数据集和其他超参数的设置。<h2 id="风格迁移"><a href="#风格迁移" class="headerlink" title="风格迁移"></a>风格迁移</h2>首先，我们初始化合成图像，例如将其初始化为内容图像。 该合成图像是风格迁移过程中唯一需要更新的变量，即风格迁移所需迭代的模型参数。 然后，我们选择一个预训练的卷积神经网络来抽取图像的特征，其中的模型参数在训练中无须更新。 这个深度卷积神经网络凭借多个层逐级抽取图像的特征，我们可以选择其中某些层的输出作为内容特征或风格特征。这里选取的预训练的神经网络含有3个卷积层，其中第二层输出内容特征，第一层和第三层输出风格特征。<br>![[Pasted image 20240409223638.png]]<h3 id="练习-59"><a href="#练习-59" class="headerlink" title="练习"></a>练习</h3></li><li>选择不同的内容和风格层，输出有什么变化？<br> 选择更靠近网络底层的内容层可以保留更多的图像内容，而选择更靠近网络顶层的风格层可以强调更多的风格特征。</li><li>调整损失函数中的权重超参数。输出是否保留更多内容或减少更多噪点？<br> 增加内容损失的权重可能会使输出更接近于内容图像，减少噪点；而增加风格损失的权重可能会使输出更接近于风格图像，但也可能导致一些失真或不自然的效果。</li><li>替换实验中的内容图像和风格图像，能创作出更有趣的合成图像吗？<br> 可以</li><li>我们可以对文本使用风格迁移吗？<br> 在文本领域，风格迁移可以应用于文本生成、翻译和修改等任务，以生成具有不同风格的文本。<br>我的实验：<br>希望将如下两张图片进行风格迁移：<br>![[3.png]]<br>![[2.jpg]]<br>![[Pasted image 20240410004055.png]]</li></ol>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;预备知识&quot;&gt;&lt;a href=&quot;#预备知识&quot; class=&quot;headerlink&quot; title=&quot;预备知识&quot;&gt;&lt;/a&gt;预备知识&lt;/h1&gt;&lt;h2 id=&quot;数据操作&quot;&gt;&lt;a href=&quot;#数据操作&quot; class=&quot;headerlink&quot; title=&quot;数据操作&quot;&gt;&lt;/a</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title>sam入门</title>
    <link href="http://wwffyy.life/posts/11203c58.html"/>
    <id>http://wwffyy.life/posts/11203c58.html</id>
    <published>2024-01-07T03:38:31.000Z</published>
    <updated>2024-01-07T06:59:00.230Z</updated>
    
    <content type="html"><![CDATA[<h2 id="工作介绍"><a href="#工作介绍" class="headerlink" title="工作介绍"></a>工作介绍</h2><p>基于分割大模型sam，通过微调等方法迁移到分割相关领域辅助模型训练。</p><h2 id="相关文章"><a href="#相关文章" class="headerlink" title="相关文章"></a>相关文章</h2><h3 id="Segment-Anything"><a href="#Segment-Anything" class="headerlink" title="Segment Anything"></a>Segment Anything</h3><p>原始SAM文章，给出了模型架构，训练方法。<br>价值：学习到SAM基本输入输出形式，方便调用。<br>代码：<a href="https://github.com/facebookresearch/segment-anything">facebookresearch/segment-anything: The repository provides code for running inference with the SegmentAnything Model (SAM), links for downloading the trained model checkpoints, and example notebooks that show how to use the model. (github.com)</a></p><h3 id="SAM-Assisted-Remote-Sensing-Imagery-Semantic-Segmentation-with-Object-and-Boundary-Constraints"><a href="#SAM-Assisted-Remote-Sensing-Imagery-Semantic-Segmentation-with-Object-and-Boundary-Constraints" class="headerlink" title="SAM-Assisted Remote Sensing Imagery Semantic Segmentation with Object and Boundary Constraints"></a>SAM-Assisted Remote Sensing Imagery Semantic Segmentation with Object and Boundary Constraints</h3><p>一个基于sam辅助训练的语义分割模型。SAM 仅限于生成没有类别信息的分割结果，提出了一种辅助训练方法。通过调用sam大模型的接口，辅助遥感变换检测类任务训练。<br>数据集：ISPRS Vaihingen 和 LoveDA Urban遥感语义分割<br>代码：<a href="https://github.com/sstary/SSRS">sstary/SSRS (github.com)</a></p><h4 id="价值："><a href="#价值：" class="headerlink" title="价值："></a>价值：</h4><p>通过sam模型获取到图片的对象（SGO）和边界（SGB），提出了提出了一种新颖的对象一致性损失，并在这项工作中进一步引入了边界保留损失。</p><h4 id="方法："><a href="#方法：" class="headerlink" title="方法："></a>方法：</h4><p><img src="https://pic.imgdb.cn/item/659a23f9871b83018a3eed30.png" alt=""><br>主干网络使用Unetformer等主流分割方法，外加从sam导出的SGO与SGD进行损失计算。<br>冻结SAM的使用方法：<br><img src="https://pic.imgdb.cn/item/659a23fa871b83018a3eee1e.png" alt=""></p><h4 id="SAM使用方法："><a href="#SAM使用方法：" class="headerlink" title="SAM使用方法："></a>SAM使用方法：</h4><p>SAM 提供了一组应用程序编程接口 (API)，只需几行代码即可获得分段掩码。 API 中的不同提示支持不同的分割模式选项，例如全自动、边界框、点模式。</p><h4 id="损失计算方法："><a href="#损失计算方法：" class="headerlink" title="损失计算方法："></a>损失计算方法：</h4><ul><li>字母含义介绍：<br>$X$：输入图像<br>$P$：模型预测分割输出<br>$C$：需要分割的总类别数<br>$Y$：真实分割标签图<br>$Y<em>{o}$:sam生成对象图（SGO）<br>$Y</em>{b}$:sam生成边界图（SGB）<br>$M^{i}$:从SGO中对每一个对象提取得到的掩码</li></ul><ol><li>$L_{seg}$分割损失计算方法：<script type="math/tex; mode=display">L_{seg}=-\sum_{H,W}^{}\sum_{c\in C}^{}Y^{(H,W,c)}log (P^{(H,W,c)})</script></li><li>$L_{obj}$对象一致性损失计算方法：<br><img src="https://pic.imgdb.cn/item/659a23fa871b83018a3eee9c.png" alt=""><br>对象特征：<script type="math/tex; mode=display">F_{o}^{i}=P\odot M^{i}</script>对象平均特征：<script type="math/tex; mode=display">F_{avg}^{i}=\frac{G(F_i^o)}{N^i+1}\odot  M^i</script>其中G计算空间维度中所有像素的总和并重塑为其原始形状。<br>$N^i$是第$i$个对象中的点数<br>对象一致性损失计算：<br>MSE是均方误差<script type="math/tex; mode=display">L_{obj}=\sum_{K}^{i=1}MSE(F_o^i,F^i_{avg})</script></li><li>$L_{obj}$边界保留损失计算：<br>合并边缘约束可以有效增强遥感任务中语义分割模型的性能<br>边界全部设置为0<br>计算方法为：<script type="math/tex; mode=display">L_{bdy}=1-BF_1</script><script type="math/tex; mode=display">BF_1=2\times \frac{p_br_b}{p_b+r_b}</script>$p_b$和$r_b$分别为边界的精读和召回率</li><li>总损失<script type="math/tex; mode=display">L_{total}=L_{seg}+\lambda _oL_{obj}+\lambda _bL_{bdy}</script><h3 id="SamLP-A-Customized-Segment-Anything-Model-for-License-Plate-Detection"><a href="#SamLP-A-Customized-Segment-Anything-Model-for-License-Plate-Detection" class="headerlink" title="SamLP A Customized Segment Anything Model for License Plate Detection"></a>SamLP A Customized Segment Anything Model for License Plate Detection</h3>用sam定制的车牌检测模型<br>设计了一种低秩适应（LoRA）微调策略，将额外的参数注入到 SAM 中，并将 SAM 转移到 LP 检测任务中。<br>代码：<a href="https://github.com/Dinghaoxuan/SamLP">Dinghaoxuan/SamLP (github.com)</a><h4 id="贡献："><a href="#贡献：" class="headerlink" title="贡献："></a>贡献：</h4></li><li>将参数高效微调（PEFT）引入到SAM的传输中。具体来说，我们在我们提出的 SamLP 中提出了低秩适应 (LoRA)  调整策略，以使 SAM 适应 LP 检测数据集。LoRA：使SAM 最独特的能力（即位置提示）被抑制</li><li>可提示的（promptable）微调训练策略，及时车牌检测</li><li>少样本迁移能力<br>参数高效微调技术（PEFT）：</li><li>适配器调优Adapter tuning：在模型的每个层中插入小的、可训练的神经网络（称为 “adapters”），而不是调整模型的全部参数。</li><li>提示调优prompt tuning：在输入给模型的提示（prompt）中添加或优化一些 “tokens”，这些 “tokens” 被训练来改善模型的任务性能。</li><li>LoRA调优： 对 Transformer 中每一层的可训练低秩分解矩阵进行微调，即添加较小的可训练矩阵，显着减少可训练参数的数量。<br><img src="https://pic.imgdb.cn/item/659a23fa871b83018a3eef6f.png" alt=""><br>MaskDecoder的输出有：预测输出的掩码$\widehat{S}$，Iou分数Score，feature logits，各三个:<script type="math/tex; mode=display">\widehat{S}=\left \{  \widehat{S_1},\widehat{S_2},\widehat{S_3}\right \}</script><script type="math/tex; mode=display">\widehat{S} ,Score,lodits=Dec_M(F_I,Concat(T_M,T_P))</script><h4 id="pipeline："><a href="#pipeline：" class="headerlink" title="pipeline："></a>pipeline：</h4>具体而言，微调分为两个步骤：LoRA微调和promptable微调。首先，LoRA微调策略在SAM模型的图像编码器和掩码解码器的Transformer块中插入了一些LoRA层。这些LoRA层用于调整图像编码器以提取与车牌相关的特征，并使掩码解码器生成车牌分割掩码。在LoRA微调步骤中，由于输入仅为单个图像I，因此车牌的位置先验是未知的，因此没有提示编码器的信息，LoRA微调的输入提示为None。这意味着提示编码器没有适应车牌检测任务。为了保持SAM模型的可提示分割能力，设计了第二步，即可提示微调。在可提示微调中，来自LoRA微调的车牌分割结果被视为先验提示，用于引导可提示分割的训练。整个图像编码器及其LoRA层被冻结，只有提示编码器和掩码解码器中的LoRA层进行训练，以避免在图像嵌入中发生灾难性的遗忘，并加速可提示微调的训练过程。<h4 id="LoRA调优方法："><a href="#LoRA调优方法：" class="headerlink" title="LoRA调优方法："></a>LoRA调优方法：</h4>预训练语言模型的学习能力在将模型投影到低秩子空间时仍然有效。这意味着预训练模型具有较低的内在维度，在模型适应过程中参数的更新也具有较低的内在秩。<br><img src="https://pic.imgdb.cn/item/659a23fa871b83018a3ef009.png" alt=""><br>基础模型的预训练参数为$W_0 ∈ R^{d×k}$，其中d为输入特征维度，k为输出特征维度。$W_0$的优化方法：<script type="math/tex; mode=display">\widehat{W}_0=W_0+\Delta W=W_0+BA</script></li><li>A矩阵（$A ∈ R^{r×k}$）：这是一个低秩分解矩阵，其中r是低秩的大小，k是输出特征的维度。</li><li>B矩阵（$B ∈ R^{d×r}$）：这也是一个低秩分解矩阵，其中r是低秩的大小，d是输入特征的维度。<br>输入特征 $x ∈ R^{n×d}$ （n 是扁平化输入特征 x 的序列长度）同时与 $W_0$ 和 $ΔW$ 相乘，然后将它们的输出按元素求和作为输出特征 $h ∈ R^{n×k}$：<script type="math/tex; mode=display">h=W_0x+\Delta Wx=W_0x+BAx</script><img src="https://pic.imgdb.cn/item/659a2443871b83018a3ff542.png" alt=""><br>在 LoRA 微调步骤中，输入图像 I 被馈送到具有 LoRA 层的图像编码器中以获得图像嵌入。然后，输入提示P为None，这意味着没有点、框或掩模输入到提示编码器。之后，None 的提示嵌入与掩码标记相结合，然后掩码解码器从图像编码器检索图像嵌入中的高响应区域以生成二进制分割掩码。<script type="math/tex; mode=display">F_I=Enc_I(I)</script><script type="math/tex; mode=display">T_P=Enc_P(None)</script><script type="math/tex; mode=display">\widehat{S} ,Score,lodits=Dec_M(F_I,Concat(T_M,T_P))</script>最后计算Dice损失：<script type="math/tex; mode=display">L=\frac{1}{3}  {\textstyle \sum_{i=1}^{3}}Dice(\widehat{S}_i,S)</script><h4 id="可提示调优Promptable-Fine-tuning"><a href="#可提示调优Promptable-Fine-tuning" class="headerlink" title="可提示调优Promptable Fine-tuning"></a>可提示调优Promptable Fine-tuning</h4>设计了一个迭代细化训练管道，将提示引入分割中。<br><img src="https://s11.ax1x.com/2024/01/07/pizOAtU.png" alt=""></li></ol><h3 id="Adapting-Segment-Anything-Model-for-Change-Detection-in-VHR-Remote-Sensing-Images"><a href="#Adapting-Segment-Anything-Model-for-Change-Detection-in-VHR-Remote-Sensing-Images" class="headerlink" title="Adapting Segment Anything Model for Change Detection in VHR Remote Sensing Images"></a>Adapting Segment Anything Model for Change Detection in VHR Remote Sensing Images</h3><p>采用sam进行 VHR（超高分辨） 遥感图像变化检测<br>为了使 FastSAM 专注于 RS 场景中的某些特定地面物体，提出了一个卷积适配器来聚合面向任务的变化信息。为了利用 SAM 特征固有的语义表示，引入了一个与任务无关的语义学习分支来对双时态 RSI 中的潜在语义进行建模。在本文中，我们的目标是利用 SAM 强大且通用的语义开发能力来提高 CD 的准确性并减少对大量训练样本的依赖。<br>代码：<a href="https://github.com/ggsDing/SAM-CD">ggsDing/SAM-CD: Pytorch code of the SAM-CD (github.com)</a></p><h4 id="贡献：-1"><a href="#贡献：-1" class="headerlink" title="贡献："></a>贡献：</h4><ol><li>SAM-CD 将 FastSAM（SAM 的高效变体）适应 RS 场景，并利用多时态 LCLU 信息来嵌入变化表示。</li><li>将与任务无关的潜在图像学习引入到 CD 框架中。利用 SAM 的通用语义表示功能，生成的模型能够对 RSI 中的底层 LCLU 分布进行建模，以提高变化检测的准确性。这是通过测量多时间特征之间语义表示的相似性来监督的。</li></ol><h4 id="SAM局限性："><a href="#SAM局限性：" class="headerlink" title="SAM局限性："></a>SAM局限性：</h4><p>在某些领域表现出明显的局限性，包括医学图像、制造场景和 RSI 。由于它们主要接受自然图像的训练，因此它们倾向于更多地关注前景物体，并且难以分割小且不规则的物体。在这项工作中，我们利用自适应来微调 SAM（FastSAM），以学习 VHR RSI 中的潜在语义。</p><h4 id="典型的变化检测方法："><a href="#典型的变化检测方法：" class="headerlink" title="典型的变化检测方法："></a>典型的变化检测方法：</h4><p>利用两个连体编码器来提取时间特征，然后使用共享解码器将它们嵌入到变化表示中。<br>字母解释：<br>$(x_1,x_2)$：时间图像对<br>$U()$：编码器网络<br>$V()$：解码器网络<br>$m$：二进制CD图</p><script type="math/tex; mode=display">m=V[U(x_1,x_2)]</script><h4 id="本文方法"><a href="#本文方法" class="headerlink" title="本文方法"></a>本文方法</h4><p>我们期望通过比较底层语义特征可以更好地学习语义变化。通过视觉基础模型，现在可以在没有分类注释的情况下提取地面物体的语义。将视觉基础模型（VFM）作为视觉编码器，表示为$\widehat{U}()$,来提取通用语义特征（而不是时间差异）。得到预测CD图表示为：</p><script type="math/tex; mode=display">m=V[\widehat{U}(x_1),\widehat{U}(x_2)]</script><p><img src="https://s11.ax1x.com/2024/01/07/pizOmc9.png" alt=""><br>首先，我们利用 FastSAM 作为冻结编码器来利用视觉实体。为了更好地推广 RSI，引入了可训练的适配器来适应提取的特征。获得的多尺度 FastSAM 特征在类似 unet 的卷积解码器中进行融合和上采样。然后，除了嵌入变化表示的变化分支之外，我们还引入了一个额外的与任务无关的语义学习分支来对底层潜在语义进行建模。由此产生的 SAM-CD 是语义感知的，因此它可以更好地捕获 VHR RSI 中的对象变化。</p><h4 id="FastSAM-Adaptor"><a href="#FastSAM-Adaptor" class="headerlink" title="FastSAM Adaptor"></a>FastSAM Adaptor</h4><p>首先，我们将FastSAM在1/32、1/16、1/8和1/4的空间尺度上提取的特征进行聚合，表示为f1、f2、f3、f4。每个特征 fi 由相应的适配器 α 处理。<br>字母解释：<br>α ：adaptor适配器<br>$conv$：1x1卷积层<br>$bn$：批量归一化函数（batch normalization）<br>$γ()$：RELU激活函数</p><script type="math/tex; mode=display">f_i^*=\alpha(f_i)=\gamma\left \{ bn[conv(f_i)] \right \}</script><p>在这个过程中减少了通道数以降低冗余。<br>然后，我们采用类似unet的解码器来融合适应后的多尺度特征。对于每个级别的特征 fi，我们将其与解码器块中的较低级别特征 fi+1 融合，其中 di 是解码器中的第 i 层特征。然后我们将得到的特征${d1, d2, d3}$ 连接起来以获得适应 RS 域的语义表示。表示为： </p><script type="math/tex; mode=display">d_1=f_1</script><script type="math/tex; mode=display">d_{i+1}=conv[f_{i+1},upsample(d_i)]</script><h4 id="Task-agnostic-semantic-learning（任务无关的语义学习）"><a href="#Task-agnostic-semantic-learning（任务无关的语义学习）" class="headerlink" title="Task-agnostic semantic learning（任务无关的语义学习）"></a>Task-agnostic semantic learning（任务无关的语义学习）</h4><p>文献表明，多个相关任务的联合学习可以提高每个单个任务的性能。为了提高 CD 的性能，我们引入了一个额外的时间语义学习分支。即对多时相 RSI 进行分类后进行变化分割。对于上文改编后的 SAM 特征 ${d1, d2, d3}$。我们进一步使用卷积运算将它们转换为候选潜在变量$\widehat{l}$ ∈ $R^{k×w×h}$，其中 k 表示 RSI 中感兴趣的语义簇的数量。<br><img src="https://pic.imgdb.cn/item/659a2443871b83018a3ff597.png" alt=""><br>每个$f$表示一个卷积融合操作。<br>我们使用底层的时间约束来监督潜在语义的学习。与明确监督 LCLU 类别学习的文献研究不同，在二进制 CD 任务中，每个采集日期的语义标签不可靠。因此，SAM-CD 通过对齐特征表示来隐式监督双时态潜在变量的学习。对于每个候选潜在变量$\widehat{l}$，使用 softmax 函数对它们进行归一化：</p><script type="math/tex; mode=display">\phi _T(\widehat{l}_i)=\frac{e^{\widehat{l}_i/T }}{ {\textstyle \sum_{j=1}^{n}}e^{\widehat{l}_j/T } }</script><p>其中T是控制输出特征概率分布的温度参数。我们设置 T &gt; 1 以获得更多样化的语义表示。令 ${l1, l2}$为归一化双时态潜在变量，我们期望它们的语义表示在未更改的区域中相似。因此，我们提出时间约束损失$L_t$来衡量它们的时间相似性，计算如下：</p><script type="math/tex; mode=display">L_t(l_1,l_2)=[1-cosine(l_1,l_2)]\cdot c</script><p>其中，c是真实值GT的变体，将未变化的区域注释为 1。这是为了从损失计算中排除更改的区域。进一步利用注意力操作将语义焦点嵌入到变化特征 fc 中，然后将它们映射到变化映射 m 中。通过将 SAM 特征 $d1,d2,d3$ 传入到卷积块中来获得变化特征。注意力嵌入操作如下：</p><script type="math/tex; mode=display">m=conv_2\left \{\sigma[conv_1(l_1\oplus l_2)]\cdot f_c  \right \}</script><p>其中⊕是通道级联操作，σ是sigmoid归一化函数，conv1和conv2是两个用于调整特征通道的卷积模块。这确保了 CD 结果是语义感知的，从而更好地分割语义变化。</p><h4 id="训练细节："><a href="#训练细节：" class="headerlink" title="训练细节："></a>训练细节：</h4><p>SAM-CD仅利用FastSAM的视觉编码器并丢弃提示解码器。在与任务无关的语义学习分支中，语义嵌入块是单个 1×1 卷积层。考虑到典型 CD 应用中通常很少有有趣的 LCLU 类，语义通道数 k 根据经验设置为 8。在变化检测分支中，我们按照文献中的实践，利用 6 层残差卷积块来构建变化嵌入模块。这是为了更好地将语义特征转化为变化表示。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;工作介绍&quot;&gt;&lt;a href=&quot;#工作介绍&quot; class=&quot;headerlink&quot; title=&quot;工作介绍&quot;&gt;&lt;/a&gt;工作介绍&lt;/h2&gt;&lt;p&gt;基于分割大模型sam，通过微调等方法迁移到分割相关领域辅助模型训练。&lt;/p&gt;
&lt;h2 id=&quot;相关文章&quot;&gt;&lt;a href=&quot;</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title>IIM论文笔记</title>
    <link href="http://wwffyy.life/posts/bb54d268.html"/>
    <id>http://wwffyy.life/posts/bb54d268.html</id>
    <published>2023-10-20T10:59:45.000Z</published>
    <updated>2024-01-07T04:12:46.289Z</updated>
    
    <content type="html"><![CDATA[<h1 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h1><p>IIM（Independent Instance Map segmentation）独立实例地图分割，被用于人群定位领域。特点如下：</p><ol><li>端到端框架（end to end）</li><li>每个实例是不重叠的。通过将人群分割成独立的连通分量，获得位置和人群计数（分别为中心和分量的数目）</li><li>创新点：提出可微分二值化模块：（BM）<ul><li>针对不同图像自适应地学习阈值图，以更准确地检测每个实例; </li><li>使用二进制预测和标签的损失直接训练模型。</li></ul></li><li>主要方法：遵循启发式分支，并利用连通分量分割进行人群定位<h1 id="数据"><a href="#数据" class="headerlink" title="数据"></a>数据</h1>本文中所使用的数据类型为独立实例映射（Independent Instance Map），每个实例不重叠<br><img src="https://img-blog.csdnimg.cn/211bc5d4151b48d4bff98dc07fcd1f38.png" alt=""><br><img src="https://img-blog.csdnimg.cn/28bece5258014fa3a038c3e92e0f4c89.png" alt=""><br>其中images文件夹下为真实图片，masks文件夹包含了每张图片对应的独立实例，展示如下：</li></ol><p><img src="https://img-blog.csdnimg.cn/c9ac5c5277804bc7bd92fc362437e7dc.png" alt=""><img src="https://img-blog.csdnimg.cn/484f31888bbc4bd5837f6b1283a8acab.png" alt=""><br><strong>NWPU-Crowd</strong> ：是目前最大和最具挑战性的开源人群数据集。它包含标头点和框标签。共有5109张图片和2133238个注释实例。<br><strong>Shanghai Tech</strong> ：包含两个子集：A部分有482张图像，共241677个实例，B部分包含716张图像，包括88，488个标记的头。<br><strong>UCF-QNRF</strong> ：是一个密集的人群数据集，由1535张图像组成，共有1251642个实例。<br><strong>FDST</strong> ：由13个不同场景中捕获的100个视频组成。它包含15000个帧，共有394081个头，包括点和框标签。</p><h1 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h1><p>首先对人群场景中的头部区域进行置信度预测。图像或像素级二值化模块将置信度图分割成独立的实例图。在推断期间，阈值模块为每个置信图在线预测阈值。最后，通过检测4个连通分量，得到每个独立实例区域的框和中心。<img src="https://img-blog.csdnimg.cn/49ade09f0de0482f8b373cd4649b7cbf.png#pic_center" alt=""></p><h2 id="二值化层"><a href="#二值化层" class="headerlink" title="二值化层"></a>二值化层</h2><p><img src="https://img-blog.csdnimg.cn/f2212564f0764f359ab57c8e90e0358e.png#pic_center" alt=""><br>二值化层的目标是学习阈值T以分割输入图像，使得其输出图像O尽可能接近目标图像G。<br>对于输入I，输出O，二值化层B给出如下定义：<br><img src="https://img-blog.csdnimg.cn/79071cf7f7b74e2196e9d7692807281c.png#pic_center" alt=""><br>其中阈值T是要学习的参数。这个前向过程是不可微的，因为它是一个比较运算。因此，不可能自动计算T的梯度。这里我们定义了一个类似的数学模型来模拟二进制输出O和T之间的关系，其中输出仅与T有关系。<br>假设I ∈ RH×W满足两个条件：</p><ol><li>$H ×W → ∞$; </li><li>$I（i，j）\sim U（0，1）$，均匀分布</li></ol><p>那么，从概率统计的角度来看，累积分布函数FI（i，j）（T）与T的关系如下：<br><img src="https://img-blog.csdnimg.cn/3753d05abb9c46d096a3ef2e0efcffe9.png#pic_center" alt=""><br>其中$P（I（i，j）&lt;T）$表示I中的像素值小于T的概率。</p><h2 id="损失与反向传播"><a href="#损失与反向传播" class="headerlink" title="损失与反向传播"></a>损失与反向传播</h2><p>为了计算反向传播的损失，论文中将输出图像O中的所有像素求和为标量,并进行归一化，定义yˆ：<br><img src="https://img-blog.csdnimg.cn/3c845580dd3740b780522315d60848c9.png#pic_center" alt=""><br>根据上述公式推导，易知y^与阈值T的关系有：<br><img src="https://img-blog.csdnimg.cn/9d44232413b04b62b59369c48a37a422.png#pic_center" alt=""><br>可以看出y ^和阈值T是线性运算，y ^对T的导数值为1。为了使真实标签与预测值具有同样的形式，将G定义为：<br>       <img src="https://img-blog.csdnimg.cn/262ffbd4118d4091b2ac5d0e5dd2354c.png#pic_center" alt=""><br>即将真实数据中所有点的像素值相加并进行归一化<br>根据学习速率α，更新阈值T，公式如下：<br><img src="https://img-blog.csdnimg.cn/19cad6ed422e41d1969c8ac2f420758e.png#pic_center" alt=""><br>$L(yˆ, y)$是真实值G与输出O之间的损失函数，可以选择为$L1loss$或者其他。</p><h2 id="二值化模块"><a href="#二值化模块" class="headerlink" title="二值化模块"></a>二值化模块</h2><p>为了使阈值根据图像内容进行更新，文章根据上述二值化层进一步提出了二值化模块（BM），该模块由阈值编码器和二值化层组成。</p><ul><li>阈值编码器：对图像的特征图进行编码并生成单个值或图</li><li>二值化层：利用该值/图来二值化置信度图并输出实例图</li></ul><p>作为阈值编码器，特征F可以由具有参数Θ的阈值编码器映射到阈值T：<br><img src="https://img-blog.csdnimg.cn/1aeab8a815c444edb58bf3f4195aff13.png#pic_center" alt=""><br>在此公式中，阈值T可以有两种形式，可以是标量或大小与输入I相同的矩阵。这两种阈值表达方式分别对应了固定阈值与逐像素级阈值。固定阈值对于一张图片为一固定标量值，像素级阈值对于每一个像素点处具有不同的阈值。<br><img src="https://img-blog.csdnimg.cn/b08ae747ec874644b1f365d983219d36.png#pic_center" alt=""><br><img src="https://img-blog.csdnimg.cn/1796ad212a714fe483414a164f5d8568.png#pic_center" alt=""><br>对于示意图来说，存在用于二值化层的两个输入：欠分割图像I和阈值T。对于阈值编码器，T的每个像素处的导数为-1。这意味着当梯度流到阈值编码器时，梯度将被反转阈值T。因此，阈值学习器中的参数Θ可以通过如下的梯度下降来优化：<br><img src="https://img-blog.csdnimg.cn/4525b639fb05403b903dee506268176f.png#pic_center" alt=""><br>其中β是阈值编码器的学习速率。除了优化阈值编码器之外，可以用二进制预测和标签来计算硬损失，并且进行从输入I到置信度图的预测网络的反向传播（例如，置信度预测器）。与阈值编码器不同，置信度预测器的梯度具有与损失L（y，y）相同的符号。假设输入I由具有参数θ的置信度预测器输出，θ利用学习速率γ更新为：<br><img src="https://img-blog.csdnimg.cn/9ae0f1d0c70748738e3060f9bdd8ba57.png#pic_center" alt=""><br>阈值编码器和置信度预测器起对抗作用。置信度编码器希望使目标区域具有较高置信度，背景区域具有较低置信度。阈值学习器力求使目标区域具有低阈值，而背景区域具有高阈值。通过这种方式，可以尽可能多地滤除背景噪声，并且可以保留低置信度前景，如小尺度和大尺度头部，用于定位。由于这两个任务是对抗的，我们在主干网络结构（卷积层）和阈值学习器之间添加梯度分离操作。</p><h1 id="框架建立"><a href="#框架建立" class="headerlink" title="框架建立"></a>框架建立</h1><h2 id="置信图预测器（CP）"><a href="#置信图预测器（CP）" class="headerlink" title="置信图预测器（CP）"></a>置信图预测器（CP）</h2><p>我们希望通过检测连接组件来实现人群定位，因此使用高分辨率特征表示的主干网络结构来提取特征。论文中，有两种流行的网络被用作置信度预测器：</p><ol><li>VGG-16 + FPN，其使用VGG-16作为骨干并利用特征金字塔网络来编码多尺度特征; </li><li>HRNet-W48，一种高分辨率网络，具有强大的视觉识别特征表示能力<h2 id="阈值编码器（TE）"><a href="#阈值编码器（TE）" class="headerlink" title="阈值编码器（TE）"></a>阈值编码器（TE）</h2><strong>两种方法：</strong> 这里，提出了两种方案来二值化置信图预测器（CP）的输出：图像级和像素级二值化模块（简称“IBM”和“PBM”）。IBM和PBM分别使用单个值和映射对预测进行二进制化。<br>为了从图像内容学习阈值，设计了阈值编码器（TE），其输入等于原始特征图乘置信度图：$F := I  \bigodot F$ 使用预测的置信图进行哈达玛积(即形状相同的矩阵对应位置的元素相乘）以过滤背景噪声。</li></ol><p><strong>IBM</strong>：在IBM中，应用1 × 1卷积层和全局平均池化（GAP）来输出单个值作为二值化层的可学习阈值。<br><strong>PBM</strong>：IBM只能对于一张输入图像只能学习到一个特定阈值，但不同尺度的头部的置信度分布是非常不同的。因此，PBM中的阈值编码器（TE）被提出以产生像素级阈值图，其由具有PReLU的四个卷积层和具有步长为1的两个大内核平均池化组成。配置如下所示：</p><p><em>Conv：3×3</em>，<em>PReLU</em>; <em>Conv：3×3</em>，<em>PReLU</em>; <em>Conv：3×3</em>，<em>PReLU</em>;<em>Avgpool</em>（平均池化）：15 ×15; Conv：1×1，<em>Avgpool</em>（平均池化）：15×15，<em>Sigmoid</em>。</p><p>在PBM的TE中，为了覆盖大的空间感受野并保存存储器，输入特征被调整为原始尺寸的1/8。此外，在最后两个卷积层之后，应用步长为1的15 × 15平均池化来平滑输出。</p><p><strong>压缩Sigmoid</strong> ：在实验中，TE可能会在阈值图中产生一些很低（小于0.1）或很高（接近1）的值，这使得网络很容易波动。此外，高阈值导致头部区域中的许多空洞。因此，为了限制Sigmoid激活函数的输出范围。提出了压缩Sigmoid，将输出约束在（0.2，0.7）之间，其公式为：<br><img src="https://img-blog.csdnimg.cn/e64d6c871a5944189209e73a72bea58e.png#pic_center" alt=""></p><ul><li>在得到阈值图之后，它将与置信度图一起被馈送到二值化层。然后二值化层产生分割图。最后，通过检测连通区域，得到独立实例的盒和中心。</li></ul><h2 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h2><p>在该框架中涉及两个损失函数：</p><p><strong>置信图学习损失函数</strong>：<br>置信图学习是一个回归问题，MSELoss（均方误差）可以很好地训练它</p><p><strong>阈值映射学习损失函数</strong>：<br>阈值映射学习也是一个回归问题，阈值范围为（0.2，0.7）.因此，L1损失函数被应用于等式中的<br>L（y^，y）用于训练阈值学习器。除此之外，该目标函数为置信度预测器提供梯度。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>总的来说，IIM运行的流程为：</p><ol><li>原图进入置信度预测器（参数为$θ$），可选择$HR-net$或$VGG-16 + FPN$，得到置信度图为$I(x,y)$，尺寸与原图大小相同</li><li>置信度图与原图对应位置元素相乘，得到$F := I  \bigodot F$，作为阈值编码器的输入</li><li>输入数据$F :$进入阈值编码器后经过具有$PReLU$的四个卷积层和池化层后，得到逐像素的阈值图$T(x,y)$</li><li>阈值图$T(x,y)$和置信度图$I(x,y)$同时进入二值化层，对置信度图进行二值化并输出实例图$O(x,y)$</li><li>将输出实例图与真实图转化成标量后进行计算损失，损失函数采用$L1loss$,用于训练阈值编码器，不要忘记在二值化层的梯度反转</li><li>使用$MSELoss$训练置信度预测器<h2 id="模型评估"><a href="#模型评估" class="headerlink" title="模型评估"></a>模型评估</h2>本文中在模型评估时使用的Instance-level Precision（准确率）、Recall（召回率）和F1-measure（F1值）是用于评估二分类模型性能的常用指标<br>准确率（Precision）衡量了模型预测为正例的样本中有多少是真正的正例。准确率可以通过以下公式计算：<script type="math/tex; mode=display">Precision= \frac{TP}{TP+FP}</script>​</li></ol><p>其中，TP表示真阳性（True Positives），FP表示假阳性（False Positives）。准确率越高，模型预测为正例的样本中真正的正例越多。</p><p>召回率（Recall）衡量了模型能够正确预测出多少真正的正例。召回率可以通过以下公式计算：</p><script type="math/tex; mode=display">Recall= \frac{TP}{TP+FN}</script><p>其中，TP表示真阳性（True Positives），FN表示假阴性（False Negatives）。召回率越高，模型能够正确预测出更多的真正的正例。</p><p>F1值（F1-measure）是准确率和召回率的加权调和平均值，综合考虑了模型的准确性和召回能力。F1值可以通过以下公式计算：</p><script type="math/tex; mode=display">F1= \frac{2\cdot Precision\cdot Recall}{Precision+Recall}</script><p>F1值的取值范围为0到1，值越高表示模型的性能越好。</p><h1 id="方法对比"><a href="#方法对比" class="headerlink" title="方法对比"></a>方法对比</h1><p>三个经典的人群定位方法和IIM在四个数据集上的性能：<br><img src="https://img-blog.csdnimg.cn/e019aa6c5f2b452b9b8042f6bd589f6a.png" alt=""></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;概述&quot;&gt;&lt;a href=&quot;#概述&quot; class=&quot;headerlink&quot; title=&quot;概述&quot;&gt;&lt;/a&gt;概述&lt;/h1&gt;&lt;p&gt;IIM（Independent Instance Map segmentation）独立实例地图分割，被用于人群定位领域。特点如下：&lt;/p&gt;</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title>Norm_Softmax_VIT笔记</title>
    <link href="http://wwffyy.life/posts/464412ff.html"/>
    <id>http://wwffyy.life/posts/464412ff.html</id>
    <published>2023-06-03T11:23:49.000Z</published>
    <updated>2023-08-04T11:32:23.861Z</updated>
    
    <content type="html"><![CDATA[<h2 id="一些术语"><a href="#一些术语" class="headerlink" title="一些术语"></a>一些术语</h2><h3 id="batch：批次，一批处理，"><a href="#batch：批次，一批处理，" class="headerlink" title="batch：批次，一批处理，"></a>batch：批次，一批处理，</h3><h3 id="batch-size：表示每个batch有多少样本"><a href="#batch-size：表示每个batch有多少样本" class="headerlink" title="batch_size：表示每个batch有多少样本"></a>batch_size：表示每个batch有多少样本</h3><h3 id="LR（learning-rate-：学习率"><a href="#LR（learning-rate-：学习率" class="headerlink" title="LR（learning rate)：学习率"></a>LR（learning rate)：学习率</h3><h3 id="patch：补丁"><a href="#patch：补丁" class="headerlink" title="patch：补丁"></a>patch：补丁</h3><h3 id="epoch：周期，阶段"><a href="#epoch：周期，阶段" class="headerlink" title="epoch：周期，阶段"></a>epoch：周期，阶段</h3><h3 id="criterion：评判准则（一般用于命名损失函数）"><a href="#criterion：评判准则（一般用于命名损失函数）" class="headerlink" title="criterion：评判准则（一般用于命名损失函数）"></a>criterion：评判准则（一般用于命名损失函数）</h3><h3 id="optimizer：优化器"><a href="#optimizer：优化器" class="headerlink" title="optimizer：优化器"></a>optimizer：优化器</h3><h3 id="BP：反向传播算法（Back-Propagation），"><a href="#BP：反向传播算法（Back-Propagation），" class="headerlink" title="BP：反向传播算法（Back Propagation），"></a>BP：反向传播算法（Back Propagation），</h3><p>通过计算误差的反向传播来更新网络的权重和偏置</p><h3 id="Embedding："><a href="#Embedding：" class="headerlink" title="Embedding："></a>Embedding：</h3><p>是指将高维的离散型数据（如词汇、用户ID等）转换为低维的连续型向量的过程，也可以指转换后的向量</p><h3 id="感受野（Receptive-Field-："><a href="#感受野（Receptive-Field-：" class="headerlink" title="感受野（Receptive Field)："></a>感受野（Receptive Field)：</h3><p>指在神经网络中，输出特征映射上的一个像素点对应在输入图像中的区域。感受野的大小取决于网络的架构和层数，它可以用来衡量网络对输入信息的感知范围和理解能力。</p><p>具体来说，感受野的大小在卷积神经网络(Convolutional Neural Network, CNN)中是递增的，随着网络层数的增加，感受野的大小也随之增加。在前面的卷积层中，每个像素点的感受野通常只是输入图像的一个小区域，但是在后面的卷积层中，每个像素点的感受野可以覆盖整个输入图像。这样，网络就可以学习到更全局的特征和上下文信息，以更好地理解输入图像并提高分类或检测的准确性。</p><p>感受野的大小与卷积核的大小、步幅和填充有关。通常，较大的卷积核、较小的步幅和合适的填充可以增加感受野的大小。此外，池化层(Pooling Layer)也可以增加感受野大小，它通过池化操作对输入特征映射进行缩小，从而扩大感受野范围。</p><p>在神经网络设计中，理解和控制感受野大小可以帮助我们更好地设计网络结构和优化算法，从而提高网络的性能和稳定性。</p><h3 id="MLP（Multilayer-Perceptron，多层感知器）"><a href="#MLP（Multilayer-Perceptron，多层感知器）" class="headerlink" title="MLP（Multilayer Perceptron，多层感知器）"></a>MLP（Multilayer Perceptron，多层感知器）</h3><p>是一种基于前馈神经网络（Feedforward Neural Network）的模型，通常用于处理分类和回归问题。它由多个节点（神经元）组成，每个节点与前一层的所有节点相连。每个神经元的输出值是由前一层的神经元输出值通过权重参数的线性组合和激活函数的非线性变换计算得到的。MLP通常包含一个或多个隐藏层，以及一个输出层。也成为全连接神经网络（Fully Connected Neural Network）</p><h2 id="BN-Details"><a href="#BN-Details" class="headerlink" title="BN Details"></a>BN Details</h2><ol><li>Batch Normalization（BN）（批量归一化）是一种常用的神经网络正则化技术，旨在解决深度神经网络训练过程中的梯度消失和梯度爆炸问题，并改善网络性能和收敛速度。</li><li>为什么？<br> 每一层参数更新，会导致输入分布差距</li><li>是什么？<br>小批量数据计算均值方差，处理归一</li><li>怎么做？<br>B={x1,x2,…xn}<br>计算出该小批量数据的均值与方差，归一化<script type="math/tex; mode=display">\widehat{x}_{i}=\frac{x_{i}-\mu _{B}}{\sqrt{\sigma _{B}^{2}+\varepsilon}}</script><script type="math/tex; mode=display">y_{i}=\gamma \widehat{x}_{i}+\beta</script>乘系数加偏置是为了输入不是集中在-1，1，在通过激活函数后非线性.<h4 id="优缺点："><a href="#优缺点：" class="headerlink" title="优缺点："></a>优缺点：</h4></li><li><p>BN的基本思想是将神经网络中每个隐藏层的输入标准化，以使其均值为0，方差为1。具体来说，BN在每个训练批次中计算出批次输入的均值和方差，然后将其应用于批次中的每个样本。标准化后的样本通过调整缩放因子和偏移因子来恢复其原始输入特征空间，这些因子通过学习得到。这样，即使网络的输入和参数分布发生变化，BN也可以保持网络的稳定性。</p></li><li><p>BN的主要优点是可以提高网络的收敛速度和泛化能力，并具有一定的正则化作用，可以减少过拟合的风险。此外，BN可以减少对学习率和其他超参数的依赖，提高网络的可训练性和可调节性。</p></li><li><p>BN的一些缺点包括增加了网络的计算复杂性和内存使用，特别是在大型网络和GPU计算上。此外，BN对小批量数据的表现可能很差，因为它需要计算均值和方差的无偏估计，必须依赖于有效的批量大小。</p></li><li><p>总之，BN是一种强大的神经网络正则化技术，可以改善网络性能和训练过程的稳定性，特别是在深度神经网络和大型数据集上。</p><h2 id="Layer-Norm"><a href="#Layer-Norm" class="headerlink" title="Layer Norm"></a>Layer Norm</h2><p>Layer Norm（层标准化）是深度学习中一种常用的归一化方法，用于减小模型中许多常见问题，如梯度消失（vanishing gradients）和梯度爆炸（exploding gradients）等。它是对Batch Norm的一种改进，解决了Batch Norm难以应用于序列模型和小批量数据的问题。</p></li></ol><p>Layer Norm对于每个样本的每个特征分别进行归一化，使得每个样本在每个特征上都具有相似的分布，从而提高了模型的泛化性能。在Layer Norm中，对于每个样本的第i个特征，其标准化后的值为：</p><script type="math/tex; mode=display">\widehat{x}_{norm}=\frac{x_{i}-\mu _{B}}{\sqrt{\sigma _{B}^{2}+\varepsilon}}</script><p>其中，均值和方差是在特征维度上计算的，epsilon是一个很小的常量，用于防止除以0</p><script type="math/tex; mode=display">LayerNorm(x) = \gamma * x_{norm} + \beta</script><h3 id="区别"><a href="#区别" class="headerlink" title="区别"></a>区别</h3><ul><li>Layer Norm与Batch Norm的区别在于，Layer Norm是对每一层的输出进行标准化处理，而Batch Norm是对每一个batch进行标准化处理。<h2 id="Softmax-Details"><a href="#Softmax-Details" class="headerlink" title="Softmax Details"></a>Softmax Details</h2>softmax是一种用于多分类问题的激活函数，主要用于将一个向量转换为概率分布。在深度学习中，softmax通常用于输出层，将神经元的输出转换为类别概率。softmax函数的实现原理如下：<ol><li>计算每个神经元的指数值：对于输入向量x中的每个元素xi，计算<script type="math/tex">exp(xi)</script></li><li>计算指数值的和：将每个指数值相加，得到:<script type="math/tex">sum = Σ(exp(xi))</script></li><li>计算每个神经元的概率值：对于每个xi，计算:<script type="math/tex">pi = exp(xi) / sum</script></li><li>输出概率分布：将所有pi组成的向量作为输出，即<script type="math/tex">softmax(x) = [p1, p2, ..., pn]</script><br>其中，n是神经元的数量。softmax函数的输出可以被视为一个概率分布，其中每个元素pi表示输入向量属于第i个类别的概率。<h2 id="Transformer-VIT"><a href="#Transformer-VIT" class="headerlink" title="Transformer/VIT"></a>Transformer/VIT</h2>VIT（Vision Transformer）是用于图像分类的基于Transformer的模型。其步骤如下：</li></ol></li></ul><p><img src="https://pic4.zhimg.com/v2-5afd38bd10b279f3a572b13cda399233_r.jpg" alt="1132"></p><ol><li><p>图像的处理：将输入图像分割成多个小图像（patch）。</p></li><li><p>patch的线性变换：对于每个patch，使用一个线性变换将其转换为一个向量(patch_embesdding)</p></li><li><p>位置编码：为每个patch添加位置编码，以表示它们在原始图像中的位置(posttion_embedding)（与patch_embedding的形状完全一样）</p></li><li><p>输入嵌入：将所有patch的向量和位置编码拼接(相加)在一起，形成一个输入嵌入（input embedding）矩阵。</p></li><li><p>Layer Norm</p></li><li><p>Transformer Encoder：对输入嵌入进行多层Transformer编码器的处理，以便进行全局的特征提取。这里使用Multi-Head Attention方法，具体步骤如下：</p><ul><li><p>特征线性变换：每个输入序列复制3份（Q，K，V），经过线性变换，即将输入序列转换为多个查询（query）、键（key）和值（value）向量。</p></li><li><p>计算注意力分数：对于每个查询向量，计算它与所有键向量的内积，得到注意力分数的一组向量。<script type="math/tex">Output_{p1}=Attention(p1,p2)*p2+Attention(p1,p3)*p3+...+Attention(p1,p64)*p64</script></p></li><li><p>归一化：对注意力分数进行softmax归一化，得到所有键的权重分布。</p></li><li><p>加权求值：将所有值向量按照权重分布加权求和，得到该查询向量的注意力表示:引入wq，wk，wv权重，得到新的Q，K，V</p></li><li><p>多头注意力（Multi-Head）：将上述步骤在多个子空间中分别进行，得到不同子空间的注意力表示。即：wq，wk，wv权重矩阵有H组，计算注意力分数的操作可以进行H次。</p></li><li><p>输出合并：将各子空间的注意力（H次Multi-Head运算的结果）表示拼接成一个向量。使用softmax进行处理。</p></li><li><p>线性变换：将合并后的向量再进行一次线性变换，得到最终输出。</p></li></ul></li><li><p>Norm</p></li><li><p>（全局池化）：对于输出矩阵中的所有特征向量，进行全局池化（如平均池化或最大池化）得到一个全局特征向量。</p></li><li><p>全连接层：将全局特征向量输入到一个全连接层，进行分类预测。为了实现每个像素之间的交互。<br>附：师兄笔记<br><img src="https://i.imgtg.com/2023/06/03/Oqj90I.jpg" alt=""><br><img src="https://i.imgtg.com/2023/06/03/Oqj2MD.jpg" alt=""></p></li></ol><p><link rel="stylesheet" href="https://cdn1.tianli0.top/gh/zhheo/Post-Abstract-AI@0.7/tianli_gpt.css"></p><script>let tianliGPT_postSelector = '#post #article-container';let tianliGPT_key = 'd90e7ff2968fd4a313cd';</script><script src="https://cdn1.tianli0.top/gh/zhheo/Post-Abstract-AI@0.7/tianli_gpt.js"></script>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;一些术语&quot;&gt;&lt;a href=&quot;#一些术语&quot; class=&quot;headerlink&quot; title=&quot;一些术语&quot;&gt;&lt;/a&gt;一些术语&lt;/h2&gt;&lt;h3 id=&quot;batch：批次，一批处理，&quot;&gt;&lt;a href=&quot;#batch：批次，一批处理，&quot; class=&quot;headerli</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title>斯坦福 cs231n笔记</title>
    <link href="http://wwffyy.life/posts/c1ef5b44.html"/>
    <id>http://wwffyy.life/posts/c1ef5b44.html</id>
    <published>2023-04-25T00:42:02.000Z</published>
    <updated>2024-01-07T04:14:03.641Z</updated>
    
    <content type="html"><![CDATA[<p><img src="/images/2.jpg" alt=""></p><h1 id="图像分类"><a href="#图像分类" class="headerlink" title="图像分类"></a>图像分类</h1><h3 id="目标"><a href="#目标" class="headerlink" title="目标"></a>目标</h3><p>所谓图像分类问题，就是已有固定的分类标签集合，然后对于输入的图像，从分类标签集合中找出一个分类标签，最后把分类标签分配给该输入图像。虽然看起来挺简单的，但这可是计算机视觉领域的核心问题之一，并且有着各种各样的实际应用。计算机视觉领域中很多看似不同的问题（比如物体检测和分割），都可以被归结为图像分类问题。</p><h3 id="困难和挑战"><a href="#困难和挑战" class="headerlink" title="困难和挑战"></a>困难和挑战</h3><p>对于人来说，识别出一个像“猫”一样视觉概念是简单至极的，然而从计算机视觉算法的角度来看就值得深思了。以下为计算机视觉算法在图像识别方面遇到的一些困难，图像是以3维数组来表示的，数组中的元素是亮度值。</p><ul><li><strong>视角变化（Viewpoint variation</strong>）**：同一个物体，摄像机可以从多个角度来展现。</li><li><strong>大小变化（Scale variation</strong>）**：物体可视的大小通常是会变化的（不仅是在图片中，在真实世界中大小也是变化的）。</li><li><strong>形变（Deformation</strong>）：很多东西的形状并非一成不变，会有很大变化。</li><li><strong>遮挡（Occlusion</strong>）：目标物体可能被挡住。有时候只有物体的一小部分（可以小到几个像素）是可见的。</li><li><strong>光照条件（Illumination conditions</strong>）：在像素层面上，光照的影响非常大。</li><li><strong>背景干扰（Background clutter</strong>）：物体可能混入背景之中，使之难以被辨认。</li><li><strong>类内差异（Intra-class variation</strong>）：一类物体的个体之间的外形差异很大，比如椅子。这一类物体有许多不同的对象，每个都有自己的外形。</li></ul><p>面对以上所有变化及其组合，好的图像分类模型能够在维持分类结论稳定的同时，保持对类间差异足够敏感。</p><h3 id="最近邻分类器Nearest-Neighbour"><a href="#最近邻分类器Nearest-Neighbour" class="headerlink" title="最近邻分类器Nearest Neighbour"></a>最近邻分类器Nearest Neighbour</h3><p>那么具体如何比较两张图片呢？在本例中，就是比较32（长）x32（宽）x3（RGB）的像素块。最简单的方法就是逐个像素比较，最后将差异值全部加起来。换句话说，就是将两张图片先转化为两个向量<code>I1</code>和<code>I2</code>，然后计算他们的<strong>L1距离：</strong></p><script type="math/tex; mode=display">d_{1}(I_{1},I_{2})=\sum_{p}^{}|I_{1}^{p}-I_{2}^{p}|</script><p>以图片中的一个颜色通道为例来进行说明。两张图片使用L1距离来进行比较。逐个像素求差值，然后将所有差值加起来得到一个数值。如果两张图片一模一样，那么L1距离为0，但是如果两张图片很是不同，那L1值将会非常大。<br>代码操作：首先，将CIFAR-10的数据加载到内存中，并分成4个数组：训练数据和标签，测试数据和标签。在下面的代码中，<strong>Xtr</strong>（大小是50000x32x32x3）存有训练集中所有的图像，<strong>Ytr</strong>是对应的长度为50000的1维数组，存有图像对应的分类标签（从0到9）：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Xtr, Ytr, Xte, Yte = load_CIFAR10(<span class="string">&#x27;data/cifar10/&#x27;</span>) <span class="comment"># a magic function we provide</span></span><br><span class="line"><span class="comment"># flatten out all images to be one-dimensional</span></span><br><span class="line">Xtr_rows = Xtr.reshape(Xtr.shape[<span class="number">0</span>], <span class="number">32</span> * <span class="number">32</span> * <span class="number">3</span>) <span class="comment"># Xtr_rows becomes 50000 x 3072</span></span><br><span class="line">Xte_rows = Xte.reshape(Xte.shape[<span class="number">0</span>], <span class="number">32</span> * <span class="number">32</span> * <span class="number">3</span>) <span class="comment"># Xte_rows becomes 10000 x 3072</span></span><br></pre></td></tr></table></figure><br>训练一个分类器<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">nn = NearestNeighbor() <span class="comment"># create a Nearest Neighbor classifier class</span></span><br><span class="line">nn.train(Xtr_rows, Ytr) <span class="comment"># train the classifier on the training images and labels</span></span><br><span class="line">Yte_predict = nn.predict(Xte_rows) <span class="comment"># predict labels on the test images</span></span><br><span class="line"><span class="comment"># and now print the classification accuracy, which is the average number</span></span><br><span class="line"><span class="comment"># of examples that are correctly predicted (i.e. label matches)</span></span><br><span class="line"><span class="built_in">print</span> <span class="string">&#x27;accuracy: %f&#x27;</span> % ( np.mean(Yte_predict == Yte) )</span><br></pre></td></tr></table></figure><br>作为评价标准，我们常常使用<strong>准确率</strong>，它描述了我们预测正确的得分。请注意以后我们实现的所有分类器都需要有这个API：train(X, y)函数。该函数使用训练集的数据和标签来进行训练。从其内部来看，类应该实现一些关于标签和标签如何被预测的模型。这里还有个predict(X)函数，它的作用是预测输入的新数据的分类标签。现在还没介绍分类器的实现，下面就是使用L1距离的Nearest Neighbor分类器的实现套路：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">NearestNeighbor</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line">  <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">    <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">def</span> <span class="title function_">train</span>(<span class="params">self, X, y</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot; X is N x D where each row is an example. Y is 1-dimension of size N &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># the nearest neighbor classifier simply remembers all the training data</span></span><br><span class="line">    self.Xtr = X</span><br><span class="line">    self.ytr = y</span><br><span class="line"></span><br><span class="line">  <span class="keyword">def</span> <span class="title function_">predict</span>(<span class="params">self, X</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot; X is N x D where each row is an example we wish to predict label for &quot;&quot;&quot;</span></span><br><span class="line">    num_test = X.shape[<span class="number">0</span>]</span><br><span class="line">    <span class="comment"># lets make sure that the output type matches the input type</span></span><br><span class="line">    Ypred = np.zeros(num_test, dtype = self.ytr.dtype)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># loop over all test rows</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> xrange(num_test):</span><br><span class="line">      <span class="comment"># find the nearest training image to the i&#x27;th test image</span></span><br><span class="line">      <span class="comment"># using the L1 distance (sum of absolute value differences)</span></span><br><span class="line">      distances = np.<span class="built_in">sum</span>(np.<span class="built_in">abs</span>(self.Xtr - X[i,:]), axis = <span class="number">1</span>)</span><br><span class="line">      min_index = np.argmin(distances) <span class="comment"># get the index with smallest distance</span></span><br><span class="line">      Ypred[i] = self.ytr[min_index] <span class="comment"># predict the label of the nearest example</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> Ypred</span><br></pre></td></tr></table></figure><br>另一种计算向量间距离的方法，L2距离</p><script type="math/tex; mode=display">d_{1}(I_{1},I_{2})=\sqrt{\sum (I_{1}^{p}-I_{2}^{p})^{2}}</script><p>修改距离计算方法<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">distances = np.sqrt(np.<span class="built_in">sum</span>(np.square(self.Xtr - X[i,:]), axis = <span class="number">1</span>))</span><br></pre></td></tr></table></figure></p><h3 id="K-Nearest-Neighbour分类器"><a href="#K-Nearest-Neighbour分类器" class="headerlink" title="K-Nearest Neighbour分类器"></a>K-Nearest Neighbour分类器</h3><p>最近邻分类器只找最相近的那1个图片的标签，而k近邻是找最相似的k个图片的标签，然后让他们针对测试图片进行投票，最后把票数最高的标签作为对测试图片的预测。所以当k=1的时候，k-Nearest Neighbor分类器就是Nearest Neighbor分类器。从直观感受上就可以看到，更高的k值可以让分类的效果更平滑，使得分类器对于异常值更有抵抗力。<br>k值选取?(超参数)<br>验证集调优方法：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># assume we have Xtr_rows, Ytr, Xte_rows, Yte as before</span></span><br><span class="line"><span class="comment"># recall Xtr_rows is 50,000 x 3072 matrix</span></span><br><span class="line">Xval_rows = Xtr_rows[:<span class="number">1000</span>, :] <span class="comment"># take first 1000 for validation</span></span><br><span class="line">Yval = Ytr[:<span class="number">1000</span>]</span><br><span class="line">Xtr_rows = Xtr_rows[<span class="number">1000</span>:, :] <span class="comment"># keep last 49,000 for train</span></span><br><span class="line">Ytr = Ytr[<span class="number">1000</span>:]</span><br><span class="line"></span><br><span class="line"><span class="comment"># find hyperparameters that work best on the validation set</span></span><br><span class="line">validation_accuracies = []</span><br><span class="line"><span class="keyword">for</span> k <span class="keyword">in</span> [<span class="number">1</span>, <span class="number">3</span>, <span class="number">5</span>, <span class="number">10</span>, <span class="number">20</span>, <span class="number">50</span>, <span class="number">100</span>]:</span><br><span class="line"></span><br><span class="line">  <span class="comment"># use a particular value of k and evaluation on validation data</span></span><br><span class="line">  nn = NearestNeighbor()</span><br><span class="line">  nn.train(Xtr_rows, Ytr)</span><br><span class="line">  <span class="comment"># here we assume a modified NearestNeighbor class that can take a k as input</span></span><br><span class="line">  Yval_predict = nn.predict(Xval_rows, k = k)</span><br><span class="line">  acc = np.mean(Yval_predict == Yval)</span><br><span class="line">  <span class="built_in">print</span> <span class="string">&#x27;accuracy: %f&#x27;</span> % (acc,)</span><br><span class="line"></span><br><span class="line">  <span class="comment"># keep track of what works on the validation set</span></span><br><span class="line">  validation_accuracies.append((k, acc))</span><br></pre></td></tr></table></figure><br>程序结束后，可以作图分析出哪个k值表现最好，然后用这个k值来跑真正的测试集，并作出对算法的评价。（把训练集分成训练集和验证集。使用验证集来对所有超参数调优。最后只在测试集上跑一次并报告结果）<br><strong>交叉验证</strong>：有时候，训练集数量较小（因此验证集的数量更小），会使用一种被称为<strong>交叉验证</strong>的方法，这种方法更加复杂些。还是用刚才的例子，如果是交叉验证集，我们就不是取1000个图像，而是将训练集平均分成5份，其中4份用来训练，1份用来验证。然后我们循环着取其中4份来训练，其中1份来验证，最后取所有5次验证结果的平均值作为算法验证结果。</p><h1 id="线性分类"><a href="#线性分类" class="headerlink" title="线性分类"></a>线性分类</h1><h1 id="神经网络与反向传播"><a href="#神经网络与反向传播" class="headerlink" title="神经网络与反向传播"></a>神经网络与反向传播</h1><h1 id="卷积神经网络"><a href="#卷积神经网络" class="headerlink" title="卷积神经网络"></a>卷积神经网络</h1><h1 id="循环神经网络"><a href="#循环神经网络" class="headerlink" title="循环神经网络"></a>循环神经网络</h1>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;&lt;img src=&quot;/images/2.jpg&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;h1 id=&quot;图像分类&quot;&gt;&lt;a href=&quot;#图像分类&quot; class=&quot;headerlink&quot; title=&quot;图像分类&quot;&gt;&lt;/a&gt;图像分类&lt;/h1&gt;&lt;h3 id=&quot;目标&quot;&gt;&lt;a href=&quot;#目标&quot;</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title>线性代数知识点（遗忘则补充）</title>
    <link href="http://wwffyy.life/posts/376cf89b.html"/>
    <id>http://wwffyy.life/posts/376cf89b.html</id>
    <published>2023-03-18T14:50:20.000Z</published>
    <updated>2024-01-07T04:13:12.399Z</updated>
    
    <content type="html"><![CDATA[<h2 id="第1章-行列式"><a href="#第1章-行列式" class="headerlink" title="第1章 行列式"></a>第1章 行列式</h2><h3 id="1-1-全排列和对换"><a href="#1-1-全排列和对换" class="headerlink" title="1.1 全排列和对换"></a>1.1 全排列和对换</h3><h3 id="1-2-n阶行列式"><a href="#1-2-n阶行列式" class="headerlink" title="1.2 n阶行列式"></a>1.2 n阶行列式</h3><h3 id="1-3-行列式的性质"><a href="#1-3-行列式的性质" class="headerlink" title="1.3 行列式的性质"></a>1.3 行列式的性质</h3><h3 id="1-4-行列式按行（列）展开"><a href="#1-4-行列式按行（列）展开" class="headerlink" title="1.4 行列式按行（列）展开"></a>1.4 行列式按行（列）展开</h3><h2 id="第2章-矩阵及其运算"><a href="#第2章-矩阵及其运算" class="headerlink" title="第2章 矩阵及其运算"></a>第2章 矩阵及其运算</h2><h3 id="2-1-线性方程组和矩阵"><a href="#2-1-线性方程组和矩阵" class="headerlink" title="2.1 线性方程组和矩阵"></a>2.1 线性方程组和矩阵</h3><h3 id="2-2-矩阵的运算"><a href="#2-2-矩阵的运算" class="headerlink" title="2.2 矩阵的运算"></a>2.2 矩阵的运算</h3><h3 id="2-3-逆矩阵"><a href="#2-3-逆矩阵" class="headerlink" title="2.3 逆矩阵"></a>2.3 逆矩阵</h3><h3 id="2-4-Cramer法则"><a href="#2-4-Cramer法则" class="headerlink" title="2.4 Cramer法则"></a>2.4 Cramer法则</h3><h2 id="第3章-矩阵的初等变换与线性方程组"><a href="#第3章-矩阵的初等变换与线性方程组" class="headerlink" title="第3章 矩阵的初等变换与线性方程组"></a>第3章 矩阵的初等变换与线性方程组</h2><h3 id="3-1-矩阵的初等变换"><a href="#3-1-矩阵的初等变换" class="headerlink" title="3.1 矩阵的初等变换"></a>3.1 矩阵的初等变换</h3><h3 id="3-2-矩阵的秩"><a href="#3-2-矩阵的秩" class="headerlink" title="3.2 矩阵的秩"></a>3.2 矩阵的秩</h3><h3 id="3-3-方程组的解"><a href="#3-3-方程组的解" class="headerlink" title="3.3 方程组的解"></a>3.3 方程组的解</h3><h2 id="第4章-向量组的线性相关性"><a href="#第4章-向量组的线性相关性" class="headerlink" title="第4章 向量组的线性相关性"></a>第4章 向量组的线性相关性</h2><h3 id="4-1-向量组及其线性组合"><a href="#4-1-向量组及其线性组合" class="headerlink" title="4.1 向量组及其线性组合"></a>4.1 向量组及其线性组合</h3><h3 id="4-2-向量组的线性相关性"><a href="#4-2-向量组的线性相关性" class="headerlink" title="4.2 向量组的线性相关性"></a>4.2 向量组的线性相关性</h3><h3 id="4-3-向量组的秩"><a href="#4-3-向量组的秩" class="headerlink" title="4.3 向量组的秩"></a>4.3 向量组的秩</h3><h3 id="4-4-线性方程组解的结构"><a href="#4-4-线性方程组解的结构" class="headerlink" title="4.4 线性方程组解的结构"></a>4.4 线性方程组解的结构</h3><h3 id="4-5-向量空间"><a href="#4-5-向量空间" class="headerlink" title="4.5 向量空间"></a>4.5 向量空间</h3><h2 id="第5章-相似矩阵及二次型"><a href="#第5章-相似矩阵及二次型" class="headerlink" title="第5章 相似矩阵及二次型"></a>第5章 相似矩阵及二次型</h2><h3 id="5-1-向量的内积、长度及正交性"><a href="#5-1-向量的内积、长度及正交性" class="headerlink" title="5.1 向量的内积、长度及正交性"></a>5.1 向量的内积、长度及正交性</h3><h3 id="5-2-方阵的特征值与特征向量"><a href="#5-2-方阵的特征值与特征向量" class="headerlink" title="5.2 方阵的特征值与特征向量"></a>5.2 方阵的特征值与特征向量</h3><h3 id="5-3-相似矩阵"><a href="#5-3-相似矩阵" class="headerlink" title="5.3 相似矩阵"></a>5.3 相似矩阵</h3><h3 id="5-4-对称矩阵的对角化"><a href="#5-4-对称矩阵的对角化" class="headerlink" title="5.4 对称矩阵的对角化"></a>5.4 对称矩阵的对角化</h3><h3 id="5-5-二次型及其标准型"><a href="#5-5-二次型及其标准型" class="headerlink" title="5.5 二次型及其标准型"></a>5.5 二次型及其标准型</h3><h3 id="5-6-正定二次型"><a href="#5-6-正定二次型" class="headerlink" title="5.6 正定二次型"></a>5.6 正定二次型</h3>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;第1章-行列式&quot;&gt;&lt;a href=&quot;#第1章-行列式&quot; class=&quot;headerlink&quot; title=&quot;第1章 行列式&quot;&gt;&lt;/a&gt;第1章 行列式&lt;/h2&gt;&lt;h3 id=&quot;1-1-全排列和对换&quot;&gt;&lt;a href=&quot;#1-1-全排列和对换&quot; class=&quot;head</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title>机器学习笔记</title>
    <link href="http://wwffyy.life/posts/5a8a6c8d.html"/>
    <id>http://wwffyy.life/posts/5a8a6c8d.html</id>
    <published>2023-03-13T16:28:33.000Z</published>
    <updated>2024-01-07T04:14:22.306Z</updated>
    
    <content type="html"><![CDATA[<h1 id="统计学习方法"><a href="#统计学习方法" class="headerlink" title="统计学习方法"></a>统计学习方法</h1><p><em>赫尔伯特.西蒙：”如果一个系统能够通过执行某个过程改进它的性能，这就是学习。“</em></p><h3 id="1-1-统计学习-基本概念"><a href="#1-1-统计学习-基本概念" class="headerlink" title="1.1 统计学习 基本概念"></a>1.1 统计学习 基本概念</h3><ul><li>对象： 数据</li><li>目的： 对数据预测与分析</li><li>方法： 监督学习，无监督学习，强化学习</li><li>三要素： 模型，策略，算法</li></ul><h3 id="1-2-统计学习分类"><a href="#1-2-统计学习分类" class="headerlink" title="1.2 统计学习分类"></a>1.2 统计学习分类</h3><h4 id="1-2-1监督学习："><a href="#1-2-1监督学习：" class="headerlink" title="1.2.1监督学习："></a>1.2.1监督学习：</h4><p>学习输入到输出的统计规律。</p><ol><li>输入空间，输出空间，特征空间：输入与输出对成为样本</li><li>联合概率分布：假设输入X输出Y遵循联合分布概率P（X，Y），表示分布密度函数</li><li>假设空间：学习范围的确定</li><li>模型：用训练数据集学习一个模型，再用模型对测试样本集进行预测。由学习系统和预测系统完成。</li></ol><h4 id="1-2-2无监督学习："><a href="#1-2-2无监督学习：" class="headerlink" title="1.2.2无监督学习："></a>1.2.2无监督学习：</h4><p>从无标注数据中学习预测模型，学习数据中的统计规律或潜在结构。</p><ol><li>使用无标注数据学习或训练</li><li>可以用于对已有数据的分析，也可以对未来数据预测</li></ol><h4 id="1-2-3强化学习"><a href="#1-2-3强化学习" class="headerlink" title="1.2.3强化学习"></a>1.2.3强化学习</h4><p>智能系统在与环境的连续互动中学习最优行为策略的机器学习问题，本质为学习最优的序贯决策</p><ol><li>智能系统与环境互动：在每一步t，智能系统在环境中观测到一个状态st和一个奖励rt，采取一个动作at。环境根据智能体选择的动作，决定t+1的状态与奖励。目标是长期奖励的最大化。</li><li>马尔可夫决策过程</li></ol><h4 id="1-2-4半监督学习与主动学习"><a href="#1-2-4半监督学习与主动学习" class="headerlink" title="1.2.4半监督学习与主动学习"></a>1.2.4半监督学习与主动学习</h4><p>半监督学习，使用少量标注数据与大量未标注数据<br>主动学习，机器不断主动给出实例让教师进行标注，然后利用标注数据学习预测模型</p><h3 id="1-3-按模型分类"><a href="#1-3-按模型分类" class="headerlink" title="1.3 按模型分类"></a>1.3 按模型分类</h3><ol><li>概率模型与非概率模型：<ul><li>概率模型：决策树，朴素贝叶斯，隐马尔可夫，条件随机场，概率潜在语义分析，潜在迪利克雷分配，高斯混合模型</li><li>非概率模型：感知机，支持向量机，k近邻，adaboost，kmeans，潜在语义分析，神经网络<br>在监督学习中，概率模型是生成模型，非概率模型是判别模型<br>概率模型可用基本概率加法和乘法模型进行概率推理。</li></ul></li><li>线性模型与非线性模型：<ul><li>针对非概率模型，如果y=f（x）或z=g（x）是线性函数，则称模型为线性模型</li><li>否则为非线性模型</li></ul></li><li>参数化模型与非参数化模型：<ul><li>参数化模型假设模型参数维度固定，模型可用有限维数刻画</li><li>非参数化：维数不固定或无穷大，随训练数据量增大而增大</li></ul></li></ol><h3 id="1-4按算法分类"><a href="#1-4按算法分类" class="headerlink" title="1.4按算法分类"></a>1.4按算法分类</h3><ol><li>在线学习：每次接受一个样本进行预测，之后学习模型</li><li>批量学习：一次接受所有数据，学习模型之后进行预测</li></ol><h3 id="1-5三要素"><a href="#1-5三要素" class="headerlink" title="1.5三要素"></a>1.5三要素</h3><ol><li>模型：模型的假设空间包含所有可能的条件概率或决策函数</li><li>策略：在假设空间中寻找最优模型<ul><li>损失和风险函数：经验风险是模型关于训练样本集的平均损失记作Rexp（f）当样本容量趋于无穷时，经验风险趋于期望风险。</li><li>经验风险最小化和结构风险最小化</li><li>过拟合：为了减少训练误差而使得模型复杂度增加，导致测试误差增大（M次多项式拟合问题）</li></ul></li><li>算法：模型的具体计算方法</li></ol><h3 id="1-6正则化与交叉验证"><a href="#1-6正则化与交叉验证" class="headerlink" title="1.6正则化与交叉验证"></a>1.6正则化与交叉验证</h3><ol><li>正则化：在经验风险上加一个正则化项，模型越复杂，正则化数越大。</li><li>交叉验证：给定的数据切分，组合成为训练集与测试集，反复训练</li></ol><h3 id="1-7泛化能力"><a href="#1-7泛化能力" class="headerlink" title="1.7泛化能力"></a>1.7泛化能力</h3><ul><li>指由该方法学习到的模型对未知数据的预测能力</li><li>设学到的模型为f</li></ul><script type="math/tex; mode=display">R_{exp}(\widehat{f})=E_{p}[L(Y,\widehat{f}(X))]=\int_{\chi * \gamma }^{}L(y,\widehat{f}(x))P(x,y)d_{x}d_{y}</script><h3 id="1-8生成模型与判别模型（在监督学习中）"><a href="#1-8生成模型与判别模型（在监督学习中）" class="headerlink" title="1.8生成模型与判别模型（在监督学习中）"></a>1.8生成模型与判别模型（在监督学习中）</h3><ol><li>生成方法：由数据学习联合概率分布P（X，Y），求出P（Y|X）作为预测的模型：<script type="math/tex; mode=display">P(Y|X)=P(X,Y)/P(X)</script><ul><li>因为模型表示了给定输入X产生输出Y的生成关系</li><li>e.g.朴素贝叶斯，隐马尔可夫</li><li>特点：可以还原出联合分布概率P（X，Y），收敛速度更快</li></ul></li><li>判别方法：直接学习决策函数f（x）或条件概率分布P（X，Y）作为预测的模型。关心的是对给定的输入X，应预测什么样的输出Y<ul><li>特点：直接面对预测，直接学习P（Y|X）或决策函数f（X），可以对数据进行各种程度上的抽象、定义特征并使用特征，可以简化学习问题。</li></ul></li></ol><h3 id="1-9-监督学习应用"><a href="#1-9-监督学习应用" class="headerlink" title="1.9 监督学习应用"></a>1.9 监督学习应用</h3><ol><li>分类问题</li><li>标注问题<ul><li>学习一个模型，使它能够对观测序列给出标记序列作为预测（单词序列预测，词性标注）</li></ul></li><li>回归问题<ul><li>预测输入变量与输出变量之间的关系，特别是输入变量的值发生变化时，输出变量随之发生的变化。</li></ul></li></ol><h3 id="2-1感知机模型"><a href="#2-1感知机模型" class="headerlink" title="2.1感知机模型"></a>2.1感知机模型</h3><ul><li>感知机是二类分类的线性分类模型，输入为特征向量输出为类别，取+1，-1.将实例划分为正负两类分离超平面属于判别模型。<script type="math/tex; mode=display">f(x)=sign(\omega x+b)</script></li><li>$\omega$叫做权值向量，b叫做偏置<script type="math/tex; mode=display">sign(x)=\left\{\begin{matrix}+1 &x\geqslant 0  \\-1& x<0 \\\end{matrix}\right.</script></li><li>假设空间是特征空间中所有线性分类模型</li><li>训练数据集：实例的特征向量及类别（xi，yi）</li></ul><h3 id="2-2感知机学习策略"><a href="#2-2感知机学习策略" class="headerlink" title="2.2感知机学习策略"></a>2.2感知机学习策略</h3><ol><li>线性可分性：存在某个超平面S：$\omega x+b=0$能够将数据集的正负实例点完全正确划分到超平面的两侧，称数据集为线性可分数据集</li><li>感知机学习策略：目标是求得一个能够将训练集正实例点和负实例点完全正确分开的超平面。确定$\omega$和b，使损失最小化。<ul><li>损失函数：误分类点到超平面的总距离<script type="math/tex; mode=display">L(\omega ,b)=-\sum_{xi\in M}^{} y_{i}(\omega x_{i}+b)</script></li></ul></li></ol><h3 id="2-3感知机学习算法"><a href="#2-3感知机学习算法" class="headerlink" title="2.3感知机学习算法"></a>2.3感知机学习算法</h3><ul><li>随机梯度下降：学习率$\eta$(步长）（0-1），</li></ul><ol><li>任选取超平面w0，b0，</li><li>选取数据（xi，yi）</li><li>若<script type="math/tex">y_{i}(\omega x_{i}+b)\leqslant 0</script><script type="math/tex; mode=display">\omega<=\omega +\eta y_{i}x_{i}</script><script type="math/tex; mode=display">b<=b+\eta y_{i}</script></li><li>跳回（2），直到没有误分类点</li></ol><h3 id="2-4感知器学习算法的对偶形式"><a href="#2-4感知器学习算法的对偶形式" class="headerlink" title="2.4感知器学习算法的对偶形式"></a>2.4感知器学习算法的对偶形式</h3><ul><li>基本想法，将w和b表示为实例xi和标记yi的线性组合的形式，通过解其系数球的w和b。初始w和b为0，经过对误分类点的修改后，最后学习到的w和b可以表示为：(这里$\alpha$i=ni$\eta$)<script type="math/tex; mode=display">\omega=\sum_{i=1}^{N} \alpha _{i}x_{i}y_{i}</script><script type="math/tex; mode=display">b=\sum_{i=1}^{N} \alpha _{i}y_{i}</script><ol><li>a=0,b=0</li><li>在训练集中选取数据（xi，yi）</li><li>如果$y<em>{i}(\sum</em>{j=1}^{N}\alpha <em>{j}y</em>{j}x<em>{j}\cdot x</em>{i}+b)\leqslant 0$ :<script type="math/tex; mode=display">\alpha_{i}<=\alpha _{i}+\eta</script><script type="math/tex; mode=display">b<=b+\eta y_{i}</script></li><li>转到2直到没有误分类数据</li></ol></li></ul><h3 id="3-1k近邻算法"><a href="#3-1k近邻算法" class="headerlink" title="3.1k近邻算法"></a>3.1k近邻算法</h3><ul><li>给定一个训练数据集，对于新的训练实例，在训练数据集中找到与该实例最邻近的k个实例，这k个实例多数属于某个类，就把该输入实例分为这个类。</li></ul><h3 id="3-2k近邻模型"><a href="#3-2k近邻模型" class="headerlink" title="3.2k近邻模型"></a>3.2k近邻模型</h3><ol><li>模型：根据训练集，距离度量，k值，以及决策规则，将特征空间划分为一些子空间。</li><li>距离度量：点xi与xj的Lp距离：<script type="math/tex; mode=display">L_{p}(x_{i},x_{j})=(\sum_{l=1}^{n}\left|x_{i}^{(l)}- x_{j}^{(l)}\right|^{p})^{\frac{1}{p}}</script><ul><li>p=2,欧氏距离</li></ul></li></ol><script type="math/tex; mode=display">L_{2}(x_{i},x_{j})=(\sum_{l=1}^{n}\left|x_{i}^{(l)}- x_{j}^{(l)}\right|^{2})^{\frac{1}{2}}</script><ul><li>p=1,曼哈顿距离<script type="math/tex; mode=display">L_{1}(x_{i},x_{j})=\sum_{l=1}^{n}\left|x_{i}^{(l)}- x_{j}^{(l)}\right|</script></li><li>p=无穷<script type="math/tex; mode=display">L_{\infty }(x_{i},x_{j})=max\left|x_{i}^{(l)}- x_{j}^{(l)}\right|</script></li><li>不同距离度量确定的最近邻点是不同的</li></ul><h4 id="k值的选择"><a href="#k值的选择" class="headerlink" title="k值的选择"></a>k值的选择</h4><ul><li>k比较小，模型复杂容易过拟合</li><li>k大，学习误差小，模型简单</li></ul><h3 id="3-3k近邻的实现：kd树（未理解）"><a href="#3-3k近邻的实现：kd树（未理解）" class="headerlink" title="3.3k近邻的实现：kd树（未理解）"></a>3.3k近邻的实现：kd树（未理解）</h3><ul><li>kd树的构造：<ol><li>开始，构造根节点，由根节点生成深度为1的左右子节点。。。将落在切分超平面上的实例点保存在根节点</li><li>重复：对深度为j的结点选择x(l)为切分的坐标轴，l=j(mod k)+1,以该节点对应的区域中所有实例x(l)坐标的中位数为切分点。切分通过切分点并与坐标轴x(l)垂直的超平面实现。将落在切分超平面上的实例点保存在该节点。</li><li>直到两个子区域没有实例时停止。</li></ol></li><li>kd树搜索：</li></ul><h3 id="4-1朴素贝叶斯"><a href="#4-1朴素贝叶斯" class="headerlink" title="4.1朴素贝叶斯"></a>4.1朴素贝叶斯</h3><ol><li>基本方法：x为N维向量，y为类标记（class label），P（X，Y）为x和y的联合概率分布，训练数据集<script type="math/tex">T=\begin{Bmatrix}(x_{1},y_{1}),(x_{2},y_{2}),...,(x_{n},y_{n})\end{Bmatrix}</script><br>由P（X，Y）独立同分布产生。<ul><li>通过训练数据集学习联合分布概率P（X，Y）。先学习先验概率分布及条件概率分布：<script type="math/tex">P(Y=c_{k}),k=1,2,...,K</script></li><li>由于条件概率分布P（X=x，Y=ck）有指数量级的参数，不宜计算。朴素贝叶斯对条件概率分布作了条件独立性的假设：<script type="math/tex; mode=display">P(X=x|Y=c_{k})=P(X^{(1)}=x^{(1)},...,X^{(n)}=x^{(n)}|Y=c_{k})</script><script type="math/tex; mode=display">=\prod_{j=1}^{n}P(X^{(j)}  =x^{(j)}|Y=c_{k})</script></li><li>由学习到生成数据的机制，属于生成模型。条件独立假设等于说用于分类的特征在类确定条件下都是条件独立的。这一假设使朴素贝叶斯变得简单，有时牺牲准确率。</li><li>后验概率计算根据贝叶斯定理：<script type="math/tex; mode=display">P(Y=c_{k}|X=x)=\frac{P(X=x|Y=c_{k})P(Y=c_{k})}{\sum_{k}^{}P(X=x|Y=c_{k})P(Y=c_{k})}</script>将以上假设带入得到朴素贝叶斯分类基本公式：<script type="math/tex; mode=display">P(Y=c_{k}|X=x)=\frac{P(X=x|Y=c_{k})\prod_{j=1}^{n}P(X^{(j)}  =x^{(j)}|Y=c_{k})}{\sum_{k}^{}\prod_{j=1}^{n}P(X^{(j)}  =x^{(j)}|Y=c_{k})P(Y=c_{k})}</script></li><li>朴素贝叶斯分类器可表示为<script type="math/tex; mode=display">y=f(x)=argmaxP(Y=c_{k}|X=x)</script><script type="math/tex; mode=display">=\prod_{j=1}^{n}P(X^{(j)}  =x^{(j)}|Y=c_{k})</script>2.后验概率最大化的含义：为了使期望风险函数（条件期望）最小化，推导出后验概率最大化。</li></ul></li></ol><h3 id="4-2朴素贝叶斯的参数估计"><a href="#4-2朴素贝叶斯的参数估计" class="headerlink" title="4.2朴素贝叶斯的参数估计"></a>4.2朴素贝叶斯的参数估计</h3><ol><li>极大似然估计：<br><em>极大似然估计，通俗理解来说，就是利用已知的样本结果信息，反推最具有可能（最大概率）导致这些样本结果出现的模型参数值！换句话说，极大似然估计提供了一种给定观察数据来评估模型参数的方法，即：“模型已定，参数未知”。</em></li><li>朴素贝叶斯算法：<ul><li>计算先验概率及条件概率<script type="math/tex; mode=display">P(Y=c_{k})=\frac{\sum_{i=1}^{N}I(y_{i}=c_{k})}{N},k=1,2,...K</script><script type="math/tex; mode=display">P(X^{(j)}=a_{jl}|Y=c_{k})=\frac{\sum_{i=1}^{N}I(x_{i}^{(j)}=a_{jl},y_{i}=c_{k})}{\sum_{i=1}^{N}I(y_{i}=c_{k})}</script>j=1,2,…n;l=1,2,…Sj;k=1,2…K</li><li>对于给定的实例<script type="math/tex">x=(x^{(1)},x^{(2)},...x^{(n)})</script><script type="math/tex; mode=display">P(Y=c_{k})\prod_{j=1}^{n}P(X^{(j)}=x^{(j)}|Y=c_{k})</script></li><li>通过计算上式在$P(Y=c_{k})$为何值时取得最大，得出实例x的类。</li></ul></li><li>贝叶斯估计：用极大似然估计可能会出现所求概率值为0的情况。会影响后验概率的计算结果，使分类产生偏差。<script type="math/tex">P(X^{(j)}=a_{jl}|Y=c_{k})=\frac{\sum_{i=1}^{N}I(x_{i}^{(j)}=a_{jl},y_{i}=c_{k})+\lambda }{\sum_{i=1}^{N}I(y_{i}=c_{k})+\lambda S_{j}}</script>在随机变量各个取值的频数上赋予一个正数$\lambda$,常取1.成为拉普拉斯平滑。Sj为x所有可能总数量。同样，先验概率的贝叶斯估计是：<script type="math/tex">P(Y=c_{k})=\frac{\sum_{i=1}^{N}I(y_{i}=c_{k})+\lambda }{N+K\lambda },k=1,2,...K</script>K为所有y可能取值的总数量</li></ol><h3 id="5支持向量机"><a href="#5支持向量机" class="headerlink" title="5支持向量机"></a>5支持向量机</h3><p><em>支持向量机是一种二类分类模型。它的基本模型是定义在特征空间上的间隔最大的线性分类器，间隔最大使它有别于感知机。支持向量机的学习策略是间隔最大化，可形式化为一个求解凸二次规划的问题，等价于正则化的合页损失函数的最小化问题。</em></p><h3 id="5-1线性可分支持向量机与硬间隔最大化"><a href="#5-1线性可分支持向量机与硬间隔最大化" class="headerlink" title="5.1线性可分支持向量机与硬间隔最大化"></a>5.1线性可分支持向量机与硬间隔最大化</h3><ol><li>假设输入空间与特征空间为两个不同的空间。线性可分支持向量机假设这两个空间的元素一一对应，将输入空间中的输入映射为特征空间中的特征向量。假定给定一个特征空间上的训练数据集：<script type="math/tex">T=\begin{Bmatrix}(x_{1},y_{1}),(x_{2},y_{2}),...,(x_{n},y_{n})\end{Bmatrix}</script><br>其中：<script type="math/tex">x_{i}\in \chi =R^{n},y_{i}\in \gamma =\begin{Bmatrix}+1,-1\end{Bmatrix},i=1,2,...N</script><br>yi=1,称xi为正例；yi=-1，称xi为负例。假设训练数据是线性可分的，学习的目标是在特征空间中找到一个分离超平面，能将实例分到不同的类。分离超平面对应方程wx+b=0，由法向量w和截距决定。法向量指向的一侧为正类，一侧为负类。<br>当训练数据集线性可分时，由于存在无穷个分离超平面。感知机利用误分类最小的策略，，解有无穷多个；线性可分支持向量机利用间隔最大化求解最优分离超平面，这时，解是唯一的。<br>超平面：<script type="math/tex">\omega^{*} x+b^{*}=0</script><br>相应的分类决策函数：<script type="math/tex; mode=display">f(x)=sign(\omega^{*} x+b^{*})</script></li><li><p>函数间隔和几何间隔<br>一般来说，一个点距离超平面的远近可以表示分类预测的确信程度。在超平面$\omega x+b=0$<br>确定的情况下，用$\left| \omega x+b\right|$来相对表示x距离超平面的远近。$\omega x+b=0$的符号与标记y的符号是否一致来表示分类是否正确。<br>对于某一样本点的函数间隔：</p><script type="math/tex; mode=display">\widehat{\gamma _{i}}=y_{i}(\omega x_{i}+b)</script><ul><li>定义：超平面 （w，b）关于训练集T的函数间隔为超平面（w，b）关于T中所有样本点（xi，yi）的函数间隔的最小值：<script type="math/tex; mode=display">\widehat{\gamma }=min\widehat{\gamma _{i}}</script>函数间隔可以表示分类预测的正确性和确信度。但如果等比例改变w和b，超平面没有改变，但是函数间隔变化。因此，我们要对分离超平面的法向量w加以约束，如规范化||w||=1，使得间隔是确定的，这时间隔为几何间隔：<script type="math/tex; mode=display">\widehat{\gamma _{i}}=y_{i}(\frac{\omega}{\begin{Vmatrix}\omega \end{Vmatrix}} x_{i}+\frac{b}{\begin{Vmatrix}\omega \end{Vmatrix}})</script>同理，定义超平面（w，b）关于训练数据集T的几何间隔为各样本几何间隔最小值。<script type="math/tex; mode=display">\widehat{\gamma }=min\widehat{\gamma _{i}}</script></li></ul></li><li>硬间隔最大化：<ul><li>最大间隔分离超平面：<script type="math/tex; mode=display">\underset{\omega ,b}{min} \frac{1}{2}\begin{Vmatrix}\omega \end{Vmatrix}^{2}</script><script type="math/tex; mode=display">s.t.\ \  y_{i}(\omega x_{i}+b)-1\geqslant 0,i=1,2,...N</script>凸二次规划问题</li></ul></li><li>支持向量与间隔边界：在线性可分发情况下，训练样本数据集中的样本点中与分离超平面距离最近的样本点的实例成为支持向量.支持向量是使下式成立的点。<script type="math/tex">y_{i}(\omega x_{i}+b)-1= 0</script>对yi=+1的正例点，支持向量在超平面<script type="math/tex">H1:\omega x+b=1</script>对于yi=-1的负例点，支持向量在超平面<script type="math/tex">H2:\omega x+b=-1</script>在H1和H2上的点就是支持向量。</li></ol><h3 id="5-2软间隔最大化"><a href="#5-2软间隔最大化" class="headerlink" title="5.2软间隔最大化"></a>5.2软间隔最大化</h3><h3 id="6神经网络"><a href="#6神经网络" class="headerlink" title="6神经网络"></a>6神经网络</h3><ul><li>简介（以下由chatgpt生成）<br>神经网络算法是一类基于生物神经系统思维方式和人工智能技术的计算模型，被广泛应用于机器学习、图像识别、自然语言处理、语音识别、控制系统等领域。常用的神经网络算法包括以下几种：</li></ul><ol><li><p>前馈神经网络（Feedforward Neural Networks）：是最基本、最常用的神经网络算法，其特点是信息传递是单向的，不具有反馈和循环。可以通过调节网络的层数、神经元数量和权重等参数进行训练和优化。</p></li><li><p>卷积神经网络（Convolutional Neural Networks）：主要用于图像识别和计算机视觉等领域，其结构由多层卷积层、池化层和全连接层构成，可以有效地提取图像特征。</p></li><li><p>循环神经网络（Recurrent Neural Networks）：具有时间依赖性，可以处理序列数据、自然语言处理、语音识别等任务。其具有循环结构，可以记住过去的状态信息，并且通过反馈机制将当前状态与前面的状态相连。</p></li><li><p>长短期记忆网络（Long Short-Term Memory Networks）：是一种特殊的循环神经网络，能够有效地解决长时间依赖问题，主要应用于自然语言处理、语音识别、机器翻译等领域。</p></li><li><p>自编码器（Autoencoders）：是一种无监督学习方法，可以用于数据压缩、特征提取、图像去噪等任务，其原理是通过学习数据的低维表示来重构输入数据。</p></li></ol><p>神经网络算法的优点是能够进行大规模并行计算、处理非线性问题、具有自适应性和学习能力等，但也存在训练难度大、容易陷入局部最优解等问题。<br>当人们谈论神经网络时，通常指的是深度神经网络（Deep Neural Networks，DNNs），它们是一种人工神经网络（Artificial Neural Networks，ANNs）的变种。在深度学习中，神经网络是最流行的算法之一，用于解决各种任务，包括图像识别、自然语言处理和语音识别。</p><p>神经网络算法的基本原理是通过学习数据的规律来构建一个函数，将输入映射到输出。这个函数由神经网络的各个层组成，每个层都由多个神经元（或称为节点）组成。每个神经元接收到来自前一层的输入，并通过一些权重和偏置来计算输出。</p><p>神经网络的训练过程通常使用反向传播算法（Backpropagation）来优化神经元的权重和偏置。这个算法通过计算损失函数的梯度来更新神经元的权重和偏置，以使模型的预测更加准确。损失函数通常是一个衡量模型预测值与真实值之间差距的函数。</p><p>底层逻辑方面，神经网络是一种前馈网络，即数据从输入层开始流向输出层，中间没有反馈或循环。神经网络中每个神经元的输出是通过对它们的输入进行一系列数学运算来计算得出的。这些运算通常包括对输入进行加权求和，然后使用激活函数来转换成神经元的输出。</p><p>在深度神经网络中，通常有多个隐藏层，每个隐藏层都有多个神经元。每个隐藏层可以看作是对输入的一种抽象表示，它将输入中的一些特征提取出来并传递给下一层，最终得到输出。通过增加隐藏层和神经元的数量，神经网络可以学习更加复杂的模式和规律。</p><h3 id="7最速下降法"><a href="#7最速下降法" class="headerlink" title="7最速下降法"></a>7最速下降法</h3><ul><li>输入：目标函数f（x），梯度函数<script type="math/tex">g(x)=\triangledown f(x)</script>,精度$\varepsilon$.</li><li>输出：f（x）的极小值点x*</li></ul><ol><li>取初始值$x^{(0)}$,置k=0</li><li>计算$f(x^{(k)})$</li><li>计算梯度$g<em>{k}=g(x^{(k)})$,当$\begin{Vmatrix}g</em>{k}\end{Vmatrix}&lt;\varepsilon$时停止迭代，令x^{*}=x^{(k)}，否则令$p<em>{k}=-g(x^{(k)})$,求$\lambda</em>{k}$,,使：<script type="math/tex; mode=display">f(x^{(k)}+\lambda _{k}p_{k})=\underset{\lambda \geqslant 0}{min}f(x^{(k)}+\lambda p_{k})</script></li><li>置$x^{(k+1)}=x^{(k)}+\lambda <em>{k}p</em>{k}$,计算 $f(x^{(k+1)})$<br>当 $\begin{Vmatrix}f(x^{(k+1)}-f(x)^{(k)})\end{Vmatrix}&lt;\varepsilon$ 或： $\begin{Vmatrix}x^{(k+1)}-x^{(k)}\end{Vmatrix}&lt;\varepsilon$ 时，停止迭代，令 $x^{*}=x^{(k+1)}$</li><li>否则，令k=k+1，转到3.<br>计算量小，存储量小，必收敛<br>局部最优，收敛速度慢，大多数条件下线性收敛</li></ol><h3 id="8牛顿法"><a href="#8牛顿法" class="headerlink" title="8牛顿法"></a>8牛顿法</h3><ul><li>优点：收敛速度快，大多数情况下超线性收敛</li><li>缺点：二阶梯度正定时才下降，计算量大，当牛顿法无法产生下降方向时，用最速下降法代替</li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;统计学习方法&quot;&gt;&lt;a href=&quot;#统计学习方法&quot; class=&quot;headerlink&quot; title=&quot;统计学习方法&quot;&gt;&lt;/a&gt;统计学习方法&lt;/h1&gt;&lt;p&gt;&lt;em&gt;赫尔伯特.西蒙：”如果一个系统能够通过执行某个过程改进它的性能，这就是学习。“&lt;/em&gt;&lt;/p&gt;
&lt;</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title>已删除的笔记</title>
    <link href="http://wwffyy.life/posts/d63b390f.html"/>
    <id>http://wwffyy.life/posts/d63b390f.html</id>
    <published>2023-03-06T08:18:32.000Z</published>
    <updated>2024-03-29T01:37:37.914Z</updated>
    
    
    
    
    
  </entry>
  
  <entry>
    <title>Hello World</title>
    <link href="http://wwffyy.life/posts/4a17b156.html"/>
    <id>http://wwffyy.life/posts/4a17b156.html</id>
    <published>2023-03-01T01:21:09.565Z</published>
    <updated>2024-01-07T04:13:25.810Z</updated>
    
    <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;Welcome to &lt;a href=&quot;https://hexo.io/&quot;&gt;Hexo&lt;/a&gt;! This is your very first post. Check &lt;a href=&quot;https://hexo.io/docs/&quot;&gt;documentation&lt;/a&gt; for</summary>
      
    
    
    
    
  </entry>
  
</feed>
