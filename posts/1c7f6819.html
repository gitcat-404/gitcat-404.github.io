<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0"><title>李沐动手学深度学习部分笔记及练习 | 启蛰海</title><meta name="author" content="whisper"><meta name="copyright" content="whisper"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="预备知识数据操作广播机制对形状不同的张量进行相加操作，规则为a矩阵复制列，b矩阵复制行，将元素相加1234a &#x3D; torch.arange(3).reshape((3, 1))b &#x3D; torch.arange(2).reshape((1, 2))a, ba + b1234567(tensor([[0],         [1],         [2]]), tensor([[0, 1]]))te">
<meta property="og:type" content="article">
<meta property="og:title" content="李沐动手学深度学习部分笔记及练习">
<meta property="og:url" content="http://wwffyy.life/posts/1c7f6819.html">
<meta property="og:site_name" content="启蛰海">
<meta property="og:description" content="预备知识数据操作广播机制对形状不同的张量进行相加操作，规则为a矩阵复制列，b矩阵复制行，将元素相加1234a &#x3D; torch.arange(3).reshape((3, 1))b &#x3D; torch.arange(2).reshape((1, 2))a, ba + b1234567(tensor([[0],         [1],         [2]]), tensor([[0, 1]]))te">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://s21.ax1x.com/2024/03/29/pFosCVS.jpg">
<meta property="article:published_time" content="2024-04-10T06:54:20.000Z">
<meta property="article:modified_time" content="2024-04-10T07:03:38.998Z">
<meta property="article:author" content="whisper">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://s21.ax1x.com/2024/03/29/pFosCVS.jpg"><link rel="shortcut icon" href="/img/siteicon/128.png"><link rel="canonical" href="http://wwffyy.life/posts/1c7f6819.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="manifest" href="/manifest.json"/><meta name="msapplication-TileColor" content="#3b70fc"/><link rel="apple-touch-icon" sizes="180x180" href="/img/siteicon/128.png"/><link rel="icon" type="image/png" sizes="32x32" href="/img/siteicon/32.png"/><link rel="icon" type="image/png" sizes="16x16" href="/img/siteicon/16.png"/><link rel="mask-icon" href="/img/siteicon/128.png" color="#5bbad5"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search.xml","preload":false,"languages":{"hits_empty":"找不到您查询的内容：${query}"}},
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  date_suffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  }
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '李沐动手学深度学习部分笔记及练习',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2024-04-10 15:03:38'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
    win.getCSS = (url,id = false) => new Promise((resolve, reject) => {
      const link = document.createElement('link')
      link.rel = 'stylesheet'
      link.href = url
      if (id) link.id = id
      link.onerror = reject
      link.onload = link.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        link.onload = link.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(link)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><link rel="stylesheet" href="/css/custom.css" media="defer" onload="this.media='all'"><!-- hexo injector head_end start --><link rel="stylesheet" href="https://cdn.cbd.int/hexo-butterfly-clock-anzhiyu/lib/clock.min.css" /><link rel="stylesheet" href="https://cdn.cbd.int/hexo-butterfly-wowjs/lib/animate.min.css" media="print" onload="this.media='screen'"><link rel="stylesheet" href="https://cdn.cbd.int/hexo-butterfly-tag-plugins-plus@latest/lib/assets/font-awesome-animation.min.css" media="defer" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.cbd.int/hexo-butterfly-tag-plugins-plus@latest/lib/tag_plugins.css" media="defer" onload="this.media='all'"><script src="https://cdn.cbd.int/hexo-butterfly-tag-plugins-plus@latest/lib/assets/carousel-touch.js"></script><!-- hexo injector head_end end --><meta name="generator" content="Hexo 6.3.0"><link rel="alternate" href="/atom.xml" title="启蛰海" type="application/atom+xml">
</head><body><div id="loading-box" onclick="document.getElementById(&quot;loading-box&quot;).classList.add(&quot;loaded&quot;)"><div class="loading-bg"><div class="loading-img"></div><div class="loading-image-dot"></div></div></div><script>const preloader = {
  endLoading: () => {
    document.body.style.overflow = 'auto';
    document.getElementById('loading-box').classList.add("loaded")
  },
  initLoading: () => {
    document.body.style.overflow = '';
    document.getElementById('loading-box').classList.remove("loaded")

  }
}
window.addEventListener('load',()=> { preloader.endLoading() })

if (true) {
  document.addEventListener('pjax:send', () => { preloader.initLoading() })
  document.addEventListener('pjax:complete', () => { preloader.endLoading() })
}</script><link rel="stylesheet" href="/css/progress_bar.css"/><script src="https://cdn.jsdelivr.net/npm/pace-js/pace.min.js"></script><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="https://s21.ax1x.com/2024/03/29/pFosCVS.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">9</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">0</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">0</div></a></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/bangumis/index.html"><i class="fa-fw fas fa-heart"></i><span> 追番</span></a></div><div class="menus_item"><a class="site-page" href="/comments/"><i class="fa-fw fas fa-envelope"></i><span> 留言板</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header"><nav id="nav"><span id="blog-info"><a href="/" title="启蛰海"><img class="site-icon" src="https://s21.ax1x.com/2024/03/29/pFoskCj.png"/><span class="site-name">启蛰海</span></a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search" href="javascript:void(0);"><i class="fas fa-search fa-fw"></i><span> 搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/bangumis/index.html"><i class="fa-fw fas fa-heart"></i><span> 追番</span></a></div><div class="menus_item"><a class="site-page" href="/comments/"><i class="fa-fw fas fa-envelope"></i><span> 留言板</span></a></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">李沐动手学深度学习部分笔记及练习</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2024-04-10T06:54:20.000Z" title="发表于 2024-04-10 14:54:20">2024-04-10</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2024-04-10T07:03:38.998Z" title="更新于 2024-04-10 15:03:38">2024-04-10</time></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="李沐动手学深度学习部分笔记及练习"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h1 id="预备知识"><a href="#预备知识" class="headerlink" title="预备知识"></a>预备知识</h1><h2 id="数据操作"><a href="#数据操作" class="headerlink" title="数据操作"></a>数据操作</h2><h3 id="广播机制"><a href="#广播机制" class="headerlink" title="广播机制"></a>广播机制</h3><p>对形状不同的张量进行相加操作，规则为a矩阵复制列，b矩阵复制行，将元素相加<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">a = torch.arange(<span class="number">3</span>).reshape((<span class="number">3</span>, <span class="number">1</span>))</span><br><span class="line">b = torch.arange(<span class="number">2</span>).reshape((<span class="number">1</span>, <span class="number">2</span>))</span><br><span class="line">a, b</span><br><span class="line">a + b</span><br></pre></td></tr></table></figure><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">(tensor([[<span class="number">0</span>],</span><br><span class="line">         [<span class="number">1</span>],</span><br><span class="line">         [<span class="number">2</span>]]),</span><br><span class="line"> tensor([[<span class="number">0</span>, <span class="number">1</span>]]))</span><br><span class="line">tensor([[<span class="number">0</span>, <span class="number">1</span>],</span><br><span class="line">        [<span class="number">1</span>, <span class="number">2</span>],</span><br><span class="line">        [<span class="number">2</span>, <span class="number">3</span>]])</span><br></pre></td></tr></table></figure></p>
<h3 id="节省内存"><a href="#节省内存" class="headerlink" title="节省内存"></a>节省内存</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Z = torch.zeros_like(Y)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;id(Z):&#x27;</span>, <span class="built_in">id</span>(Z))</span><br><span class="line">Z[:] = X + Y</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;id(Z):&#x27;</span>, <span class="built_in">id</span>(Z))</span><br></pre></td></tr></table></figure>
<p>发现Z的id未变化，减少了内存开销</p>
<h3 id="转换numpy对象"><a href="#转换numpy对象" class="headerlink" title="转换numpy对象"></a>转换numpy对象</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">A = X.numpy()</span><br><span class="line">B = torch.tensor(A)</span><br><span class="line"><span class="built_in">type</span>(A), <span class="built_in">type</span>(B)</span><br></pre></td></tr></table></figure>
<p>将大小为1的张量转换为python标量，调用item函数，或者float，int等函数进行类型转换<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">a = torch.tensor([<span class="number">3.5</span>])</span><br><span class="line">a, a.item(), <span class="built_in">float</span>(a), <span class="built_in">int</span>(a)</span><br><span class="line">(tensor([<span class="number">3.5000</span>]), <span class="number">3.5</span>, <span class="number">3.5</span>, <span class="number">3</span>)</span><br></pre></td></tr></table></figure></p>
<h3 id="练习"><a href="#练习" class="headerlink" title="练习"></a>练习</h3><ol>
<li>运行本节中的代码。将本节中的条件语句<code>X == Y</code>更改为<code>X &lt; Y</code>或<code>X &gt; Y</code>，然后看看你可以得到什么样的张量。<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tensor([[<span class="literal">False</span>, <span class="literal">False</span>, <span class="literal">False</span>, <span class="literal">False</span>], [ <span class="literal">True</span>, <span class="literal">True</span>, <span class="literal">True</span>, <span class="literal">True</span>], [ <span class="literal">True</span>, <span class="literal">True</span>, <span class="literal">True</span>, <span class="literal">True</span>]])</span><br></pre></td></tr></table></figure>
会发现同样得到了与原tensor大小相同的逻辑值</li>
<li>用其他形状（例如三维张量）替换广播机制中按元素操作的两个张量。结果是否与预期相同？<br>如果两个张量在某个维度上的大小不同，其中一个的大小必须是1，这样它就可以在该维度上进行扩展。<br>如果两个张量在某个维度上的大小都是1，或者其中一个张量在该维度上不存在（即它的大小在该维度上为1），则在该维度上的大小将被设置为较大的那个大小。<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">a = torch.tensor([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>])</span><br><span class="line">b = torch.tensor([<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>])</span><br><span class="line">result = a + b</span><br><span class="line"><span class="built_in">print</span>(result)</span><br><span class="line">a_3d = a.unsqueeze(<span class="number">0</span>).unsqueeze(-<span class="number">1</span>)</span><br><span class="line">b_3d = b.unsqueeze(<span class="number">0</span>).unsqueeze(<span class="number">0</span>) </span><br><span class="line">result_3d = a_3d + b_3d</span><br><span class="line"><span class="built_in">print</span>(result_3d)</span><br><span class="line"></span><br><span class="line">tensor([<span class="number">5</span>, <span class="number">7</span>, <span class="number">9</span>]) tensor([[[<span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>], [<span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>], [<span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>]]])</span><br></pre></td></tr></table></figure>
<h2 id="数据预处理"><a href="#数据预处理" class="headerlink" title="数据预处理"></a>数据预处理</h2><h3 id="数据集读取"><a href="#数据集读取" class="headerlink" title="数据集读取"></a>数据集读取</h3>基本操作：创建文件、路径组合、文件写入<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line">os.makedirs(os.path.join(<span class="string">&#x27;..&#x27;</span>, <span class="string">&#x27;data&#x27;</span>), exist_ok=<span class="literal">True</span>)</span><br><span class="line">data_file = os.path.join(<span class="string">&#x27;..&#x27;</span>, <span class="string">&#x27;data&#x27;</span>, <span class="string">&#x27;house_tiny.csv&#x27;</span>)</span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(data_file, <span class="string">&#x27;w&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">    f.write(<span class="string">&#x27;NumRooms,Alley,Price\n&#x27;</span>)  <span class="comment"># 列名</span></span><br></pre></td></tr></table></figure>
读取csv方法<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line">data = pd.read_csv(data_file)</span><br></pre></td></tr></table></figure>
<h3 id="缺失值处理"><a href="#缺失值处理" class="headerlink" title="缺失值处理"></a>缺失值处理</h3>对于一组数据：<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">   NumRooms Alley   Price</span><br><span class="line"><span class="number">0</span>       NaN  Pave  <span class="number">127500</span></span><br><span class="line"><span class="number">1</span>       <span class="number">2.0</span>   NaN  <span class="number">106000</span></span><br><span class="line"><span class="number">2</span>       <span class="number">4.0</span>   NaN  <span class="number">178100</span></span><br><span class="line"><span class="number">3</span>       NaN   NaN  <span class="number">140000</span></span><br></pre></td></tr></table></figure>
均值插值法<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">inputs = inputs.fillna(inputs.mean())</span><br></pre></td></tr></table></figure>
独热编码<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">inputs = pd.get_dummies(inputs, dummy_na=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>
<h3 id="练习-1"><a href="#练习-1" class="headerlink" title="练习"></a>练习</h3>创建包含更多行和列的原始数据集。</li>
<li>删除缺失值最多的列。</li>
<li>将预处理后的数据集转换为张量格式。<br>首先创建数据<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">data = &#123; <span class="string">&#x27;NumRooms&#x27;</span>: [np.nan, <span class="number">2.0</span>, <span class="number">4.0</span>, np.nan, <span class="number">3.0</span>], <span class="string">&#x27;Alley&#x27;</span>: [<span class="string">&#x27;Pave&#x27;</span>, np.nan, np.nan, np.nan, <span class="string">&#x27;Grvl&#x27;</span>], <span class="string">&#x27;Price&#x27;</span>: [<span class="number">127500</span>, <span class="number">106000</span>, <span class="number">178100</span>, <span class="number">140000</span>, <span class="number">165000</span>], <span class="string">&#x27;Garage&#x27;</span>: [<span class="number">1</span>, <span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">2</span>], <span class="string">&#x27;YearBuilt&#x27;</span>: [<span class="number">2000</span>, <span class="number">1995</span>, <span class="number">2005</span>, <span class="number">2010</span>, <span class="number">2003</span>] &#125;</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">df.dropna(axis=<span class="number">1</span>, inplace=<span class="literal">True</span>) </span><br><span class="line">imputer = SimpleImputer(strategy=<span class="string">&#x27;mean&#x27;</span>) </span><br><span class="line">df_imputed = pd.DataFrame(imputer.fit_transform(df), columns=df.columns) </span><br><span class="line">tensor_data = torch.tensor(df_imputed.values) <span class="comment">#转为tensor</span></span><br></pre></td></tr></table></figure>
<h2 id="线性代数"><a href="#线性代数" class="headerlink" title="线性代数"></a>线性代数</h2></li>
</ol>
<h3 id="矩阵的转置"><a href="#矩阵的转置" class="headerlink" title="矩阵的转置"></a>矩阵的转置</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">A.T</span><br></pre></td></tr></table></figure>
<h3 id="张量基本算法"><a href="#张量基本算法" class="headerlink" title="张量基本算法"></a>张量基本算法</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">A = torch.arange(<span class="number">20</span>, dtype=torch.float32).reshape(<span class="number">5</span>, <span class="number">4</span>)</span><br><span class="line">B = A.clone()  <span class="comment"># 通过分配新内存，将A的一个副本分配给B</span></span><br><span class="line">A, A + B</span><br></pre></td></tr></table></figure>
<p>.clone()类似于深拷贝，B的反向传播不影响A。<br><em>Hadamard积</em>：数学符号⊙，指两个相同形状的矩阵（或向量）对应位置上元素相乘的结果。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">A * B</span><br></pre></td></tr></table></figure></p>
<h3 id="降维"><a href="#降维" class="headerlink" title="降维"></a>降维</h3><p>默认情况下，调用求和函数会沿所有的轴降低张量的维度，使它变为一个标量。 我们还可以指定张量沿哪一个轴来通过求和降低维度。 以矩阵为例，为了通过求和所有行的元素来降维（轴0），可以在调用函数时指定<code>axis=0</code>。 由于输入矩阵沿0轴降维以生成输出向量，因此输入轴0的维数在输出形状中消失。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">A_sum_axis0 = A.<span class="built_in">sum</span>(axis=<span class="number">0</span>)</span><br><span class="line">A_sum_axis0, A_sum_axis0.shape</span><br><span class="line"></span><br><span class="line">(tensor([<span class="number">40.</span>, <span class="number">45.</span>, <span class="number">50.</span>, <span class="number">55.</span>]), torch.Size([<span class="number">4</span>]))</span><br><span class="line"></span><br><span class="line">A_sum_axis1 = A.<span class="built_in">sum</span>(axis=<span class="number">1</span>)</span><br><span class="line">A_sum_axis1, A_sum_axis1.shape</span><br><span class="line"></span><br><span class="line">(tensor([ <span class="number">6.</span>, <span class="number">22.</span>, <span class="number">38.</span>, <span class="number">54.</span>, <span class="number">70.</span>]), torch.Size([<span class="number">5</span>]))</span><br></pre></td></tr></table></figure><br>沿着行和列对矩阵求和，等价于对矩阵的所有元素进行求和。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">A.<span class="built_in">sum</span>(axis=[<span class="number">0</span>, <span class="number">1</span>])  <span class="comment"># 结果和A.sum()相同</span></span><br></pre></td></tr></table></figure></p>
<h3 id="非降维求和"><a href="#非降维求和" class="headerlink" title="非降维求和"></a>非降维求和</h3><p>如果我们想沿某个轴计算<code>A</code>元素的累积总和， 比如<code>axis=0</code>（按行计算），可以调用<code>cumsum</code>函数。 此函数不会沿任何轴降低输入张量的维度。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">A.cumsum(axis=<span class="number">0</span>)</span><br></pre></td></tr></table></figure></p>
<h3 id="点积"><a href="#点积" class="headerlink" title="点积"></a>点积</h3><ul>
<li>点积也称为内积或数量积，是两个向量之间的运算。</li>
<li>对于两个长度相同的向量a和b，它们的点积为a·b = a₁b₁ + a₂b₂ + … + aₙbₙ，其中aᵢ和bᵢ分别表示向量a和b的第i个元素。</li>
<li>点积的结果是一个标量，表示了两个向量在同一方向上的投影的乘积。<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.dot(x, y)</span><br></pre></td></tr></table></figure>
<h3 id="矩阵乘法"><a href="#矩阵乘法" class="headerlink" title="矩阵乘法"></a>矩阵乘法</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.mm(A, B)</span><br></pre></td></tr></table></figure>
<h3 id="范数"><a href="#范数" class="headerlink" title="范数"></a>范数</h3>范数（Norm）是一个函数，通常用来衡量向量的大小或长度。范数的定义通常满足以下性质：</li>
</ul>
<ol>
<li><strong>非负性（Non-negativity）</strong>：对于任意向量x，其范数必须为非负数，即∥x∥ ≥ 0，且当且仅当x=0时，范数为0。</li>
<li><strong>齐次性（Homogeneity）</strong>：对于任意标量α，向量x的范数乘以α等于向量αx的范数，即∥αx∥ = |α| ∥x∥。</li>
<li><strong>三角不等式（Triangle Inequality）</strong>：对于任意两个向量x和y，有∥x+y∥ ≤ ∥x∥ + ∥y∥。<br>常见向量范数包括：</li>
<li><strong>L1范数</strong>：向量中所有元素的绝对值之和，表示为∥x∥₁ = |x₁| + |x₂| + … + |xₙ|。</li>
<li><strong>L2范数</strong>：向量中所有元素的平方和的平方根，表示为∥x∥₂ = √(x₁² + x₂² + … + xₙ²)。</li>
<li><strong>L∞范数</strong>：向量中所有元素的绝对值的最大值，表示为∥x∥₊ = max(|x₁|, |x₂|, …, |xₙ|)。<br>范数在机器学习和优化问题中经常被用来作为正则化项，添加到损失函数中，帮助控制模型的复杂度并避免过拟合。</li>
</ol>
<h3 id="练习-2"><a href="#练习-2" class="headerlink" title="练习"></a>练习</h3><ol>
<li>对于任意方阵A，A+AT是对称的</li>
<li>对于形状为(2, 3, 4)的张量X，len(X)的输出结果是2、</li>
<li>不是。在Python中，len()函数用于获取对象的长度或大小，对于张量来说，len(X)返回的是张量的第一个维度的大小，而不是张量特定轴的长度。、</li>
<li>考虑一个具有形状(3, 4, 5)的张量，对它在轴0、1、2上求和，输出的形状为(1, 1, 1)，即一个标量值。<h2 id="微积分"><a href="#微积分" class="headerlink" title="微积分"></a>微积分</h2><h3 id="练习-3"><a href="#练习-3" class="headerlink" title="练习"></a>练习</h3></li>
<li>如果有函数u=f(x,y,z)，其中 x=x(a,b)，y=y(a,b)，z=z(a,b)，根据链式法则，u 对 a 的导数可以表示为：<script type="math/tex; mode=display">\frac{du}{da}=\frac{∂u}{∂x}\frac{dx}{da}+\frac{∂u}{∂y}\frac{dy}{da}+\frac{∂u}{∂z}\frac{dz}{da}</script><h2 id="自动微分"><a href="#自动微分" class="headerlink" title="自动微分"></a>自动微分</h2>深度学习框架通过自动计算导数，即<em>自动微分</em>（automatic differentiation）来加快求导。 实际中，根据设计好的模型，系统会构建一个<em>计算图</em>（computational graph）， 来跟踪计算是哪些数据通过哪些操作组合起来产生输出。 自动微分使系统能够随后反向传播梯度。 这里，<em>反向传播</em>（backpropagate）意味着跟踪整个计算图，填充关于每个参数的偏导数。<br>在我们计算y关于x的梯度之前，需要一个地方来存储梯度。 重要的是，我们不会在每次对一个参数求导时都分配新的内存。 因为我们经常会成千上万次地更新相同的参数，每次都分配新的内存可能很快就会将内存耗尽。 注意，一个标量函数关于向量x的梯度是向量，并且与x具有相同的形状。<br>举例说明：y=2x*x的反向传播<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">x = np.arange(<span class="number">4.0</span>)</span><br><span class="line"><span class="comment"># 通过调用attach_grad来为一个张量的梯度分配内存</span></span><br><span class="line">x <span class="comment"># array([0., 1., 2., 3.])</span></span><br><span class="line">x.requires_grad_(<span class="literal">True</span>)  <span class="comment"># 等价于x=torch.arange(4.0,requires_grad=True)</span></span><br><span class="line">x.grad  <span class="comment"># 默认值是None</span></span><br><span class="line">y = <span class="number">2</span> * np.dot(x, x)</span><br><span class="line">y <span class="comment"># array(28.)</span></span><br><span class="line">y.backward()</span><br><span class="line">x.grad <span class="comment"># tensor([ 0.,  4.,  8., 12.]) x.grad == 4 * x</span></span><br></pre></td></tr></table></figure>
<h3 id="非标量变量的反向传播"><a href="#非标量变量的反向传播" class="headerlink" title="非标量变量的反向传播"></a>非标量变量的反向传播</h3>当<code>y</code>不是标量时，向量<code>y</code>关于向量<code>x</code>的导数的最自然解释是一个矩阵。 对于高阶和高维的<code>y</code>和<code>x</code>，求导的结果可以是一个高阶张量。</li>
</ol>
<p>然而，虽然这些更奇特的对象确实出现在高级机器学习中（包括深度学习中）， 但当调用向量的反向计算时，我们通常会试图计算一批训练样本中每个组成部分的损失函数的导数。 这里，我们的目的不是计算微分矩阵，而是单独计算批量中每个样本的偏导数之和。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 对非标量调用backward需要传入一个gradient参数，该参数指定微分函数关于self的梯度。</span></span><br><span class="line"><span class="comment"># 本例只想求偏导数的和，所以传递一个1的梯度是合适的</span></span><br><span class="line">x.grad.zero_()</span><br><span class="line">y = x * x</span><br><span class="line"><span class="comment"># 等价于y.backward(torch.ones(len(x)))</span></span><br><span class="line">y.<span class="built_in">sum</span>().backward()</span><br><span class="line">x.grad <span class="comment"># tensor([0., 2., 4., 6.])</span></span><br></pre></td></tr></table></figure></p>
<h3 id="分离计算"><a href="#分离计算" class="headerlink" title="分离计算"></a>分离计算</h3><p>有时，我们希望将某些计算移动到记录的计算图之外。 例如，假设<code>y</code>是作为<code>x</code>的函数计算的，而<code>z</code>则是作为<code>y</code>和<code>x</code>的函数计算的。 想象一下，我们想计算<code>z</code>关于<code>x</code>的梯度，但由于某种原因，希望将<code>y</code>视为一个常数， 并且只考虑到<code>x</code>在<code>y</code>被计算后发挥的作用。</p>
<p>这里可以分离<code>y</code>来返回一个新变量<code>u</code>，该变量与<code>y</code>具有相同的值， 但丢弃计算图中如何计算<code>y</code>的任何信息。 换句话说，梯度不会向后流经<code>u</code>到<code>x</code>。 因此，下面的反向传播函数计算<code>z=u*x</code>关于<code>x</code>的偏导数，同时将<code>u</code>作为常数处理， 而不是<code>z=x*x*x</code>关于<code>x</code>的偏导数。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">x.grad.zero_()</span><br><span class="line">y = x * x</span><br><span class="line">u = y.detach()</span><br><span class="line">z = u * x</span><br><span class="line"></span><br><span class="line">z.<span class="built_in">sum</span>().backward()</span><br><span class="line">x.grad == u <span class="comment"># tensor([True, True, True, True])</span></span><br></pre></td></tr></table></figure><br>由于记录了<code>y</code>的计算结果，我们可以随后在<code>y</code>上调用反向传播， 得到<code>y=x*x</code>关于的<code>x</code>的导数，即<code>2*x</code>。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">x.grad.zero_()</span><br><span class="line">y.<span class="built_in">sum</span>().backward()</span><br><span class="line">x.grad == <span class="number">2</span> * x <span class="comment"># tensor([True, True, True, True])</span></span><br></pre></td></tr></table></figure></p>
<h3 id="练习-4"><a href="#练习-4" class="headerlink" title="练习"></a>练习</h3><ol>
<li>为什么计算二阶导数比一阶导数的开销要更大？<br>计算二阶导数需要首先计算一阶导数，然后再对一阶导数进行求导；计算二阶导数需要应用链式法则多次，涉及到更多的函数组合和嵌套。</li>
<li>在运行反向传播函数之后，立即再次运行它，看看会发生什么？<br>会报错：RuntimeError: Trying to backward through the graph a second time</li>
<li>在控制流的例子中，我们计算<code>d</code>关于<code>a</code>的导数，如果将变量<code>a</code>更改为随机向量或矩阵，会发生什么？<br>如果<code>a</code>是一个随机向量或矩阵，那么计算其导数的过程会考虑到<code>a</code>的每个元素，并计算相应的雅可比矩阵。（雅可比矩阵是一个将一个向量值函数的梯度向量（或梯度向量的转置）表示为每个自变量的偏导数的矩阵）</li>
</ol>
<h2 id="查阅文档"><a href="#查阅文档" class="headerlink" title="查阅文档"></a>查阅文档</h2><h3 id="查找模块中的所有函数和类"><a href="#查找模块中的所有函数和类" class="headerlink" title="查找模块中的所有函数和类"></a>查找模块中的所有函数和类</h3><p>了知道模块中可以调用哪些函数和类，可以调用<code>dir</code>函数。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">dir</span>(torch.distributions)) </span><br><span class="line"><span class="comment"># [&#x27;AbsTransform&#x27;, &#x27;AffineTransform&#x27;, &#x27;Bernoulli&#x27;, &#x27;Beta&#x27;, &#x27;Binomial&#x27;, &#x27;CatTransform&#x27;, &#x27;Categorical&#x27;, &#x27;Cauchy&#x27;, &#x27;Chi2&#x27;, &#x27;ComposeTransform&#x27;, &#x27;ContinuousBernoulli&#x27;, &#x27;CorrCholeskyTransform&#x27;, ....</span></span><br></pre></td></tr></table></figure><br>通常可以忽略以“<code>__</code>”（双下划线）开始和结束的函数，它们是Python中的特殊对象， 或以单个“<code>_</code>”（单下划线）开始的函数，它们通常是内部函数。</p>
<h3 id="查找特定函数和类的用法"><a href="#查找特定函数和类的用法" class="headerlink" title="查找特定函数和类的用法"></a>查找特定函数和类的用法</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">help</span>(torch.ones)</span><br></pre></td></tr></table></figure>
<h1 id="线性神经网络"><a href="#线性神经网络" class="headerlink" title="线性神经网络"></a>线性神经网络</h1><h2 id="线性回归"><a href="#线性回归" class="headerlink" title="线性回归"></a>线性回归</h2><p><em>回归</em>（regression）是能为一个或多个自变量与因变量之间关系建模的一类方法。 在自然科学和社会科学领域，回归经常用来表示输入和输出之间的关系。</p>
<h3 id="线性模型"><a href="#线性模型" class="headerlink" title="线性模型"></a>线性模型</h3><script type="math/tex; mode=display">\hat{y}=w^Tx+b</script><p>权重与偏置</p>
<h3 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h3><p><em>损失函数</em>（loss function）能够量化目标的实际值与预测值之间的差距。 通常我们会选择非负数作为损失，且数值越小表示损失越小，完美预测时的损失为0。 回归问题中最常用的损失函数是平方误差函数。</p>
<h2 id="从零实现线性回归"><a href="#从零实现线性回归" class="headerlink" title="从零实现线性回归"></a>从零实现线性回归</h2><h3 id="练习-5"><a href="#练习-5" class="headerlink" title="练习"></a>练习</h3><ol>
<li>如果我们将权重初始化为零，会发生什么。算法仍然有效吗？<br>可能会影响网络训练速度，收敛性和最终性能。</li>
</ol>
<ul>
<li><strong>随机初始化</strong>：将权重初始化为随机值是一种常用的做法。通过随机初始化，可以避免权重对称性，从而使每个神经元学习到不同的特征。常见的做法是从某个分布（如均匀分布或正态分布）中随机采样权重值。</li>
<li><strong>全0初始化</strong>：将权重初始化为全0是一种简单的初始化方法。然而，如果所有权重都初始化为0，那么每个神经元在前向传播时计算的值将是相同的，这会导致每个神经元学习相同的特征。这种情况下，网络无法有效地学习复杂的特征，训练过程可能会出现问题。</li>
</ul>
<ol>
<li>假设试图为电压和电流的关系建立一个模型。自动微分可以用来学习模型的参数吗?<br>V=R×I<br>R 是电阻。我们希望通过给定的电压和电流的数据样本，学习到电阻 R 的值。<br>我们可以将电阻 R 视为模型的参数，然后定义一个损失函数来衡量模型预测的电压与实际观测值之间的差异。</li>
<li>能基于<a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Planck%27s_law">普朗克定律</a>使用光谱能量密度来确定物体的温度吗？可以</li>
<li>计算二阶导数时可能会遇到什么问题？这些问题可以如何解决？<br>计算量大，计算复杂度高，由于计算机表示的精读限制，可能出现数值不稳定问题<br>符号计算：对于简单的函数和表达式，可以使用符号计算来精确地计算二阶导数，避免数值计算的误差。在进行数值计算时，可以采用数值稳定的算法和技巧，例如使用高精度算法或避免数值不稳定的操作。</li>
<li>为什么在<code>squared_loss</code>（均方损失）函数中需要使用<code>reshape</code>函数？<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">squared_loss</span>(<span class="params">y_hat, y</span>):  <span class="comment">#@save</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;均方损失&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">return</span> (y_hat - y.reshape(y_hat.shape)) ** <span class="number">2</span> / <span class="number">2</span></span><br></pre></td></tr></table></figure>
我们需要将真实值<code>y</code>的形状转换为和预测值<code>y_hat</code>的形状相同</li>
<li>尝试使用不同的学习率，观察损失函数值下降的快慢</li>
<li>如果样本个数不能被批量大小整除，<code>data_iter</code>函数的行为会有什么变化？<br>最后一个批次的大小会小于设定的批量大小。</li>
</ol>
<h2 id="线性回归的简洁实现"><a href="#线性回归的简洁实现" class="headerlink" title="线性回归的简洁实现"></a>线性回归的简洁实现</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch.utils <span class="keyword">import</span> data</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br><span class="line"></span><br><span class="line">true_w = torch.tensor([<span class="number">2</span>, -<span class="number">3.4</span>])</span><br><span class="line">true_b = <span class="number">4.2</span></span><br><span class="line">features, labels = d2l.synthetic_data(true_w, true_b, <span class="number">1000</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">load_array</span>(<span class="params">data_arrays, batch_size, is_train=<span class="literal">True</span></span>):  <span class="comment">#@save</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;构造一个PyTorch数据迭代器&quot;&quot;&quot;</span></span><br><span class="line">    dataset = data.TensorDataset(*data_arrays)</span><br><span class="line">    <span class="keyword">return</span> data.DataLoader(dataset, batch_size, shuffle=is_train)</span><br><span class="line"></span><br><span class="line">batch_size = <span class="number">10</span></span><br><span class="line">data_iter = load_array((features, labels), batch_size)</span><br><span class="line"><span class="built_in">next</span>(<span class="built_in">iter</span>(data_iter))</span><br></pre></td></tr></table></figure>
<h3 id="定义模型"><a href="#定义模型" class="headerlink" title="定义模型"></a>定义模型</h3><p>正如我们在构造<code>nn.Linear</code>时指定输入和输出尺寸一样， 现在我们能直接访问参数以设定它们的初始值。 我们通过<code>net[0]</code>选择网络中的第一个图层， 然后使用<code>weight.data</code>和<code>bias.data</code>方法访问参数。 我们还可以使用替换方法<code>normal_</code>和<code>fill_</code>来重写参数值。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line">net = nn.Sequential(nn.Linear(<span class="number">2</span>, <span class="number">1</span>))</span><br><span class="line">net[<span class="number">0</span>].weight.data.normal_(<span class="number">0</span>, <span class="number">0.01</span>)</span><br><span class="line">net[<span class="number">0</span>].bias.data.fill_(<span class="number">0</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure></p>
<h3 id="定义损失函数与优化器"><a href="#定义损失函数与优化器" class="headerlink" title="定义损失函数与优化器"></a>定义损失函数与优化器</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">loss = nn.MSELoss()</span><br><span class="line">trainer = torch.optim.SGD(net.parameters(), lr=<span class="number">0.03</span>)</span><br></pre></td></tr></table></figure>
<h3 id="训练"><a href="#训练" class="headerlink" title="训练"></a>训练</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">num_epochs = <span class="number">3</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epochs):</span><br><span class="line">    <span class="keyword">for</span> X, y <span class="keyword">in</span> data_iter:</span><br><span class="line">        l = loss(net(X) ,y)</span><br><span class="line">        trainer.zero_grad()</span><br><span class="line">        l.backward()</span><br><span class="line">        trainer.step()</span><br><span class="line">    l = loss(net(features), labels)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;epoch <span class="subst">&#123;epoch + <span class="number">1</span>&#125;</span>, loss <span class="subst">&#123;l:f&#125;</span>&#x27;</span>)</span><br><span class="line"><span class="comment"># epoch 1, loss 0.000248</span></span><br><span class="line"><span class="comment"># epoch 2, loss 0.000103</span></span><br><span class="line"><span class="comment"># epoch 3, loss 0.000103 </span></span><br></pre></td></tr></table></figure>
<h3 id="练习-6"><a href="#练习-6" class="headerlink" title="练习"></a>练习</h3><ol>
<li>如果将小批量的总损失替换为小批量损失的平均值，需要如何更改学习率？<br>如果将小批量的总损失替换为小批量损失的平均值，通常不需要更改学习率。这是因为损失的平均值通常与损失的总和成比例，只是在数值上有所不同。因此，如果将损失的总和除以批量大小得到平均损失，学习率可以保持不变。</li>
<li>查看深度学习框架文档，它们提供了哪些损失函数和初始化方法？用Huber损失代替原损失。</li>
</ol>
<h1 id="多层感知机"><a href="#多层感知机" class="headerlink" title="多层感知机"></a>多层感知机</h1><h2 id="多层感知机-1"><a href="#多层感知机-1" class="headerlink" title="多层感知机"></a>多层感知机</h2><h3 id="激活函数"><a href="#激活函数" class="headerlink" title="激活函数"></a>激活函数</h3><p>RELU</p>
<script type="math/tex; mode=display">ReLU(x)=max(x,0)</script><p>sigmoid函数</p>
<script type="math/tex; mode=display">sigmoid(x)=\frac{1}{1+exp(-x)}</script><p>tanh函数</p>
<script type="math/tex; mode=display">tanh(x)=\frac{1-exp(-2x)}{1+exp(-2x)}</script><h3 id="练习-7"><a href="#练习-7" class="headerlink" title="练习"></a>练习</h3><ol>
<li>计算pReLU激活函数的导数。<script type="math/tex; mode=display">f(x)=\begin{cases}
x & \text{ if } x>0 \\
\alpha x  & \text{ otherwise } 
\end{cases}</script>导数为<script type="math/tex; mode=display">{f}'(x)=\begin{cases}
1 & \text{ if } x>0 \\
\alpha   & \text{ otherwise } 
\end{cases}</script></li>
<li>假设我们有一个非线性单元，将它一次应用于一个小批量的数据。这会导致什么样的问题？</li>
</ol>
<ul>
<li><strong>过拟合</strong>：在小批量上应用非线性单元可能导致模型在训练集上过拟合，因为模型可能会过度记住小批量的特定模式或噪声。</li>
<li><strong>收敛速度</strong>：在小批量上应用非线性单元可能会影响模型的收敛速度，因为模型可能需要更多的迭代才能收敛到最优解。<h2 id="多层感知机简洁实现"><a href="#多层感知机简洁实现" class="headerlink" title="多层感知机简洁实现"></a>多层感知机简洁实现</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br><span class="line">net = nn.Sequential(nn.Flatten(),</span><br><span class="line">                    nn.Linear(<span class="number">784</span>, <span class="number">256</span>),</span><br><span class="line">                    nn.ReLU(),</span><br><span class="line">                    nn.Linear(<span class="number">256</span>, <span class="number">10</span>))</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">init_weights</span>(<span class="params">m</span>):</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">type</span>(m) == nn.Linear:</span><br><span class="line">        nn.init.normal_(m.weight, std=<span class="number">0.01</span>)</span><br><span class="line"></span><br><span class="line">net.apply(init_weights)</span><br><span class="line">batch_size, lr, num_epochs = <span class="number">256</span>, <span class="number">0.1</span>, <span class="number">10</span></span><br><span class="line">loss = nn.CrossEntropyLoss(reduction=<span class="string">&#x27;none&#x27;</span>)</span><br><span class="line">trainer = torch.optim.SGD(net.parameters(), lr=lr)</span><br><span class="line"></span><br><span class="line">train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)</span><br><span class="line">d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, trainer)</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h3 id="练习-8"><a href="#练习-8" class="headerlink" title="练习"></a>练习</h3><ol>
<li>尝试添加不同数量的隐藏层，修改学习率。</li>
<li>尝试不同的激活函数，哪个效果最好？<br>MSEloss</li>
<li>尝试不同的方案来初始化权重，什么方法效果最好？<br>正态分布初始化</li>
</ol>
<h2 id="模型选择，欠拟合，过拟合"><a href="#模型选择，欠拟合，过拟合" class="headerlink" title="模型选择，欠拟合，过拟合"></a>模型选择，欠拟合，过拟合</h2><p>将模型在训练数据上拟合的比在潜在分布中更接近的现象称为<em>过拟合（overfitting）， 用于对抗过拟合的技术称为正则化（regularization）。<br>在训练参数化机器学习模型时， </em>权重衰减<em>（weight decay）是最广泛使用的正则化的技术之一， 它通常也被称为L2</em>正则化_。 这项技术通过函数与零的距离来衡量函数的复杂度， 因为在所有函数f中，函数f=0（所有输入都得到值0） 在某种意义上是最简单的。这个正则化项通常是权重的平方和或绝对值和，以惩罚模型的复杂度。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">train_concise</span>(<span class="params">wd</span>):</span><br><span class="line">    net = nn.Sequential(nn.Linear(num_inputs, <span class="number">1</span>))</span><br><span class="line">    <span class="keyword">for</span> param <span class="keyword">in</span> net.parameters():</span><br><span class="line">        param.data.normal_()</span><br><span class="line">    loss = nn.MSELoss(reduction=<span class="string">&#x27;none&#x27;</span>)</span><br><span class="line">    num_epochs, lr = <span class="number">100</span>, <span class="number">0.003</span></span><br><span class="line">    <span class="comment"># 偏置参数没有衰减</span></span><br><span class="line">    trainer = torch.optim.SGD([</span><br><span class="line">        &#123;<span class="string">&quot;params&quot;</span>:net[<span class="number">0</span>].weight,<span class="string">&#x27;weight_decay&#x27;</span>: wd&#125;,</span><br><span class="line">        &#123;<span class="string">&quot;params&quot;</span>:net[<span class="number">0</span>].bias&#125;], lr=lr)</span><br><span class="line">    animator = d2l.Animator(xlabel=<span class="string">&#x27;epochs&#x27;</span>, ylabel=<span class="string">&#x27;loss&#x27;</span>, yscale=<span class="string">&#x27;log&#x27;</span>,xlim=[<span class="number">5</span>, num_epochs], legend=[<span class="string">&#x27;train&#x27;</span>, <span class="string">&#x27;test&#x27;</span>])</span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epochs):</span><br><span class="line">        <span class="keyword">for</span> X, y <span class="keyword">in</span> train_iter:</span><br><span class="line">            trainer.zero_grad()</span><br><span class="line">            l = loss(net(X), y)</span><br><span class="line">            l.mean().backward()</span><br><span class="line">            trainer.step()</span><br><span class="line">        <span class="keyword">if</span> (epoch + <span class="number">1</span>) % <span class="number">5</span> == <span class="number">0</span>:</span><br><span class="line">            animator.add(epoch + <span class="number">1</span>,</span><br><span class="line">            (d2l.evaluate_loss(net, train_iter, loss),</span><br><span class="line">            d2l.evaluate_loss(net, test_iter, loss)))</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;w的L2范数：&#x27;</span>, net[<span class="number">0</span>].weight.norm().item())</span><br></pre></td></tr></table></figure>
<h3 id="练习-9"><a href="#练习-9" class="headerlink" title="练习"></a>练习</h3><ol>
<li>在本节的估计问题中使用$\lambda$的值进行实验。绘制训练和测试精度关于$\lambda$的函数。观察到了什么？<br>λ 太大后，train和test的loss会变得很大，太小后，train的loss会低，但是test的loss会很高。</li>
<li>使用验证集来找到最佳值λ。它真的是最优值吗？这有关系吗？<br>它是对于该验证集的最优值，但不是全局最优值，会随着训练集与验证集而变化。</li>
<li>回顾训练误差和泛化误差之间的关系。除了权重衰减、增加训练数据、使用适当复杂度的模型之外，还能想出其他什么方法来处理过拟合？<br>Dropout，数据增强，模型集成</li>
</ol>
<h2 id="暂退法（Dropout）"><a href="#暂退法（Dropout）" class="headerlink" title="暂退法（Dropout）"></a>暂退法（Dropout）</h2><p>暂退法在前向传播过程中，计算每一内部层的同时注入噪声，这已经成为训练神经网络的常用技术。 这种方法之所以被称为<em>暂退法</em>，因为我们从表面上看是在训练过程中丢弃（drop out）一些神经元。 在整个训练过程的每一次迭代中，标准暂退法包括在计算下一层之前将当前层中的一些节点置零。</p>
<h3 id="从零开始实现"><a href="#从零开始实现" class="headerlink" title="从零开始实现"></a>从零开始实现</h3><p>要实现单层的暂退法函数， 我们从均匀分布U[0,1]中抽取样本，样本数与这层神经网络的维度一致。 然后我们保留那些对应样本大于p的节点，把剩下的丢弃。</p>
<p>在下面的代码中，我们实现 <code>dropout_layer</code> 函数， 该函数以<code>dropout</code>的概率丢弃张量输入<code>X</code>中的元素， 如上所述重新缩放剩余部分：将剩余部分除以<code>1.0-dropout</code>。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">dropout_layer</span>(<span class="params">X, dropout</span>):</span><br><span class="line">    <span class="keyword">assert</span> <span class="number">0</span> &lt;= dropout &lt;= <span class="number">1</span></span><br><span class="line">    <span class="comment"># 在本情况中，所有元素都被丢弃</span></span><br><span class="line">    <span class="keyword">if</span> dropout == <span class="number">1</span>:</span><br><span class="line">        <span class="keyword">return</span> torch.zeros_like(X)</span><br><span class="line">    <span class="comment"># 在本情况中，所有元素都被保留</span></span><br><span class="line">    <span class="keyword">if</span> dropout == <span class="number">0</span>:</span><br><span class="line">        <span class="keyword">return</span> X</span><br><span class="line">    mask = (torch.rand(X.shape) &gt; dropout).<span class="built_in">float</span>()</span><br><span class="line">    <span class="keyword">return</span> mask * X / (<span class="number">1.0</span> - dropout)</span><br></pre></td></tr></table></figure><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">X= torch.arange(<span class="number">16</span>, dtype = torch.float32).reshape((<span class="number">2</span>, <span class="number">8</span>))</span><br><span class="line"><span class="built_in">print</span>(X)</span><br><span class="line"><span class="built_in">print</span>(dropout_layer(X, <span class="number">0.</span>))</span><br><span class="line"><span class="built_in">print</span>(dropout_layer(X, <span class="number">0.5</span>))</span><br><span class="line"><span class="built_in">print</span>(dropout_layer(X, <span class="number">1.</span>))</span><br><span class="line"><span class="comment">#tensor([[ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.],</span></span><br><span class="line"><span class="comment">#        [ 8.,  9., 10., 11., 12., 13., 14., 15.]])</span></span><br><span class="line"><span class="comment">#tensor([[ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.],</span></span><br><span class="line"><span class="comment">#        [ 8.,  9., 10., 11., 12., 13., 14., 15.]])</span></span><br><span class="line"><span class="comment">#tensor([[ 0.,  2.,  0.,  6.,  0.,  0.,  0., 14.],</span></span><br><span class="line"><span class="comment">#       [16., 18.,  0., 22.,  0., 26., 28., 30.]])</span></span><br><span class="line"><span class="comment">#tensor([[0., 0., 0., 0., 0., 0., 0., 0.],</span></span><br><span class="line"><span class="comment">#        [0., 0., 0., 0., 0., 0., 0., 0.]])</span></span><br></pre></td></tr></table></figure></p>
<h3 id="定义模型-1"><a href="#定义模型-1" class="headerlink" title="定义模型"></a>定义模型</h3><p>我们可以将暂退法应用于每个隐藏层的输出（在激活函数之后）， 并且可以为每一层分别设置暂退概率： 常见的技巧是在靠近输入层的地方设置较低的暂退概率。 下面的模型将第一个和第二个隐藏层的暂退概率分别设置为0.2和0.5， 并且暂退法只在训练期间有效。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">dropout1, dropout2 = <span class="number">0.2</span>, <span class="number">0.5</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Net</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, num_inputs, num_outputs, num_hiddens1, num_hiddens2,</span></span><br><span class="line"><span class="params">                 is_training = <span class="literal">True</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(Net, self).__init__()</span><br><span class="line">        self.num_inputs = num_inputs</span><br><span class="line">        self.training = is_training</span><br><span class="line">        self.lin1 = nn.Linear(num_inputs, num_hiddens1)</span><br><span class="line">        self.lin2 = nn.Linear(num_hiddens1, num_hiddens2)</span><br><span class="line">        self.lin3 = nn.Linear(num_hiddens2, num_outputs)</span><br><span class="line">        self.relu = nn.ReLU()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, X</span>):</span><br><span class="line">        H1 = self.relu(self.lin1(X.reshape((-<span class="number">1</span>, self.num_inputs))))</span><br><span class="line">        <span class="comment"># 只有在训练模型时才使用dropout</span></span><br><span class="line">        <span class="keyword">if</span> self.training == <span class="literal">True</span>:</span><br><span class="line">            <span class="comment"># 在第一个全连接层之后添加一个dropout层</span></span><br><span class="line">            H1 = dropout_layer(H1, dropout1)</span><br><span class="line">        H2 = self.relu(self.lin2(H1))</span><br><span class="line">        <span class="keyword">if</span> self.training == <span class="literal">True</span>:</span><br><span class="line">            <span class="comment"># 在第二个全连接层之后添加一个dropout层</span></span><br><span class="line">            H2 = dropout_layer(H2, dropout2)</span><br><span class="line">        out = self.lin3(H2)</span><br><span class="line">        <span class="keyword">return</span> out</span><br><span class="line"></span><br><span class="line">net = Net(num_inputs, num_outputs, num_hiddens1, num_hiddens2)</span><br></pre></td></tr></table></figure></p>
<h3 id="简介实现"><a href="#简介实现" class="headerlink" title="简介实现"></a>简介实现</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">net = nn.Sequential(nn.Flatten(),</span><br><span class="line">        nn.Linear(<span class="number">784</span>, <span class="number">256</span>),</span><br><span class="line">        nn.ReLU(),</span><br><span class="line">        <span class="comment"># 在第一个全连接层之后添加一个dropout层</span></span><br><span class="line">        nn.Dropout(dropout1),</span><br><span class="line">        nn.Linear(<span class="number">256</span>, <span class="number">256</span>),</span><br><span class="line">        nn.ReLU(),</span><br><span class="line">        <span class="comment"># 在第二个全连接层之后添加一个dropout层</span></span><br><span class="line">        nn.Dropout(dropout2),</span><br><span class="line">        nn.Linear(<span class="number">256</span>, <span class="number">10</span>))</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">init_weights</span>(<span class="params">m</span>):</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">type</span>(m) == nn.Linear:</span><br><span class="line">        nn.init.normal_(m.weight, std=<span class="number">0.01</span>)</span><br><span class="line"></span><br><span class="line">net.apply(init_weights)</span><br><span class="line">trainer = torch.optim.SGD(net.parameters(), lr=lr)</span><br><span class="line">d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, trainer)</span><br></pre></td></tr></table></figure>
<h3 id="练习-10"><a href="#练习-10" class="headerlink" title="练习"></a>练习</h3><ol>
<li>如果更改第一层和第二层的暂退法概率，会发生什么情况？具体地说，如果交换这两个层，会发生什么情况？设计一个实验来回答这些问题，定量描述该结果，并总结定性的结论。<br>在调换第一和第二层概率后，结果基本没有什么变化，但是改变第一层和第二层的概率总和后，结果会有明显的变化。</li>
<li>增加训练轮数，并将使用暂退法和不使用暂退法时获得的结果进行比较。<br>没有dropout的训练效果会更好，但是泛化性不够好。</li>
<li>为什么在测试时通常不使用暂退法？<br>为了保持模型的一致性和稳定性，在测试时，我们希望模型能够产生确定性的预测结果，而不是随机性的输出。</li>
<li>如果我们将暂退法应用到权重矩阵的各个权重，而不是激活值，会发生什么？<br>train_loss会下降的比较慢。</li>
</ol>
<h2 id="数值稳定性与模型初始化"><a href="#数值稳定性与模型初始化" class="headerlink" title="数值稳定性与模型初始化"></a>数值稳定性与模型初始化</h2><h3 id="梯度消失和梯度爆炸"><a href="#梯度消失和梯度爆炸" class="headerlink" title="梯度消失和梯度爆炸"></a>梯度消失和梯度爆炸</h3><p><strong>梯度消失</strong>：当网络的层数较多时，梯度在反向传播过程中可能会变得非常小，甚至接近于零。这样，深层网络中较早层的参数将无法得到有效更新，导致模型无法学习到有效的特征表示，从而影响模型的性能。<br><strong>梯度爆炸</strong>：与梯度消失相反，梯度爆炸指的是在反向传播过程中，梯度变得非常大，甚至超出了计算机能够表示的范围。这样会导致参数更新过大，模型参数发散，无法收敛到有效的解决方案。<br>为了解决梯度消失和梯度爆炸问题，可以采取以下几种方法：</p>
<ol>
<li><p><strong>权重初始化</strong>：合适的权重初始化可以帮助减少梯度消失和爆炸的可能性。例如，使用较小的随机数来初始化权重。</p>
</li>
<li><p><strong>梯度裁剪</strong>：在反向传播过程中，如果梯度的范数超过了设定的阈值，可以对梯度进行裁剪，将其限制在一个合理的范围内，防止梯度爆炸。</p>
</li>
<li><p><strong>使用激活函数</strong>：合适的激活函数可以帮助缓解梯度消失和梯度爆炸问题。例如，ReLU等激活函数在一定程度上可以防止梯度消失。</p>
</li>
<li><p><strong>Batch Normalization</strong>：批量归一化可以减少内部协变量漂移，并有助于稳定训练过程，从而减少梯度消失和爆炸的可能性。</p>
</li>
<li><p><strong>使用更深层次的结构</strong>：使用一些技术，如残差连接（Residual Connections），可以帮助信息在网络中更好地传播，减少梯度消失的影响。</p>
<h1 id="深度学习计算"><a href="#深度学习计算" class="headerlink" title="深度学习计算"></a>深度学习计算</h1><h2 id="层和块"><a href="#层和块" class="headerlink" title="层和块"></a>层和块</h2><p><em>块</em>（block）可以描述单个层、由多个层组成的组件或整个模型本身。从编程的角度来看，块由类（class）表示。 它的任何子类都必须定义一个将其输入转换为输出的前向传播函数， 并且必须存储任何必需的参数。</p>
<h3 id="自定义块"><a href="#自定义块" class="headerlink" title="自定义块"></a>自定义块</h3><p>一个多层感知器块：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">MLP</span>(nn.Module):</span><br><span class="line">    <span class="comment"># 用模型参数声明层。这里，我们声明两个全连接的层</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="comment"># 调用MLP的父类Module的构造函数来执行必要的初始化。</span></span><br><span class="line">        <span class="comment"># 这样，在类实例化时也可以指定其他函数参数，例如模型参数params（稍后将介绍）</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.hidden = nn.Linear(<span class="number">20</span>, <span class="number">256</span>)  <span class="comment"># 隐藏层</span></span><br><span class="line">        self.out = nn.Linear(<span class="number">256</span>, <span class="number">10</span>)  <span class="comment"># 输出层</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 定义模型的前向传播，即如何根据输入X返回所需的模型输出</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, X</span>):</span><br><span class="line">        <span class="comment"># 注意，这里我们使用ReLU的函数版本，其在nn.functional模块中定义。</span></span><br><span class="line">        <span class="keyword">return</span> self.out(F.relu(self.hidden(X)))</span><br></pre></td></tr></table></figure>
<p>使用：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">net = MLP()</span><br><span class="line">net(X)</span><br></pre></td></tr></table></figure>
<h3 id="顺序块"><a href="#顺序块" class="headerlink" title="顺序块"></a>顺序块</h3><p>自己定义一个sequential类</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">MySequential</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, *args</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="keyword">for</span> idx, module <span class="keyword">in</span> <span class="built_in">enumerate</span>(args):</span><br><span class="line">            <span class="comment"># 这里，module是Module子类的一个实例。我们把它保存在&#x27;Module&#x27;类的成员</span></span><br><span class="line">            <span class="comment"># 变量_modules中。_module的类型是OrderedDict</span></span><br><span class="line">            self._modules[<span class="built_in">str</span>(idx)] = module</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, X</span>):</span><br><span class="line">        <span class="comment"># OrderedDict保证了按照成员添加的顺序遍历它们</span></span><br><span class="line">        <span class="keyword">for</span> block <span class="keyword">in</span> self._modules.values():</span><br><span class="line">            X = block(X)</span><br><span class="line">        <span class="keyword">return</span> X</span><br></pre></td></tr></table></figure>
<p>使用方法：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">net = MySequential(nn.Linear(<span class="number">20</span>, <span class="number">256</span>), nn.ReLU(), nn.Linear(<span class="number">256</span>, <span class="number">10</span>))</span><br><span class="line">net(X)</span><br></pre></td></tr></table></figure>
<p>我们可以混合搭配各种组合块的方法：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">NestMLP</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.net = nn.Sequential(nn.Linear(<span class="number">20</span>, <span class="number">64</span>), nn.ReLU(),</span><br><span class="line">                                 nn.Linear(<span class="number">64</span>, <span class="number">32</span>), nn.ReLU())</span><br><span class="line">        self.linear = nn.Linear(<span class="number">32</span>, <span class="number">16</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, X</span>):</span><br><span class="line">        <span class="keyword">return</span> self.linear(self.net(X))</span><br><span class="line"></span><br><span class="line">chimera = nn.Sequential(NestMLP(), nn.Linear(<span class="number">16</span>, <span class="number">20</span>), FixedHiddenMLP())</span><br><span class="line">chimera(X)</span><br></pre></td></tr></table></figure>
<h3 id="练习-11"><a href="#练习-11" class="headerlink" title="练习"></a>练习</h3></li>
<li>如果将<code>MySequential</code>中存储块的方式更改为Python列表，会出现什么样的问题？<br>print(net) 会报错，不能更清晰的看到网络结构。</li>
<li>实现一个块，它以两个块为参数，例如<code>net1</code>和<code>net2</code>，并返回前向传播中两个网络的串联输出。这也被称为平行块。<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">paarallelblock</span>(nn.Module):</span><br><span class="line">	<span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self,net1,net2</span>):</span><br><span class="line">		<span class="built_in">super</span>(parallelblock,self).__init__()</span><br><span class="line">		self.net1 = net1</span><br><span class="line">		self.net2 = net2</span><br><span class="line">	<span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self,x</span>):</span><br><span class="line">		out1 = self.net1(x)</span><br><span class="line">		out2 = self.net2(x)</span><br><span class="line">		<span class="keyword">return</span> torch.cat((out1,out2),dim = <span class="number">1</span>)</span><br><span class="line">	</span><br></pre></td></tr></table></figure></li>
<li>假设我们想要连接同一网络的多个实例。实现一个函数，该函数生成同一个块的多个实例，并在此基础上构建更大的网络。<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">block1</span>():</span><br><span class="line">    <span class="keyword">return</span> nn.Sequential(nn.Linear(<span class="number">4</span>, <span class="number">8</span>), nn.ReLU(),</span><br><span class="line">                         nn.Linear(<span class="number">8</span>, <span class="number">4</span>), nn.ReLU())</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">block2</span>():</span><br><span class="line">    net = nn.Sequential()</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">4</span>):</span><br><span class="line">        <span class="comment"># 在这里嵌套</span></span><br><span class="line">        net.add_module(<span class="string">f&#x27;block <span class="subst">&#123;i&#125;</span>&#x27;</span>, block1())</span><br><span class="line">    <span class="keyword">return</span> net</span><br><span class="line"></span><br><span class="line">rgnet = nn.Sequential(block2(), nn.Linear(<span class="number">4</span>, <span class="number">1</span>))</span><br><span class="line">rgnet(X)</span><br></pre></td></tr></table></figure>
<h2 id="参数管理"><a href="#参数管理" class="headerlink" title="参数管理"></a>参数管理</h2><h3 id="参数访问"><a href="#参数访问" class="headerlink" title="参数访问"></a>参数访问</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"></span><br><span class="line">net = nn.Sequential(nn.Linear(<span class="number">4</span>, <span class="number">8</span>), nn.ReLU(), nn.Linear(<span class="number">8</span>, <span class="number">1</span>))</span><br><span class="line">X = torch.rand(size=(<span class="number">2</span>, <span class="number">4</span>))</span><br><span class="line">net(X)</span><br><span class="line"><span class="built_in">print</span>(net[<span class="number">2</span>].state_dict())</span><br><span class="line"><span class="comment"># OrderedDict([(&#x27;weight&#x27;, tensor([[-0.0427, -0.2939, -0.1894,  0.0220, -0.1709, -0.1522, -0.0334, -0.2263]])), (&#x27;bias&#x27;, tensor([0.0887]))])</span></span><br></pre></td></tr></table></figure>
每个参数都表示为参数类的一个实例。 要对参数执行任何操作，首先我们需要访问底层的数值。 有几种方法可以做到这一点。有些比较简单，而另一些则比较通用。 下面的代码从第二个全连接层（即第三个神经网络层）提取偏置， 提取后返回的是一个参数类实例，并进一步访问该参数的值。<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(<span class="built_in">type</span>(net[<span class="number">2</span>].bias))</span><br><span class="line"><span class="built_in">print</span>(net[<span class="number">2</span>].bias)</span><br><span class="line"><span class="built_in">print</span>(net[<span class="number">2</span>].bias.data)</span><br><span class="line"><span class="comment"># &lt;class &#x27;torch.nn.parameter.Parameter&#x27;&gt;</span></span><br><span class="line"><span class="comment"># Parameter containing:</span></span><br><span class="line"><span class="comment"># tensor([0.0887], requires_grad=True)</span></span><br><span class="line"><span class="comment"># tensor([0.0887])</span></span><br></pre></td></tr></table></figure>
<h3 id="一次性访问所有参数"><a href="#一次性访问所有参数" class="headerlink" title="一次性访问所有参数"></a>一次性访问所有参数</h3>递归整个树来提取每个子块的参数：<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(*[(name, param.shape) <span class="keyword">for</span> name, param <span class="keyword">in</span> net[<span class="number">0</span>].named_parameters()])</span><br><span class="line"><span class="comment"># (&#x27;weight&#x27;, torch.Size([8, 4])) (&#x27;bias&#x27;, torch.Size([8]))</span></span><br><span class="line"><span class="built_in">print</span>(*[(name, param.shape) <span class="keyword">for</span> name, param <span class="keyword">in</span> net.named_parameters()])</span><br><span class="line"></span><br></pre></td></tr></table></figure>
这为我们提供了另一种访问网络参数的方式，如下所示。<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">net.state_dict()[<span class="string">&#x27;2.bias&#x27;</span>].data</span><br><span class="line"><span class="comment"># tensor([0.0887])</span></span><br></pre></td></tr></table></figure>
观察网络是如何工作的：<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">block1</span>():</span><br><span class="line">    <span class="keyword">return</span> nn.Sequential(nn.Linear(<span class="number">4</span>, <span class="number">8</span>), nn.ReLU(),</span><br><span class="line">                         nn.Linear(<span class="number">8</span>, <span class="number">4</span>), nn.ReLU())</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">block2</span>():</span><br><span class="line">    net = nn.Sequential()</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">4</span>):</span><br><span class="line">        <span class="comment"># 在这里嵌套</span></span><br><span class="line">        net.add_module(<span class="string">f&#x27;block <span class="subst">&#123;i&#125;</span>&#x27;</span>, block1())</span><br><span class="line">    <span class="keyword">return</span> net</span><br><span class="line"></span><br><span class="line">rgnet = nn.Sequential(block2(), nn.Linear(<span class="number">4</span>, <span class="number">1</span>))</span><br><span class="line">rgnet(X)</span><br><span class="line"><span class="built_in">print</span>(regnet)</span><br><span class="line"><span class="comment"># Sequential(</span></span><br><span class="line">  <span class="comment">#(0): Sequential(</span></span><br><span class="line">    <span class="comment">#(block 0): Sequential(</span></span><br><span class="line">      <span class="comment">#(0): Linear(in_features=4, out_features=8, bias=True)</span></span><br><span class="line">      <span class="comment">#(1): ReLU()</span></span><br><span class="line">      <span class="comment">#(2): Linear(in_features=8, out_features=4, bias=True)</span></span><br><span class="line">      <span class="comment">#(3): ReLU()</span></span><br><span class="line">    <span class="comment">#)</span></span><br><span class="line">    <span class="comment">#(block 1): Sequential(</span></span><br><span class="line">      <span class="comment">#(0): Linear(...</span></span><br></pre></td></tr></table></figure>
<h3 id="参数绑定"><a href="#参数绑定" class="headerlink" title="参数绑定"></a>参数绑定</h3>我们希望在多个层间共享参数，可以定义一个稠密层，使用它的参数来设置另一个层的参数：<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 我们需要给共享层一个名称，以便可以引用它的参数</span></span><br><span class="line">shared = nn.Linear(<span class="number">8</span>, <span class="number">8</span>)</span><br><span class="line">net = nn.Sequential(nn.Linear(<span class="number">4</span>, <span class="number">8</span>), nn.ReLU(),</span><br><span class="line">                    shared, nn.ReLU(),</span><br><span class="line">                    shared, nn.ReLU(),</span><br><span class="line">                    nn.Linear(<span class="number">8</span>, <span class="number">1</span>))</span><br><span class="line">net(X)</span><br><span class="line"><span class="comment"># 检查参数是否相同</span></span><br><span class="line"><span class="built_in">print</span>(net[<span class="number">2</span>].weight.data[<span class="number">0</span>] == net[<span class="number">4</span>].weight.data[<span class="number">0</span>])</span><br><span class="line">net[<span class="number">2</span>].weight.data[<span class="number">0</span>, <span class="number">0</span>] = <span class="number">100</span></span><br><span class="line"><span class="comment"># 确保它们实际上是同一个对象，而不只是有相同的值</span></span><br><span class="line"><span class="built_in">print</span>(net[<span class="number">2</span>].weight.data[<span class="number">0</span>] == net[<span class="number">4</span>].weight.data[<span class="number">0</span>])</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tensor([<span class="literal">True</span>, <span class="literal">True</span>, <span class="literal">True</span>, <span class="literal">True</span>, <span class="literal">True</span>, <span class="literal">True</span>, <span class="literal">True</span>, <span class="literal">True</span>])</span><br><span class="line">tensor([<span class="literal">True</span>, <span class="literal">True</span>, <span class="literal">True</span>, <span class="literal">True</span>, <span class="literal">True</span>, <span class="literal">True</span>, <span class="literal">True</span>, <span class="literal">True</span>])</span><br></pre></td></tr></table></figure>
会发现$shared$层在net中被使用了两次，但它们依然拥有相同的参数，它们不仅值相等，而且由相同的张量表示。因此我们改变其中一个参数，另一个参数也会改变。问题：当参数绑定时，梯度会发生什么情况？答案时由于模型参数包含梯度，因此在反向传播期间死三个神经网络层和第五个神经网络层的梯度会加到一起。</li>
</ol>
<h3 id="练习-12"><a href="#练习-12" class="headerlink" title="练习"></a>练习</h3><p>为什么共享参数是个好主意？</p>
<ol>
<li>可以减少模型的参数量，降低复杂度</li>
<li>提高泛化能力</li>
<li>在循环神经网络/卷积神经网络中参数共享可以帮助模型更好地捕捉数据的时空特征，提高学习效率<h2 id="自定义层"><a href="#自定义层" class="headerlink" title="自定义层"></a>自定义层</h2>首先构造一个没有任何参数的层（减去均值）<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">CenteredLayer</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, X</span>):</span><br><span class="line">        <span class="keyword">return</span> X - X.mean()</span><br></pre></td></tr></table></figure>
将层作为组件合并到更复杂的模型中：<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">net = nn.Sequential(nn.Linear(<span class="number">8</span>, <span class="number">128</span>), CenteredLayer())</span><br></pre></td></tr></table></figure>
<h3 id="设计带参数的层"><a href="#设计带参数的层" class="headerlink" title="设计带参数的层"></a>设计带参数的层</h3>(其实是自定义了参数)<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">MyLinear</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, in_units, units</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.weight = nn.Parameter(torch.randn(in_units, units))</span><br><span class="line">        self.bias = nn.Parameter(torch.randn(units,))</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, X</span>):</span><br><span class="line">        linear = torch.matmul(X, self.weight.data) + self.bias.data</span><br><span class="line">        <span class="keyword">return</span> F.relu(linear)</span><br></pre></td></tr></table></figure>
初始化<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">linear = MyLinear(<span class="number">5</span>,<span class="number">3</span>)</span><br><span class="line">linear.weight</span><br><span class="line">linear(torch.rand(<span class="number">2</span>,<span class="number">5</span>))</span><br></pre></td></tr></table></figure>
<h3 id="练习-13"><a href="#练习-13" class="headerlink" title="练习"></a>练习</h3>设计一个返回输入数据的傅里叶前半部分的层<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">FourierFrontHalf</span>(nn.Module): </span><br><span class="line">	<span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>): <span class="built_in">super</span>(FourierFrontHalf, self).__init__() </span><br><span class="line">	<span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>): <span class="comment"># 计算傅里叶变换 </span></span><br><span class="line">		x_fft = torch.fft.fft(x) <span class="comment"># 获取前半部分 </span></span><br><span class="line">		front_half = x_fft[:, :x_fft.shape[<span class="number">1</span>] // <span class="number">2</span>] </span><br><span class="line">		<span class="keyword">return</span> front_half</span><br></pre></td></tr></table></figure>
<h2 id="读写文件"><a href="#读写文件" class="headerlink" title="读写文件"></a>读写文件</h2><h3 id="基本load，save操作"><a href="#基本load，save操作" class="headerlink" title="基本load，save操作"></a>基本load，save操作</h3>对于单个张量可以调用load和save函数分别读写它们。<br>这两个函数都要求我们提供一个名称，<code>save</code>要求将要保存的变量作为输入。<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line">form torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.nn <span class="keyword">import</span> functional <span class="keyword">as</span> F</span><br><span class="line">x = torch.arrange(<span class="number">4</span>)</span><br><span class="line">torch.save(x,<span class="string">&#x27;x-file&#x27;</span>)</span><br></pre></td></tr></table></figure>
将存储在文件中的数据读回内存：<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">x2 = torch.load(<span class="string">&#x27;x-file&#x27;</span>)</span><br></pre></td></tr></table></figure>
存储一个张量列表，然后把它们（分别）读回内存。<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">y = torch.zeros(<span class="number">4</span>)</span><br><span class="line">torch.save([x, y],<span class="string">&#x27;x-files&#x27;</span>)</span><br><span class="line">x2, y2 = torch.load(<span class="string">&#x27;x-files&#x27;</span>)</span><br><span class="line">(x2, y2)</span><br></pre></td></tr></table></figure>
可以写入或读取从字符串映射到张量的字典。 当我们要读取或写入模型中的所有权重时，这很方便<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">mydict = &#123;<span class="string">&#x27;x&#x27;</span>: x, <span class="string">&#x27;y&#x27;</span>: y&#125;</span><br><span class="line">torch.save(mydict, <span class="string">&#x27;mydict&#x27;</span>)</span><br><span class="line">mydict2 = torch.load(<span class="string">&#x27;mydict&#x27;</span>)</span><br><span class="line">mydict2</span><br><span class="line"><span class="comment"># &#123;&#x27;x&#x27;: tensor([0, 1, 2, 3]), &#x27;y&#x27;: tensor([0., 0., 0., 0.])&#125;</span></span><br></pre></td></tr></table></figure>
<h3 id="模型的加载与保存"><a href="#模型的加载与保存" class="headerlink" title="模型的加载与保存"></a>模型的加载与保存</h3>深度学习框架提供了内置函数来保存和加载整个网络。 需要注意的一个重要细节是，这将保存模型的参数而不是保存整个模型。<br>多层感知机模型参数存储：<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">MLP</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.hidden = nn.Linear(<span class="number">20</span>, <span class="number">256</span>)</span><br><span class="line">        self.output = nn.Linear(<span class="number">256</span>, <span class="number">10</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="keyword">return</span> self.output(F.relu(self.hidden(x)))</span><br><span class="line"></span><br><span class="line">net = MLP()</span><br><span class="line">X = torch.randn(size=(<span class="number">2</span>, <span class="number">20</span>))</span><br><span class="line">Y = net(X)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.save(net.state_dict(), <span class="string">&#x27;mlp.params&#x27;</span>)</span><br></pre></td></tr></table></figure>
为了恢复模型，我们实例化了原始多层感知机模型的一个备份。 这里我们不需要随机初始化模型参数，而是直接读取文件中存储的参数：<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">clone = MLP()</span><br><span class="line">clone.load_state_dict(torch.load(<span class="string">&#x27;mlp.params&#x27;</span>))</span><br><span class="line">clone.<span class="built_in">eval</span>()</span><br></pre></td></tr></table></figure>
<h3 id="练习-14"><a href="#练习-14" class="headerlink" title="练习"></a>练习</h3></li>
<li>即使不需要将经过训练的模型部署到不同的设备上，存储模型参数还有什么实际的好处？<br> 节省内存，模型压缩，快速部署，方便下一次训练</li>
<li>假设我们只想复用网络的一部分，以将其合并到不同的网络架构中。比如想在一个新的网络中使用之前网络的前两层，该怎么做？</li>
</ol>
<ul>
<li><strong>提取需要复用的部分</strong>：从原始网络中提取前两层的参数。</li>
<li><strong>构建新网络</strong>：构建一个新的网络，将提取的部分作为其中的一部分。（以resnet为例）<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">from</span> torchvision.models <span class="keyword">import</span> resnet18</span><br><span class="line"><span class="comment"># 加载预训练的ResNet模型</span></span><br><span class="line">pretrained_model = resnet18(pretrained=<span class="literal">True</span>)</span><br><span class="line"><span class="comment"># 提取前两层</span></span><br><span class="line">conv1 = pretrained_model.conv1</span><br><span class="line">bn1 = pretrained_model.bn1</span><br><span class="line"><span class="comment"># 构建新网络</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">NewNetwork</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(NewNetwork, self).__init__()</span><br><span class="line">        self.conv1 = conv1</span><br><span class="line">        self.bn1 = bn1</span><br><span class="line">        <span class="comment"># 添加新的层</span></span><br><span class="line">        self.conv2 = nn.Conv2d(<span class="number">64</span>, <span class="number">128</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>)</span><br><span class="line">        self.bn2 = nn.BatchNorm2d(<span class="number">128</span>)</span><br><span class="line">        self.relu = nn.ReLU(inplace=<span class="literal">True</span>)</span><br><span class="line">        self.pool = nn.MaxPool2d(kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>)</span><br><span class="line">        <span class="comment"># 其他层...</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = self.conv1(x)</span><br><span class="line">        x = self.bn1(x)</span><br><span class="line">        x = self.relu(x)</span><br><span class="line">        x = self.conv2(x)</span><br><span class="line">        x = self.bn2(x)</span><br><span class="line">        x = self.relu(x)</span><br><span class="line">        x = self.pool(x)</span><br><span class="line">        <span class="comment"># 其他层的前向传播...</span></span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"><span class="comment"># 创建新网络的实例</span></span><br><span class="line">new_network = NewNetwork()</span><br></pre></td></tr></table></figure></li>
</ul>
<ol>
<li>如何同时保存网络架构和参数？需要对架构加上什么限制？<br>将参数与架构同时保存在同一文件中，（实例化模型并保存）加载时分开加载：<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 假设我有一个模型SimpleModel()</span></span><br><span class="line">model = SimpleModel() </span><br><span class="line">torch.save(&#123;<span class="string">&#x27;model_state_dict&#x27;</span>: model.state_dict(), <span class="string">&#x27;model_architecture&#x27;</span>: SimpleModel&#125;, <span class="string">&#x27;model.pth&#x27;</span>)</span><br></pre></td></tr></table></figure>
加载时：<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 加载模型</span></span><br><span class="line">checkpoint = torch.load(<span class="string">&#x27;model.pth&#x27;</span>)</span><br><span class="line">model_architecture = checkpoint[<span class="string">&#x27;model_architecture&#x27;</span>]</span><br><span class="line">model = model_architecture()</span><br><span class="line">model.load_state_dict(checkpoint[<span class="string">&#x27;model_state_dict&#x27;</span>])</span><br></pre></td></tr></table></figure>
</li>
</ol>
<h2 id="GPU"><a href="#GPU" class="headerlink" title="GPU"></a>GPU</h2><p>PyTorch中，CPU和GPU可以用<code>torch.device(&#39;cpu&#39;)</code> 和<code>torch.device(&#39;cuda&#39;)</code>表示。 应该注意的是，<code>cpu</code>设备意味着所有物理CPU和内存， 这意味着PyTorch的计算将尝试使用所有CPU核心。 然而，<code>gpu</code>设备只代表一个卡和相应的显存。 如果有多个GPU，我们使用<code>torch.device(f&#39;cuda:&#123;i&#125;&#39;)</code> 来表示第�块GPU（�从0开始）。 另外，<code>cuda:0</code>和<code>cuda</code>是等价的。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"></span><br><span class="line">torch.device(<span class="string">&#x27;cpu&#x27;</span>), torch.device(<span class="string">&#x27;cuda&#x27;</span>), torch.device(<span class="string">&#x27;cuda:1&#x27;</span>)</span><br></pre></td></tr></table></figure><br>查询gpu数量<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.cuda.device_count()</span><br></pre></td></tr></table></figure><br>可以定义一个函数，可以在不存在所有所需gpu的情况下运行代码：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">try_gpu</span>(<span class="params">i=<span class="number">0</span></span>):  <span class="comment">#@save</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;如果存在，则返回gpu(i)，否则返回cpu()&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">if</span> torch.cuda.device_count() &gt;= i + <span class="number">1</span>:</span><br><span class="line">        <span class="keyword">return</span> torch.device(<span class="string">f&#x27;cuda:<span class="subst">&#123;i&#125;</span>&#x27;</span>)</span><br><span class="line">    <span class="keyword">return</span> torch.device(<span class="string">&#x27;cpu&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">try_all_gpus</span>():  <span class="comment">#@save</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;返回所有可用的GPU，如果没有GPU，则返回[cpu(),]&quot;&quot;&quot;</span></span><br><span class="line">    devices = [torch.device(<span class="string">f&#x27;cuda:<span class="subst">&#123;i&#125;</span>&#x27;</span>)</span><br><span class="line">             <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(torch.cuda.device_count())]</span><br><span class="line">    <span class="keyword">return</span> devices <span class="keyword">if</span> devices <span class="keyword">else</span> [torch.device(<span class="string">&#x27;cpu&#x27;</span>)]</span><br><span class="line"></span><br><span class="line">try_gpu(), try_gpu(<span class="number">10</span>), try_all_gpus()</span><br></pre></td></tr></table></figure><br>默认创建张量是在CPU上的<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">X = torch.ones(<span class="number">2</span>, <span class="number">3</span>, device=try_gpu())</span><br><span class="line">X</span><br></pre></td></tr></table></figure><br>可以在指定的gpu上创建张量：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Y = torch.rand(<span class="number">2</span>, <span class="number">3</span>, device=try_gpu(<span class="number">1</span>))</span><br><span class="line">Y</span><br></pre></td></tr></table></figure><br>这时，X在’cuda:0’上，Y在‘cuda:1’上，如果我们要计算<code>X + Y</code>，我们需要决定在哪里执行这个操作。们可以将<code>X</code>传输到第二个GPU并在那里执行操作。 <em>不要</em>简单地<code>X</code>加上<code>Y</code>，因为这会导致异常， 运行时引擎不知道该怎么做：它在同一设备上找不到数据会导致失败。 由于<code>Y</code>位于第二个GPU上，所以我们需要将<code>X</code>移到那里， 然后才能执行相加运算。<br>可以将X复制到与Y相同的GPU进行相加<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Z = X.cuda(<span class="number">1</span>)</span><br><span class="line">Y + Z</span><br></pre></td></tr></table></figure><br>将模型参数放在GPU上：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">net = nn.Sequential(nn.Linear(<span class="number">3</span>, <span class="number">1</span>))</span><br><span class="line">net = net.to(device=try_gpu())</span><br></pre></td></tr></table></figure></p>
<h3 id="练习-15"><a href="#练习-15" class="headerlink" title="练习"></a>练习</h3><ol>
<li>尝试一个计算量更大的任务，比如大矩阵的乘法，看看CPU和GPU之间的速度差异。再试一个计算量很小的任务呢？</li>
</ol>
<ul>
<li>大任务测试：<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="comment"># 设置矩阵大小</span></span><br><span class="line">size = <span class="number">5000</span></span><br><span class="line"><span class="comment"># 创建两个大矩阵</span></span><br><span class="line">a = torch.rand(size, size)</span><br><span class="line">b = torch.rand(size, size)</span><br><span class="line"><span class="comment"># 在CPU上进行矩阵乘法</span></span><br><span class="line">start_time = time.time()</span><br><span class="line">result_cpu = torch.matmul(a, b)</span><br><span class="line">cpu_time = time.time() - start_time</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;CPU time: <span class="subst">&#123;cpu_time:<span class="number">.6</span>f&#125;</span> seconds&quot;</span>)</span><br><span class="line"><span class="comment"># 如果可以使用GPU，进行相同的测试</span></span><br><span class="line"><span class="keyword">if</span> torch.cuda.is_available():</span><br><span class="line">    a = a.cuda()</span><br><span class="line">    b = b.cuda()</span><br><span class="line">    start_time = time.time()</span><br><span class="line">    result_gpu = torch.matmul(a, b)</span><br><span class="line">    gpu_time = time.time() - start_time</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;GPU time: <span class="subst">&#123;gpu_time:<span class="number">.6</span>f&#125;</span> seconds&quot;</span>)</span><br></pre></td></tr></table></figure>
CPU time: 1.693618 seconds<br>GPU time: 0.643845 seconds<br>小任务：<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 设置矩阵大小</span></span><br><span class="line">size = <span class="number">100</span></span><br><span class="line"><span class="comment"># 创建两个小矩阵</span></span><br><span class="line">a = torch.rand(size, size)</span><br><span class="line">b = torch.rand(size, size)</span><br><span class="line"><span class="comment"># 在CPU上进行矩阵乘法</span></span><br><span class="line">start_time = time.time()</span><br><span class="line">result_cpu = torch.matmul(a, b)</span><br><span class="line">cpu_time = time.time() - start_time</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;CPU time (small task): <span class="subst">&#123;cpu_time:<span class="number">.6</span>f&#125;</span> seconds&quot;</span>)</span><br><span class="line"><span class="comment"># 如果可以使用GPU，进行相同的测试</span></span><br><span class="line"><span class="keyword">if</span> torch.cuda.is_available():</span><br><span class="line">    a = a.cuda()</span><br><span class="line">    b = b.cuda()</span><br><span class="line">    start_time = time.time()</span><br><span class="line">    result_gpu = torch.matmul(a, b)</span><br><span class="line">    gpu_time = time.time() - start_time</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;GPU time (small task): <span class="subst">&#123;gpu_time:<span class="number">.6</span>f&#125;</span> seconds&quot;</span>)</span><br></pre></td></tr></table></figure>
CPU time (small task): 0.010012 seconds<br>GPU time (small task): 0.002001 seconds</li>
</ul>
<ol>
<li>我们应该如何在GPU上读写模型参数？<br>可以通过<em>model.parameters</em>获取模型终端 参数<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> param <span class="keyword">in</span> model.parameters(): <span class="built_in">print</span>(param)</span><br></pre></td></tr></table></figure></li>
<li>测量计算1000个100×100矩阵的矩阵乘法所需的时间，并记录输出矩阵的Frobenius范数，一次记录一个结果，而不是在GPU上保存日志并仅传输最终结果。<br>首先，Frobenius范数可以看作是矩阵中所有元素的平方和的平方根，类似于向量的欧几里得范数（L2范数）在矩阵上的推广。<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="comment"># 设置矩阵大小和数量</span></span><br><span class="line">matrix_size = <span class="number">100</span></span><br><span class="line">num_matrices = <span class="number">1000</span></span><br><span class="line">device = torch.device(<span class="string">&#x27;cuda&#x27;</span>)</span><br><span class="line">frobenius_norms = []</span><br><span class="line"><span class="comment"># 开始计时</span></span><br><span class="line">start_time = time.time()</span><br><span class="line"><span class="comment"># 进行矩阵乘法并记录Frobenius范数</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(num_matrices):</span><br><span class="line">    <span class="comment"># 创建两个随机矩阵并移动到GPU</span></span><br><span class="line">    a = torch.rand(matrix_size, matrix_size, device=device)</span><br><span class="line">    b = torch.rand(matrix_size, matrix_size, device=device)</span><br><span class="line">    <span class="comment"># 在GPU上进行矩阵乘法</span></span><br><span class="line">    result = torch.matmul(a, b)</span><br><span class="line">    <span class="comment"># 计算Frobenius范数并移动到CPU</span></span><br><span class="line">    frob_norm = torch.norm(result, p=<span class="string">&#x27;fro&#x27;</span>).cpu().item()</span><br><span class="line">    <span class="comment"># 记录结果</span></span><br><span class="line">    frobenius_norms.append(frob_norm)</span><br><span class="line"><span class="comment"># 结束计时</span></span><br><span class="line">end_time = time.time()</span><br><span class="line">total_time = end_time - start_time</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Total time for <span class="subst">&#123;num_matrices&#125;</span> matrix multiplications: <span class="subst">&#123;total_time:<span class="number">.6</span>f&#125;</span> seconds&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Frobenius norms of output matrices:&quot;</span>, frobenius_norms)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
Total time for 1000 matrix multiplications: 4.377934 seconds<h1 id="卷积神经网络"><a href="#卷积神经网络" class="headerlink" title="卷积神经网络"></a>卷积神经网络</h1><h2 id="全连接到卷积"><a href="#全连接到卷积" class="headerlink" title="全连接到卷积"></a>全连接到卷积</h2></li>
<li>为什么平移不变性可能也不是好主意呢？<br> 平移不变性意味着卷积层对输入数据的平移具有不变性，这在很多情况下是有益的，因为它允许模型识别对象，无论它们在图像中的位置如何。然而，在某些情况下，平移不变性可能不是一个好主意。例如，如果图像中对象的位置对于任务非常重要，那么平移不变性可能会导致信息丢失。</li>
<li>当从图像边界像素获取隐藏表示时，我们需要思考哪些问题？<br> 我们需要考虑边界效应和填充策略。由于卷积核在边界处可能无法完全覆盖所有像素，这可能导致边界像素的信息不足。为了解决这个问题，我们可以采用填充（padding）策略</li>
<li>描述一个类似的音频卷积层的架构。<br> 音频数据通常以一维时间序列的形式出现，因此音频卷积层通常使用一维卷积（1D convolution）。这种卷积层在时间轴上滑动卷积核，提取音频信号的局部特征。例如，一个音频卷积层可以由一系列一维卷积层组成，每个层使用不同大小的卷积核和步长来捕捉不同时间尺度上的特征。这类似于图像卷积层使用二维卷积提取空间特征。</li>
<li>卷积层也适合于文本数据吗？为什么？<br> 在文本处理中，卷积层可以用来捕捉词汇或短语的局部模式。例如，通过在嵌入层（embedding layer）之后应用一维卷积层，模型可以学习到文本中词语的组合特征。<h2 id="图像卷积"><a href="#图像卷积" class="headerlink" title="图像卷积"></a>图像卷积</h2><h3 id="互相关运算"><a href="#互相关运算" class="headerlink" title="互相关运算"></a>互相关运算</h3>在二维互相关运算中，卷积窗口从输入张量的左上角开始，从左到右、从上到下滑动。 当卷积窗口滑动到新一个位置时，包含在该窗口中的部分张量与卷积核张量进行按元素相乘，得到的张量再求和得到一个单一的标量值，由此我们得出了这一位置的输出张量值。<h3 id="卷积层"><a href="#卷积层" class="headerlink" title="卷积层"></a>卷积层</h3>卷积层中的两个被训练的参数是卷积核权重和标量偏置。 就像我们之前随机初始化全连接层一样，在训练基于卷积层的模型时，我们也随机初始化卷积核权重。<h3 id="感受野"><a href="#感受野" class="headerlink" title="感受野"></a>感受野</h3>在卷积神经网络中，对于某一层的任意元素x，其_感受野（receptive field）是指在前向传播期间可能影响x计算的所有元素（来自所有先前层）。<h3 id="练习-16"><a href="#练习-16" class="headerlink" title="练习"></a>练习</h3></li>
<li>在我们创建的<code>Conv2D</code>自动求导时，有什么错误消息？<br> 没有使用PyTorch的操作来确保梯度可以被追踪</li>
<li>如何通过改变输入张量和卷积核张量，将互相关运算表示为矩阵乘法？<br> 把输入和卷积展平，卷积核每滑动一个步幅取一个样本。</li>
<li>手工设计一些卷积核。<ol>
<li>二阶导数的核的形式是什么？<br> 二阶导数卷积核就是拉普拉斯算子[[0,1,0],[1,-4,1],[0,1,0]]  </li>
<li>积分的核的形式是什么？<br> d+1<h2 id="填充与步幅"><a href="#填充与步幅" class="headerlink" title="填充与步幅"></a>填充与步幅</h2><h3 id="通用公式"><a href="#通用公式" class="headerlink" title="通用公式"></a>通用公式</h3><script type="math/tex; mode=display">o = [( i + 2p - k) / s] + 1</script>其中：</li>
</ol>
</li>
</ol>
<ul>
<li>i：输入尺寸input</li>
<li>o：输出output</li>
<li>s：步长stride、</li>
<li>p：填充padding（一般都是零）</li>
<li>k：卷积核（kernel）大小<h3 id="展开公式"><a href="#展开公式" class="headerlink" title="展开公式"></a>展开公式</h3><script type="math/tex; mode=display">[(n_h-k_h+p_h+s_h)/s_h]*[(n_w-k_w+p_w+s_w)/s_w]</script><h2 id="多输入多输出通道"><a href="#多输入多输出通道" class="headerlink" title="多输入多输出通道"></a>多输入多输出通道</h2><h3 id="多输入通道"><a href="#多输入通道" class="headerlink" title="多输入通道"></a>多输入通道</h3>当输入包含多个通道时，需要构造一个与输入数据具有相同输入通道数的卷积核，以便与输入数据进行互相关运算。<h3 id="多输出通道"><a href="#多输出通道" class="headerlink" title="多输出通道"></a>多输出通道</h3>在最流行的神经网络架构中，随着神经网络层数的加深，我们常会增加输出通道的维数，通过减少空间分辨率以获得更大的通道深度。直观地说，我们可以将每个通道看作对不同特征的响应。而现实可能更为复杂一些，因为每个通道不是独立学习的，而是为了共同使用而优化的。因此，多输出通道并不仅是学习多个单通道的检测器。用$c_i$和$c_o$分别表示输入和输出通道的数目，并让$k_h$和$k_w$为卷积核的高度和宽度。为了获得多个通道的输出，我们可以为每个输出通道创建一个形状为$c_i<em>k_h</em>k_w$的卷积核张量，这样卷积核的形状是$c_o<em>c_i</em>k_h*k_w$。在互相关运算中，每个输出通道先获取所有输入通道，再以对应该输出通道的卷积核计算出结果。<h3 id="1-1卷积"><a href="#1-1卷积" class="headerlink" title="1 * 1卷积"></a>1 * 1卷积</h3><h2 id="汇聚层（pooling池化层）"><a href="#汇聚层（pooling池化层）" class="headerlink" title="汇聚层（pooling池化层）"></a>汇聚层（pooling池化层）</h2>它具有双重目的：降低卷积层对位置的敏感性，同时降低对空间降采样表示的敏感性。<h3 id="最大汇聚层和平均汇聚层"><a href="#最大汇聚层和平均汇聚层" class="headerlink" title="最大汇聚层和平均汇聚层"></a>最大汇聚层和平均汇聚层</h3>与卷积层类似，汇聚层运算符由一个固定形状的窗口组成，该窗口根据其步幅大小在输入的所有区域上滑动，为固定形状窗口（有时称为<em>汇聚窗口</em>）遍历的每个位置计算一个输出。 然而，不同于卷积层中的输入与卷积核之间的互相关计算，汇聚层不包含参数。 相反，池运算是确定性的，我们通常计算汇聚窗口中所有元素的最大值或平均值。这些操作分别称为<em>最大汇聚层</em>（maximum pooling）和<em>平均汇聚层</em>（average pooling）。</li>
</ul>
<p>在这两种情况下，与互相关运算符一样，汇聚窗口从输入张量的左上角开始，从左往右、从上往下的在输入张量内滑动。在汇聚窗口到达的每个位置，它计算该窗口中输入子张量的最大值或平均值。计算最大值或平均值是取决于使用了最大汇聚层还是平均汇聚层。</p>
<h3 id="练习-17"><a href="#练习-17" class="headerlink" title="练习"></a>练习</h3><p>除了平均汇聚层和最大汇聚层，是否有其它函数可以考虑（提示：回想一下softmax）？为什么它不流行？<br>除了平均汇聚层（Average Pooling）和最大汇聚层（Max Pooling），还可以考虑使用Softmax汇聚层。Softmax汇聚层是一种在汇聚操作中应用Softmax函数的方法，它可以为汇聚区域内的每个像素分配一个权重，这些权重根据像素的相对大小进行加权和。<br>Softmax汇聚层的一个潜在优点是它可以提供一种更加灵活和可学习的汇聚机制，因为它根据输入的特征自适应地调整汇聚区域内像素的权重。不流行的原因有：计算复杂度，训练困难（梯度小时或梯度爆炸），解释性差。</p>
<h2 id="卷积神经网络（LeNet）"><a href="#卷积神经网络（LeNet）" class="headerlink" title="卷积神经网络（LeNet）"></a>卷积神经网络（LeNet）</h2><ul>
<li>卷积编码器：由两个卷积层组成;</li>
<li>全连接层密集块：由三个全连接层组成。<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br><span class="line"></span><br><span class="line">net = nn.Sequential(</span><br><span class="line">    nn.Conv2d(<span class="number">1</span>, <span class="number">6</span>, kernel_size=<span class="number">5</span>, padding=<span class="number">2</span>), nn.Sigmoid(),</span><br><span class="line">    nn.AvgPool2d(kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>),</span><br><span class="line">    nn.Conv2d(<span class="number">6</span>, <span class="number">16</span>, kernel_size=<span class="number">5</span>), nn.Sigmoid(),</span><br><span class="line">    nn.AvgPool2d(kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>),</span><br><span class="line">    nn.Flatten(),</span><br><span class="line">    nn.Linear(<span class="number">16</span> * <span class="number">5</span> * <span class="number">5</span>, <span class="number">120</span>), nn.Sigmoid(),</span><br><span class="line">    nn.Linear(<span class="number">120</span>, <span class="number">84</span>), nn.Sigmoid(),</span><br><span class="line">    nn.Linear(<span class="number">84</span>, <span class="number">10</span>))</span><br><span class="line">X = torch.rand(size=(<span class="number">1</span>, <span class="number">1</span>, <span class="number">28</span>, <span class="number">28</span>), dtype=torch.float32)</span><br><span class="line"><span class="keyword">for</span> layer <span class="keyword">in</span> net:</span><br><span class="line">    X = layer(X)</span><br><span class="line">    <span class="built_in">print</span>(layer.__class__.__name__,<span class="string">&#x27;output shape:\t&#x27;</span>,X.shape)</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h3 id="练习-18"><a href="#练习-18" class="headerlink" title="练习"></a>练习</h3><ol>
<li>将平均汇聚层替换为最大汇聚层，会发生什么？<br> 导致模型捕捉更加显著的特征，因为最大汇聚层倾向于保留最强的信号，而忽略较弱的信号。这种变化可能会提高模型的性能，特别是在处理具有明显特征的图像时。</li>
<li>尝试构建一个基于LeNet的更复杂的网络，以提高其准确性。<ol>
<li>调整卷积窗口大小。</li>
<li>调整输出通道的数量。</li>
<li>调整激活函数（如ReLU）。</li>
<li>调整卷积层的数量。</li>
<li>调整全连接层的数量。</li>
<li>调整学习率和其他训练细节（例如，初始化和轮数）。<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">ImprovedLeNet</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(ImprovedLeNet, self).__init__()</span><br><span class="line">        self.conv1 = nn.Conv2d(<span class="number">1</span>, <span class="number">16</span>, kernel_size=<span class="number">5</span>, stride=<span class="number">1</span>, padding=<span class="number">2</span>) </span><br><span class="line">        self.conv2 = nn.Conv2d(<span class="number">16</span>, <span class="number">32</span>, kernel_size=<span class="number">5</span>, stride=<span class="number">1</span>, padding=<span class="number">2</span>) </span><br><span class="line">        self.conv3 = nn.Conv2d(<span class="number">32</span>, <span class="number">64</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>)</span><br><span class="line">        self.pool = nn.MaxPool2d(kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>)</span><br><span class="line">        self.fc1 = nn.Linear(<span class="number">64</span> * <span class="number">7</span> * <span class="number">7</span>, <span class="number">120</span>)</span><br><span class="line">        self.fc2 = nn.Linear(<span class="number">120</span>, <span class="number">84</span>)</span><br><span class="line">        self.fc3 = nn.Linear(<span class="number">84</span>, <span class="number">10</span>)</span><br><span class="line">        self.relu = nn.ReLU()</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = self.pool(self.relu(self.conv1(x)))</span><br><span class="line">        x = self.pool(self.relu(self.conv2(x)))</span><br><span class="line">        x = self.pool(self.relu(self.conv3(x)))</span><br><span class="line">        x = x.view(-<span class="number">1</span>, <span class="number">64</span> * <span class="number">7</span> * <span class="number">7</span>)</span><br><span class="line">        x = self.relu(self.fc1(x))</span><br><span class="line">        x = self.relu(self.fc2(x))</span><br><span class="line">        x = self.fc3(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line">model = ImprovedLeNet()</span><br><span class="line"></span><br><span class="line">criterion = nn.CrossEntropyLoss()</span><br><span class="line">optimizer = optim.Adam(model.parameters(), lr=<span class="number">0.001</span>)  </span><br></pre></td></tr></table></figure>
</li>
</ol>
</li>
</ol>
<h1 id="现代卷积网络"><a href="#现代卷积网络" class="headerlink" title="现代卷积网络"></a>现代卷积网络</h1><h2 id="深度卷积神经网络"><a href="#深度卷积神经网络" class="headerlink" title="深度卷积神经网络"></a>深度卷积神经网络</h2><p>在计算机视觉中，直接将神经网络与其他机器学习方法进行比较也许不公平。这是因为，卷积神经网络的输入是由原始像素值或是经过简单预处理（例如居中、缩放）的像素值组成的。但在使用传统机器学习方法时，从业者永远不会将原始像素作为输入。在传统机器学习方法中，计算机视觉流水线是由经过人的手工精心设计的特征流水线组成的。对于这些传统方法，大部分的进展都来自于对特征有了更聪明的想法，并且学习到的算法往往归于事后的解释。</p>
<h3 id="Alexnet"><a href="#Alexnet" class="headerlink" title="Alexnet"></a>Alexnet</h3><p>在AlexNet的第一层，卷积窗口的形状是11×11。 由于ImageNet中大多数图像的宽和高比MNIST图像的多10倍以上，因此，需要一个更大的卷积窗口来捕获目标。 第二层中的卷积窗口形状被缩减为5×5，然后是3×3。 此外，在第一层、第二层和第五层卷积层之后，加入窗口形状为3×3、步幅为2的最大汇聚层。 而且，AlexNet的卷积通道数目是LeNet的10倍。<br>在最后一个卷积层后有两个全连接层，分别有4096个输出。 这两个巨大的全连接层拥有将近1GB的模型参数。<br>此外，AlexNet将sigmoid激活函数改为更简单的ReLU激活函数。 一方面，ReLU激活函数的计算更简单，它不需要如sigmoid激活函数那般复杂的求幂运算。<br>AlexNet通过暂退法（dropout）控制全连接层的模型复杂度，而LeNet只使用了权重衰减。</p>
<h3 id="练习-19"><a href="#练习-19" class="headerlink" title="练习"></a>练习</h3><ol>
<li>AlexNet对Fashion-MNIST数据集来说可能太复杂了。<ol>
<li>尝试简化模型以加快训练速度，同时确保准确性不会显著下降。</li>
<li>设计一个更好的模型，可以直接在28×28图像上工作。<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">class</span> <span class="title class_">SimpleCNN</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(SimpleCNN, self).__init__()</span><br><span class="line">        self.conv1 = nn.Conv2d(<span class="number">1</span>, <span class="number">32</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>)</span><br><span class="line">        self.conv2 = nn.Conv2d(<span class="number">32</span>, <span class="number">64</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>)</span><br><span class="line">        self.pool = nn.MaxPool2d(kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>, padding=<span class="number">0</span>)</span><br><span class="line">        self.fc1 = nn.Linear(<span class="number">64</span> * <span class="number">7</span> * <span class="number">7</span>, <span class="number">128</span>)</span><br><span class="line">        self.fc2 = nn.Linear(<span class="number">128</span>, <span class="number">10</span>)</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = F.relu(self.conv1(x))</span><br><span class="line">        x = self.pool(x)</span><br><span class="line">        x = F.relu(self.conv2(x))</span><br><span class="line">        x = self.pool(x)</span><br><span class="line">        x = x.view(-<span class="number">1</span>, <span class="number">64</span> * <span class="number">7</span> * <span class="number">7</span>)</span><br><span class="line">        x = F.relu(self.fc1(x))</span><br><span class="line">        x = self.fc2(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line">model = SimpleCNN()</span><br><span class="line"><span class="built_in">print</span>(model)</span><br></pre></td></tr></table></figure></li>
</ol>
</li>
<li>分析了AlexNet的计算性能。<ol>
<li>在AlexNet中主要是哪部分占用显存？<br> 全连接的权重和偏置</li>
<li>在AlexNet中主要是哪部分需要更多的计算？<br> 卷积层</li>
<li>计算结果时显存带宽如何？<br>显存带宽决定了数据（如权重、激活值和梯度）在GPU内存和计算单元之间传输的速度<h2 id="使用块的网络（VGG）"><a href="#使用块的网络（VGG）" class="headerlink" title="使用块的网络（VGG）"></a>使用块的网络（VGG）</h2><h3 id="VGG块"><a href="#VGG块" class="headerlink" title="VGG块"></a>VGG块</h3>经典卷积神经网络基本组成部分：</li>
</ol>
</li>
<li>卷积层</li>
<li>激活函数</li>
<li><p>pooling汇聚层<br>vggblock的实现：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">vgg_block</span>(<span class="params">num_convs, in_channels, out_channels</span>):</span><br><span class="line">    layers = []</span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(num_convs):</span><br><span class="line">        layers.append(nn.Conv2d(in_channels, out_channels,</span><br><span class="line">                                kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>))</span><br><span class="line">        layers.append(nn.ReLU())</span><br><span class="line">        in_channels = out_channels</span><br><span class="line">    layers.append(nn.MaxPool2d(kernel_size=<span class="number">2</span>,stride=<span class="number">2</span>))</span><br><span class="line">    <span class="keyword">return</span> nn.Sequential(*layers)</span><br></pre></td></tr></table></figure>
<p>接受三个参数，分别对应于卷积层的数量num_convs、输入通道的数量in_channels和输出通道的数量out_channels。由于该网络使用8个卷积层和3个全连接层，因此它通常被称为VGG-11。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">vgg</span>(<span class="params">conv_arch</span>):</span><br><span class="line">    conv_blks = []</span><br><span class="line">    in_channels = <span class="number">1</span></span><br><span class="line">    <span class="comment"># 卷积层部分</span></span><br><span class="line">    <span class="keyword">for</span> (num_convs, out_channels) <span class="keyword">in</span> conv_arch:</span><br><span class="line">        conv_blks.append(vgg_block(num_convs, in_channels, out_channels))</span><br><span class="line">        in_channels = out_channels</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> nn.Sequential(</span><br><span class="line">        *conv_blks, nn.Flatten(),</span><br><span class="line">        <span class="comment"># 全连接层部分</span></span><br><span class="line">        nn.Linear(out_channels * <span class="number">7</span> * <span class="number">7</span>, <span class="number">4096</span>), nn.ReLU(), nn.Dropout(<span class="number">0.5</span>),</span><br><span class="line">        nn.Linear(<span class="number">4096</span>, <span class="number">4096</span>), nn.ReLU(), nn.Dropout(<span class="number">0.5</span>),</span><br><span class="line">        nn.Linear(<span class="number">4096</span>, <span class="number">10</span>))</span><br><span class="line"></span><br><span class="line">net = vgg(conv_arch)</span><br></pre></td></tr></table></figure>
<p>可以看出，在定义网络时，可以先用列表来存储（append）块卷积层，最后再return nn.sequential(*list)。<br>其中超参数conv_arch定义了每个VGG块李卷积层的个数和输出通道数。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conv_arch = ((<span class="number">1</span>, <span class="number">64</span>), (<span class="number">1</span>, <span class="number">128</span>), (<span class="number">2</span>, <span class="number">256</span>), (<span class="number">2</span>, <span class="number">512</span>), (<span class="number">2</span>, <span class="number">512</span>))</span><br></pre></td></tr></table></figure>
</li>
<li><p>打印层的尺寸时，我们只看到8个结果，而不是11个结果。剩余的3层信息去哪了？<br> 后三层的2个卷积看成了一个块，所以只有8层</p>
</li>
<li>与AlexNet相比，VGG的计算要慢得多，而且它还需要更多的显存。分析出现这种情况的原因。<br> VGG的网络更深，参数量更大，并且VGG的线性层参数更多</li>
<li><p>尝试将Fashion-MNIST数据集图像的高度和宽度从224改为96。这对实验有什么影响？</p>
</li>
<li><p>请参考VGG论文中的表1构建其他常见模型，如VGG-16或VGG-19。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">vgg_block</span>(<span class="params">num_convs, in_channels, out_channels</span>):</span><br><span class="line">    layers = []</span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(num_convs):</span><br><span class="line">        layers.append(nn.Conv2d(in_channels, out_channels,</span><br><span class="line">                                kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>))</span><br><span class="line">        layers.append(nn.ReLU())</span><br><span class="line"></span><br><span class="line">        in_channels = out_channels</span><br><span class="line">    <span class="keyword">if</span> out_channels &gt;= <span class="number">256</span>:</span><br><span class="line">        layers.append(nn.Conv2d(out_channels, out_channels, kernel_size=<span class="number">1</span>))</span><br><span class="line">        layers.append(nn.ReLU())</span><br><span class="line">    layers.append(nn.MaxPool2d(kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>))</span><br><span class="line">    <span class="keyword">return</span> nn.Sequential(*layers)</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">vgg16</span>(<span class="params">conv_arch</span>):</span><br><span class="line">    conv_blks = []</span><br><span class="line">    in_channels = <span class="number">1</span></span><br><span class="line">    <span class="comment"># 卷积层部分</span></span><br><span class="line">    <span class="keyword">for</span> (num_convs, out_channels) <span class="keyword">in</span> conv_arch:</span><br><span class="line">        conv_blks.append(vgg_block(num_convs, in_channels, out_channels))</span><br><span class="line">        in_channels = out_channels</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> nn.Sequential(</span><br><span class="line">        *conv_blks, nn.Flatten(),</span><br><span class="line">        <span class="comment"># 全连接层部分</span></span><br><span class="line">        nn.Linear(out_channels * <span class="number">7</span> * <span class="number">7</span>, <span class="number">4096</span>), nn.ReLU(), nn.Dropout(<span class="number">0.5</span>),</span><br><span class="line">        nn.Linear(<span class="number">4096</span>, <span class="number">4096</span>), nn.ReLU(), nn.Dropout(<span class="number">0.5</span>),</span><br><span class="line">        nn.Linear(<span class="number">4096</span>, <span class="number">10</span>))</span><br><span class="line">conv_arch = ((<span class="number">2</span>, <span class="number">64</span>), (<span class="number">2</span>, <span class="number">128</span>), (<span class="number">2</span>, <span class="number">256</span>), (<span class="number">2</span>, <span class="number">512</span>), (<span class="number">2</span>, <span class="number">512</span>))</span><br><span class="line">ratio = <span class="number">4</span></span><br><span class="line">small_conv_arch = [(pair[<span class="number">0</span>], pair[<span class="number">1</span>] // ratio) <span class="keyword">for</span> pair <span class="keyword">in</span> conv_arch]</span><br><span class="line">net = vgg16(small_conv_arch)</span><br></pre></td></tr></table></figure>
<h2 id="网络中的网络（NiN）"><a href="#网络中的网络（NiN）" class="headerlink" title="网络中的网络（NiN）"></a>网络中的网络（NiN）</h2><p>LeNet、AlexNet和VGG都有一个共同的设计模式：通过一系列的卷积层与汇聚层来提取空间结构特征；然后通过全连接层对特征的表征进行处理。 AlexNet和VGG对LeNet的改进主要在于如何扩大和加深这两个模块。 或者，可以想象在这个过程的早期使用全连接层。然而，如果使用了全连接层，可能会完全放弃表征的空间结构。 <em>网络中的网络</em>（<em>NiN</em>）提供了一个非常简单的解决方案：在每个像素的通道上分别使用多层感知机</p>
<h3 id="NiN块"><a href="#NiN块" class="headerlink" title="NiN块"></a>NiN块</h3><p>卷积层的输入和输出由四维张量组成，张量的每个轴分别对应样本，通道，高度和宽度。另外全连接额层的输入和输出时对应于样本和特征的二维张量。</p>
</li>
</ol>
<ul>
<li>NiN使用由一个卷积层和多个1×1卷积层组成的块。该块可以在卷积神经网络中使用，以允许更多的每像素非线性。</li>
<li>NiN去除了容易造成过拟合的全连接层，将它们替换为全局平均汇聚层（即在所有位置上进行求和）。该汇聚层通道数量为所需的输出数量（例如，Fashion-MNIST的输出为10）。</li>
<li>移除全连接层可减少过拟合，同时显著减少NiN的参数。</li>
<li>NiN的设计影响了许多后续卷积神经网络的设计。<h3 id="练习-20"><a href="#练习-20" class="headerlink" title="练习"></a>练习</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">nin_block</span>(<span class="params">in_channels, out_channels, kernel_size, strides, padding</span>):</span><br><span class="line">    <span class="keyword">return</span> nn.Sequential(</span><br><span class="line">        nn.Conv2d(in_channels, out_channels, kernel_size, strides, padding),</span><br><span class="line">        nn.ReLU(),</span><br><span class="line">        nn.Conv2d(out_channels, out_channels, kernel_size=<span class="number">1</span>), nn.ReLU(),</span><br><span class="line">        nn.Conv2d(out_channels, out_channels, kernel_size=<span class="number">1</span>), nn.ReLU())</span><br><span class="line"></span><br><span class="line">net = nn.Sequential(</span><br><span class="line">    nin_block(<span class="number">1</span>, <span class="number">96</span>, kernel_size=<span class="number">11</span>, strides=<span class="number">4</span>, padding=<span class="number">0</span>),</span><br><span class="line">    nn.MaxPool2d(<span class="number">3</span>, stride=<span class="number">2</span>),</span><br><span class="line">    nin_block(<span class="number">96</span>, <span class="number">256</span>, kernel_size=<span class="number">5</span>, strides=<span class="number">1</span>, padding=<span class="number">2</span>),</span><br><span class="line">    nn.MaxPool2d(<span class="number">3</span>, stride=<span class="number">2</span>),</span><br><span class="line">    nin_block(<span class="number">256</span>, <span class="number">384</span>, kernel_size=<span class="number">3</span>, strides=<span class="number">1</span>, padding=<span class="number">1</span>),</span><br><span class="line">    nn.MaxPool2d(<span class="number">3</span>, stride=<span class="number">2</span>),</span><br><span class="line">    nn.Dropout(<span class="number">0.5</span>),</span><br><span class="line">    <span class="comment"># 标签类别数是10</span></span><br><span class="line">    nin_block(<span class="number">384</span>, <span class="number">10</span>, kernel_size=<span class="number">3</span>, strides=<span class="number">1</span>, padding=<span class="number">1</span>),</span><br><span class="line">    nn.AdaptiveAvgPool2d((<span class="number">1</span>, <span class="number">1</span>)),</span><br><span class="line">    <span class="comment"># 将四维的输出转成二维的输出，其形状为(批量大小,10)</span></span><br><span class="line">    nn.Flatten())</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h2 id="含并行连结的网络（googleNet）"><a href="#含并行连结的网络（googleNet）" class="headerlink" title="含并行连结的网络（googleNet）"></a>含并行连结的网络（googleNet）</h2><p>在GoogLeNet中，基本的卷积块被称为<em>Inception块</em>（Inception block）。<br>![[Pasted image 20240402145836.png]]<br>inception块由四条并行路径组成，前三条路径使用1 <em> 1，3 </em> 3，5 <em> 5，卷积层，从不同空间大小中提取信息。中间的两条路径在输入上执行1 </em> 1卷积，以减少通道数，降低模型复杂性。第四条路径使用3 <em> 3最大汇聚层，然后使用1 </em> 1卷积层来改变通道数。这四条路径使用合适的填充使输入和输出的高和宽一直，最后我们将每条线路的输出在通道维度上连接，构成Inception块的输出。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.nn <span class="keyword">import</span> functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Inception</span>(nn.Module):</span><br><span class="line">    <span class="comment"># c1--c4是每条路径的输出通道数</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, in_channels, c1, c2, c3, c4, **kwargs</span>):</span><br><span class="line">        <span class="built_in">super</span>(Inception, self).__init__(**kwargs)</span><br><span class="line">        <span class="comment"># 线路1，单1x1卷积层</span></span><br><span class="line">        self.p1_1 = nn.Conv2d(in_channels, c1, kernel_size=<span class="number">1</span>)</span><br><span class="line">        <span class="comment"># 线路2，1x1卷积层后接3x3卷积层</span></span><br><span class="line">        self.p2_1 = nn.Conv2d(in_channels, c2[<span class="number">0</span>], kernel_size=<span class="number">1</span>)</span><br><span class="line">        self.p2_2 = nn.Conv2d(c2[<span class="number">0</span>], c2[<span class="number">1</span>], kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>)</span><br><span class="line">        <span class="comment"># 线路3，1x1卷积层后接5x5卷积层</span></span><br><span class="line">        self.p3_1 = nn.Conv2d(in_channels, c3[<span class="number">0</span>], kernel_size=<span class="number">1</span>)</span><br><span class="line">        self.p3_2 = nn.Conv2d(c3[<span class="number">0</span>], c3[<span class="number">1</span>], kernel_size=<span class="number">5</span>, padding=<span class="number">2</span>)</span><br><span class="line">        <span class="comment"># 线路4，3x3最大汇聚层后接1x1卷积层</span></span><br><span class="line">        self.p4_1 = nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>)</span><br><span class="line">        self.p4_2 = nn.Conv2d(in_channels, c4, kernel_size=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        p1 = F.relu(self.p1_1(x))</span><br><span class="line">        p2 = F.relu(self.p2_2(F.relu(self.p2_1(x))))</span><br><span class="line">        p3 = F.relu(self.p3_2(F.relu(self.p3_1(x))))</span><br><span class="line">        p4 = F.relu(self.p4_2(self.p4_1(x)))</span><br><span class="line">        <span class="comment"># 在通道维度上连结输出</span></span><br><span class="line">        <span class="keyword">return</span> torch.cat((p1, p2, p3, p4), dim=<span class="number">1</span>)</span><br></pre></td></tr></table></figure></p>
<h3 id="googlenet模型"><a href="#googlenet模型" class="headerlink" title="googlenet模型"></a>googlenet模型</h3><p>GoogLeNet一共使用9个Inception块和全局平均汇聚层的堆叠来生成其估计值。Inception块之间的最大汇聚层可降低维度。<br>![[Pasted image 20240402150750.png]]<br>模块逐渐实现：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">b1 = nn.Sequential(nn.Conv2d(<span class="number">1</span>, <span class="number">64</span>, kernel_size=<span class="number">7</span>, stride=<span class="number">2</span>, padding=<span class="number">3</span>),</span><br><span class="line">                   nn.ReLU(),</span><br><span class="line">                   nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>))</span><br></pre></td></tr></table></figure><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">b2 = nn.Sequential(nn.Conv2d(<span class="number">64</span>, <span class="number">64</span>, kernel_size=<span class="number">1</span>),</span><br><span class="line">                   nn.ReLU(),</span><br><span class="line">                   nn.Conv2d(<span class="number">64</span>, <span class="number">192</span>, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>),</span><br><span class="line">                   nn.ReLU(),</span><br><span class="line">                   nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">b3 = nn.Sequential(Inception(<span class="number">192</span>, <span class="number">64</span>, (<span class="number">96</span>, <span class="number">128</span>), (<span class="number">16</span>, <span class="number">32</span>), <span class="number">32</span>),</span><br><span class="line">                   Inception(<span class="number">256</span>, <span class="number">128</span>, (<span class="number">128</span>, <span class="number">192</span>), (<span class="number">32</span>, <span class="number">96</span>), <span class="number">64</span>),</span><br><span class="line">                   nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">b4 = nn.Sequential(Inception(<span class="number">480</span>, <span class="number">192</span>, (<span class="number">96</span>, <span class="number">208</span>), (<span class="number">16</span>, <span class="number">48</span>), <span class="number">64</span>),</span><br><span class="line">                   Inception(<span class="number">512</span>, <span class="number">160</span>, (<span class="number">112</span>, <span class="number">224</span>), (<span class="number">24</span>, <span class="number">64</span>), <span class="number">64</span>),</span><br><span class="line">                   Inception(<span class="number">512</span>, <span class="number">128</span>, (<span class="number">128</span>, <span class="number">256</span>), (<span class="number">24</span>, <span class="number">64</span>), <span class="number">64</span>),</span><br><span class="line">                   Inception(<span class="number">512</span>, <span class="number">112</span>, (<span class="number">144</span>, <span class="number">288</span>), (<span class="number">32</span>, <span class="number">64</span>), <span class="number">64</span>),</span><br><span class="line">                   Inception(<span class="number">528</span>, <span class="number">256</span>, (<span class="number">160</span>, <span class="number">320</span>), (<span class="number">32</span>, <span class="number">128</span>), <span class="number">128</span>),</span><br><span class="line">                   nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">b5 = nn.Sequential(Inception(<span class="number">832</span>, <span class="number">256</span>, (<span class="number">160</span>, <span class="number">320</span>), (<span class="number">32</span>, <span class="number">128</span>), <span class="number">128</span>),</span><br><span class="line">                   Inception(<span class="number">832</span>, <span class="number">384</span>, (<span class="number">192</span>, <span class="number">384</span>), (<span class="number">48</span>, <span class="number">128</span>), <span class="number">128</span>),</span><br><span class="line">                   nn.AdaptiveAvgPool2d((<span class="number">1</span>,<span class="number">1</span>)),</span><br><span class="line">                   nn.Flatten())</span><br><span class="line"></span><br><span class="line">net = nn.Sequential(b1, b2, b3, b4, b5, nn.Linear(<span class="number">1024</span>, <span class="number">10</span>))</span><br></pre></td></tr></table></figure></p>
<h3 id="练习-21"><a href="#练习-21" class="headerlink" title="练习"></a>练习</h3><ol>
<li>使用GoogLeNet的最小图像大小是多少？<br> 224 * 224</li>
<li>将AlexNet、VGG和NiN的模型参数大小与GoogLeNet进行比较。后两个网络架构是如何显著减少模型参数大小的？<br> AlexNet：6000万参数<br> VGG16：1.38亿参数<br> NiN：很少（1 <em> 1卷积）<br> 相比之下，<em>*GoogLeNet</em></em>引入了Inception模块，这些模块可以在不增加太多参数的情况下增加网络的深度和宽度。<h2 id="批量规范化"><a href="#批量规范化" class="headerlink" title="批量规范化"></a>批量规范化</h2>这是一种流行且有效的技术，可持续加速深层网络的收敛速度。结合残差块，可以训练100层以上的网络。<br>在训练神经网络时出现的一些实际挑战：<br>首先，数据预处理的方式会对最终结果产生巨大影响。<br>第二，对于典型的多层感知机或卷积神经网络，当我们训练时，中间层的变量可能有更广的变化范围。不论是沿着从输入到输出的层，跨同一层中的单元，或是随着时间的推移，模型参数会随着训练更新而变换莫测。假设这些变量分布中的这种偏移可能会阻碍网络的收敛，直观来说，如果一个层的可变值是另一个层的100倍，者可能需要对学习率进行补偿调整。<br>第三，更深层的网络很复杂，容易过拟合。这意味着正则化变得重要。</li>
</ol>
<p>批量规范化用于单个可选层（也可以应用到所有的层），原理：在每次训练迭代中，我们首先规范化输入，同伙减去其均值并除以标准差，其中两者均基于当前小批量处理。接下来，我们应用比例系数和比例偏移.正是由于这个基于批量统计的标准化，才有了批量规范化的名称。<br>请注意，如果我们尝试使用大小为1的小批量应用批量规范化，我们将无法学到任何东西，在减去均值后，这个单元将为0。所以，只有使用足够大的小批量，批量规范化才是有效且稳定的。在应用批量规范化时，批量的大小选择可能比没有批量规范化时重要。<br>从形式上来说，用$x$表示来自一个小批量的输入，批量规范化BN根据以下表达式转换x：</p>
<script type="math/tex; mode=display">BN(x)=\gamma \odot \frac{x-\mu_B}{\sigma_B}+\beta</script><h3 id="从零实现"><a href="#从零实现" class="headerlink" title="从零实现"></a>从零实现</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">batch_norm</span>(<span class="params">X, gamma, beta, moving_mean, moving_var, eps, momentum</span>):</span><br><span class="line">    <span class="comment"># 通过is_grad_enabled来判断当前模式是训练模式还是预测模式</span></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> torch.is_grad_enabled():</span><br><span class="line">        <span class="comment"># 如果是在预测模式下，直接使用传入的移动平均所得的均值和方差</span></span><br><span class="line">        X_hat = (X - moving_mean) / torch.sqrt(moving_var + eps)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">assert</span> <span class="built_in">len</span>(X.shape) <span class="keyword">in</span> (<span class="number">2</span>, <span class="number">4</span>)</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">len</span>(X.shape) == <span class="number">2</span>:</span><br><span class="line">            <span class="comment"># 使用全连接层的情况，计算特征维上的均值和方差</span></span><br><span class="line">            mean = X.mean(dim=<span class="number">0</span>)</span><br><span class="line">            var = ((X - mean) ** <span class="number">2</span>).mean(dim=<span class="number">0</span>)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="comment"># 使用二维卷积层的情况，计算通道维上（axis=1）的均值和方差。</span></span><br><span class="line">            <span class="comment"># 这里我们需要保持X的形状以便后面可以做广播运算</span></span><br><span class="line">            mean = X.mean(dim=(<span class="number">0</span>, <span class="number">2</span>, <span class="number">3</span>), keepdim=<span class="literal">True</span>)</span><br><span class="line">            var = ((X - mean) ** <span class="number">2</span>).mean(dim=(<span class="number">0</span>, <span class="number">2</span>, <span class="number">3</span>), keepdim=<span class="literal">True</span>)</span><br><span class="line">        <span class="comment"># 训练模式下，用当前的均值和方差做标准化</span></span><br><span class="line">        X_hat = (X - mean) / torch.sqrt(var + eps)</span><br><span class="line">        <span class="comment"># 更新移动平均的均值和方差</span></span><br><span class="line">        moving_mean = momentum * moving_mean + (<span class="number">1.0</span> - momentum) * mean</span><br><span class="line">        moving_var = momentum * moving_var + (<span class="number">1.0</span> - momentum) * var</span><br><span class="line">    Y = gamma * X_hat + beta  <span class="comment"># 缩放和移位</span></span><br><span class="line">    <span class="keyword">return</span> Y, moving_mean.data, moving_var.data</span><br></pre></td></tr></table></figure>
<h3 id="简明实现"><a href="#简明实现" class="headerlink" title="简明实现"></a>简明实现</h3><p>使用Batchnorm2d(num_feature)<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nn.Conv2d(<span class="number">1</span>, <span class="number">6</span>, kernel_size=<span class="number">5</span>), nn.BatchNorm2d(<span class="number">6</span>), nn.Sigmoid(),</span><br></pre></td></tr></table></figure><br>使用方法为输入上一层通道数即可。</p>
<h3 id="争议"><a href="#争议" class="headerlink" title="争议"></a>争议</h3><p>直观地说，批量规范化被认为可以使优化更加平滑。 然而，我们必须小心区分直觉和对我们观察到的现象的真实解释。 回想一下，我们甚至不知道简单的神经网络（多层感知机和传统的卷积神经网络）为什么如此有效。 即使在暂退法和权重衰减的情况下，它们仍然非常灵活，因此无法通过常规的学习理论泛化保证来解释它们是否能够泛化到看不见的数据。</p>
<h3 id="注："><a href="#注：" class="headerlink" title="注："></a>注：</h3><ul>
<li>批量规范化在全连接层和卷积层的使用略有不同。</li>
<li>批量规范化层和暂退层一样，在训练模式和预测模式下计算不同。<h3 id="练习-22"><a href="#练习-22" class="headerlink" title="练习"></a>练习</h3></li>
</ul>
<ol>
<li><p>在使用批量规范化之前，我们是否可以从全连接层或卷积层中删除偏置参数？为什么？<br> 可以，因为批量规范化的一个作用就是对每个特征通道进行归一化，使得偏置的影响减小。因此，如果我们使用批量规范化，偏置参数的作用可能会变得不那么重要，甚至可以被省略。</p>
</li>
<li><p>我们是否需要在每个层中进行批量规范化？尝试一下？<br> 尝试在每个层中都进行批量规范化可能会导致网络过度约束，使得网络学习能力下降。</p>
</li>
<li>可以通过批量规范化来替换暂退法吗？行为会如何改变？<br> 它们是两种不同的正则化技术，不能简单替换。但可以结合使用。暂退法是在训练过程中随机将一部分神经元的输出置为0，可以减少过拟合问题。批量规范化可以额对每个特征通道的激活值进行归一化，有助于模型加速收敛，减少梯度消失问题。但如果把所有批量归一化转换为暂退法回事模型训练变慢或者不稳定。</li>
<li>查看高级API中有关<code>BatchNorm</code>的在线文档，以查看其他批量规范化的应用。<br> 在PyTorch中，BatchNorm是通过<code>torch.nn.BatchNorm</code>模块实现的。这个模块可以对输入的特征图（feature maps）进行规范化，具体来说，它会计算每个通道的均值和方差，并使用这些统计数据来规范化输入数据。BatchNorm的参数包括<code>num_features</code>（输入特征的数量）、<code>eps</code>（数值稳定性参数）、<code>momentum</code>（动量，用于更新均值和方差的移动平均）、<code>affine</code>（是否进行可学习的缩放和偏移变换）等</li>
<li>研究思路：可以应用的其他“规范化”转换？可以应用概率积分变换吗？全秩协方差估计可以么？<br> 面临复杂度高和对大规模数据不适用的问题，因此在深度学习中并不常见。</li>
</ol>
<h2 id="残差网络"><a href="#残差网络" class="headerlink" title="残差网络"></a>残差网络</h2><h3 id="函数类"><a href="#函数类" class="headerlink" title="函数类"></a>函数类</h3><p>嵌套函数类可以避免创造的新体系偏离原来的近似。<br>残差网络的核心思想是：每个附加层都应该更容易地包含原始函数作为其元素之一。 于是，<em>残差块</em>（residual blocks）便诞生了，这个设计对如何建立深层神经网络产生了深远的影响。</p>
<h3 id="残差块"><a href="#残差块" class="headerlink" title="残差块"></a>残差块</h3><p>要求残差块的输入于输出通道数相同，使得它们可以相加，如果想改变通道数，需要引入一个额外的1 * 1卷积来讲输入变换成需要的形状后在做相加运算。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.nn <span class="keyword">import</span> functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Residual</span>(nn.Module):  <span class="comment">#@save</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, input_channels, num_channels,</span></span><br><span class="line"><span class="params">                 use_1x1conv=<span class="literal">False</span>, strides=<span class="number">1</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.conv1 = nn.Conv2d(input_channels, num_channels,</span><br><span class="line">                               kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>, stride=strides)</span><br><span class="line">        self.conv2 = nn.Conv2d(num_channels, num_channels,</span><br><span class="line">                               kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">if</span> use_1x1conv:</span><br><span class="line">            self.conv3 = nn.Conv2d(input_channels, num_channels,</span><br><span class="line">                                   kernel_size=<span class="number">1</span>, stride=strides)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.conv3 = <span class="literal">None</span></span><br><span class="line">        self.bn1 = nn.BatchNorm2d(num_channels)</span><br><span class="line">        self.bn2 = nn.BatchNorm2d(num_channels)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, X</span>):</span><br><span class="line">        Y = F.relu(self.bn1(self.conv1(X)))</span><br><span class="line">        Y = self.bn2(self.conv2(Y))</span><br><span class="line">        <span class="keyword">if</span> self.conv3:</span><br><span class="line">            X = self.conv3(X)</span><br><span class="line">        Y += X</span><br><span class="line">        <span class="keyword">return</span> F.relu(Y)</span><br></pre></td></tr></table></figure></p>
<h3 id="resnet模型"><a href="#resnet模型" class="headerlink" title="resnet模型"></a>resnet模型</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">b1 = nn.Sequential(nn.Conv2d(<span class="number">1</span>, <span class="number">64</span>, kernel_size=<span class="number">7</span>, stride=<span class="number">2</span>, padding=<span class="number">3</span>),</span><br><span class="line">                   nn.BatchNorm2d(<span class="number">64</span>), nn.ReLU(),</span><br><span class="line">                   nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">resnet_block</span>(<span class="params">input_channels, num_channels, num_residuals,</span></span><br><span class="line"><span class="params">                 first_block=<span class="literal">False</span></span>):</span><br><span class="line">    blk = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(num_residuals):</span><br><span class="line">        <span class="keyword">if</span> i == <span class="number">0</span> <span class="keyword">and</span> <span class="keyword">not</span> first_block:</span><br><span class="line">            blk.append(Residual(input_channels, num_channels,</span><br><span class="line">                                use_1x1conv=<span class="literal">True</span>, strides=<span class="number">2</span>))</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            blk.append(Residual(num_channels, num_channels))</span><br><span class="line">    <span class="keyword">return</span> blk</span><br><span class="line"></span><br><span class="line">b2 = nn.Sequential(*resnet_block(<span class="number">64</span>, <span class="number">64</span>, <span class="number">2</span>, first_block=<span class="literal">True</span>))</span><br><span class="line">b3 = nn.Sequential(*resnet_block(<span class="number">64</span>, <span class="number">128</span>, <span class="number">2</span>))</span><br><span class="line">b4 = nn.Sequential(*resnet_block(<span class="number">128</span>, <span class="number">256</span>, <span class="number">2</span>))</span><br><span class="line">b5 = nn.Sequential(*resnet_block(<span class="number">256</span>, <span class="number">512</span>, <span class="number">2</span>))</span><br></pre></td></tr></table></figure>
<p>最后在Resnet中加入全局平均汇聚层，以及全连接输出。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">net = nn.Sequential(b1,b2,b3,b4,b5,</span><br><span class="line">				   nn.AdaptiveAvgPool2d((<span class="number">1</span>,<span class="number">1</span>)),</span><br><span class="line">				   nn.Flatten(),nn.Linear(<span class="number">512</span>,<span class="number">10</span>))</span><br></pre></td></tr></table></figure><br>将模型每一层输入形状打印出来的方法：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">X = torch.rand(size=(<span class="number">1</span>, <span class="number">1</span>, <span class="number">224</span>, <span class="number">224</span>))</span><br><span class="line"><span class="keyword">for</span> layer <span class="keyword">in</span> net:</span><br><span class="line">    X = layer(X)</span><br><span class="line">    <span class="built_in">print</span>(layer.__class__.__name__,<span class="string">&#x27;output shape:\t&#x27;</span>, X.shape)</span><br></pre></td></tr></table></figure></p>
<h3 id="练习-23"><a href="#练习-23" class="headerlink" title="练习"></a>练习</h3><ol>
<li>Inception块与残差块之间的主要区别是什么？在删除了Inception块中的一些路径之后，它们是如何相互关联的？<br> Inception将输入的特征都进行了处理，最后在通道上拼接。resnet的残差块将输入保留不进行运算（保证用1 * 1卷积使尺寸一致），最后直接相加。即使在简化后的Inception块中，多尺度特征提取的概念仍然存在，这是它与纯残差块的主要区别。在实际应用中，可以根据具体任务的需求和网络设计的考虑，灵活地调整Inception块的结构，以达到最佳的性能和效率。同时，也有研究者尝试将Inception块和残差块结合起来，创造出新的网络结构，以利用两者的优势。</li>
<li>对于更深层次的网络，ResNet引入了“bottleneck”架构来降低模型复杂性。请试着去实现它。<br> 在ResNet中，”bottleneck”架构是为了减少网络中的参数数量和计算复杂性而设计的。这种架构通过引入一个较小的卷积层（通常是1x1卷积）来降低维度，从而在进行较大的卷积操作（如3x3或5x5卷积）之前和之后减少输入和输出的通道数。</li>
<li>在ResNet的后续版本中，作者将“卷积层、批量规范化层和激活层”架构更改为“批量规范化层、激活层和卷积层”架构。请尝试做这个改进。详见resnet论文中的图1。</li>
<li>为什么即使函数类是嵌套的，我们仍然要限制增加函数的复杂性呢？<br> 可读性与可维护性，调试难度，性能影响，递归深度，模块化和重用性，测试难度。</li>
</ol>
<h2 id="稠密连接网络"><a href="#稠密连接网络" class="headerlink" title="稠密连接网络"></a>稠密连接网络</h2><h3 id="从resnet到densenet"><a href="#从resnet到densenet" class="headerlink" title="从resnet到densenet"></a>从resnet到densenet</h3><p>联系泰勒展开式，将一个函数分解成越来越高阶的项，同样将resnet展开为：</p>
<script type="math/tex; mode=display">f(x) = x+g(x)</script><p>一个简单线性项和一个复杂的非线性项，想将f(x)拓展为超过两部分的信息？<br>DenseNet与ResNet的区别在于，Densenet输出是连接，而不是简单相加。<br>![[Pasted image 20240403195132.png]]</p>
<h3 id="稠密块体"><a href="#稠密块体" class="headerlink" title="稠密块体"></a>稠密块体</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">conv_block</span>(<span class="params">input_channels, num_channels</span>):</span><br><span class="line">    <span class="keyword">return</span> nn.Sequential(</span><br><span class="line">        nn.BatchNorm2d(input_channels), nn.ReLU(),</span><br><span class="line">        nn.Conv2d(input_channels, num_channels, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">DenseBlock</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, num_convs, input_channels, num_channels</span>):</span><br><span class="line">        <span class="built_in">super</span>(DenseBlock, self).__init__()</span><br><span class="line">        layer = []</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(num_convs):</span><br><span class="line">            layer.append(conv_block(</span><br><span class="line">                num_channels * i + input_channels, num_channels))</span><br><span class="line">        self.net = nn.Sequential(*layer)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, X</span>):</span><br><span class="line">        <span class="keyword">for</span> blk <span class="keyword">in</span> self.net:</span><br><span class="line">            Y = blk(X)</span><br><span class="line">            <span class="comment"># 连接通道维度上每个块的输入和输出</span></span><br><span class="line">            X = torch.cat((X, Y), dim=<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">return</span> X</span><br></pre></td></tr></table></figure>
<p>ResNet和DenseNet的关键区别在于，DenseNet输出是连接，而不是如ResNet的简单相加。<br>稠密完工罗主要由两部分组成，稠密快，过渡层，前者定义如何连接和输入，而后者控制通道数量，使其不太会复杂。</p>
<h3 id="练习-24"><a href="#练习-24" class="headerlink" title="练习"></a>练习</h3><ol>
<li>DenseNet的优点之一是其模型参数比ResNet小。为什么呢？<ul>
<li>特征重用，每一层输出会作为下一层的输入，意味着网络中的每一层都可以访问到之前所有层的特征图。这种设计显著减少了需要学习的参数数量，因为网络可以重用之前层的特征，而不是每次都从头开始学习新的特征。</li>
<li>参数共享，可以在多个层之间共享权重。</li>
<li>densenet的设计允许在不牺牲性能的情况的下对网络进行压缩，通过一处或合并一些层，可以在保持网络性能的同时减少参数量。</li>
</ul>
</li>
<li>DenseNet一个诟病的问题是内存或显存消耗过多。有另一种方法来减少显存消耗吗？需要改变框架么？<br> DenseNet由于其密集连接的特性，在训练时确实会消耗较多的内存或显存。这是因为在DenseNet中，每一层都需要接收前面所有层的特征图作为输入，这导致特征图数量随着层数的增加而呈平方级增长。<ul>
<li>共享内存分配</li>
<li>压缩特征图（1x1卷积减少特征图通道数）</li>
<li>混合精度训练，将一些变量存储为低精度，可以减少显存消耗。<h1 id="循环神经网络"><a href="#循环神经网络" class="headerlink" title="循环神经网络"></a>循环神经网络</h1><h2 id="序列模型"><a href="#序列模型" class="headerlink" title="序列模型"></a>序列模型</h2><h3 id="马尔可夫模型"><a href="#马尔可夫模型" class="headerlink" title="马尔可夫模型"></a>马尔可夫模型</h3>我们使用当前序列前一段时间内的状态进行估计当前状态。<strong>一阶马尔可夫模型（First-Order Markov Model）</strong>：最简单的马尔可夫模型，它假设下一个状态的概率仅依赖于当前状态。一阶马尔可夫模型通常用状态转移矩阵来描述。<h3 id="练习-25"><a href="#练习-25" class="headerlink" title="练习"></a>练习</h3></li>
</ul>
</li>
<li>一位投资者依赖过去的回报来决定购买哪种证券的策略可能会遇到几个问题：<br> 证券回报可能具有时间序列特性，如季节性、趋势和周期性。单纯依赖历史回报可能无法充分捕捉这些特性，而需要更复杂的时间序列分析方法。</li>
<li>时间是向前推进的因果模型在文本分析中的适用程度有限。文本数据通常是静态的，不具有时间序列数据的动态特性。然而，因果模型可以用于理解文本内容之间的逻辑关系和因果链，例如在法律文件、历史文档或科学论文中分析不同概念和论点之间的联系。</li>
<li>隐变量自回归模型（如隐马尔可夫模型）可能在以下情况下用于捕捉数据的动力学模型：</li>
</ol>
<ul>
<li><strong>序列数据</strong>：当数据表现为一系列状态，并且状态转换可能依赖于之前的状态时，隐变量自回归模型可以用来捕捉状态转换的动态过程。</li>
<li><strong>不完全观测</strong>：当数据中的某些状态或变量无法直接观测到，或者观测到的数据存在噪声时，隐变量模型可以通过观测到的数据来推断隐状态的动态变化。</li>
</ul>
<h2 id="文本预处理"><a href="#文本预处理" class="headerlink" title="文本预处理"></a>文本预处理</h2><h3 id="步骤"><a href="#步骤" class="headerlink" title="步骤"></a>步骤</h3><ol>
<li>将文本作为字符串加载到内存中。</li>
<li>将字符串拆分为词元（如单词和字符）。</li>
<li>建立一个词表，将拆分的词元映射到数字索引。</li>
<li>将文本转换为数字索引序列，方便模型操作。<h3 id="数据集读取与词元化"><a href="#数据集读取与词元化" class="headerlink" title="数据集读取与词元化"></a>数据集读取与词元化</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">d2l.DATA_HUB[<span class="string">&#x27;time_machine&#x27;</span>] = (d2l.DATA_URL + <span class="string">&#x27;timemachine.txt&#x27;</span>,                       <span class="string">&#x27;090b5e7e70c295757f55df93cb0a180b9691891a&#x27;</span>)</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">read_time_machine</span>():  <span class="comment">#@save</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;将时间机器数据集加载到文本行的列表中&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(d2l.download(<span class="string">&#x27;time_machine&#x27;</span>), <span class="string">&#x27;r&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">        lines = f.readlines()</span><br><span class="line">    <span class="keyword">return</span> [re.sub(<span class="string">&#x27;[^A-Za-z]+&#x27;</span>, <span class="string">&#x27; &#x27;</span>, line).strip().lower() <span class="keyword">for</span> line <span class="keyword">in</span> lines]</span><br><span class="line"></span><br><span class="line">lines = read_time_machine()</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;# 文本总行数: <span class="subst">&#123;<span class="built_in">len</span>(lines)&#125;</span>&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(lines[<span class="number">0</span>])</span><br><span class="line"><span class="built_in">print</span>(lines[<span class="number">10</span>])</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">tokenize</span>(<span class="params">lines, token=<span class="string">&#x27;word&#x27;</span></span>):  <span class="comment">#@save</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;将文本行拆分为单词或字符词元&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">if</span> token == <span class="string">&#x27;word&#x27;</span>:</span><br><span class="line">        <span class="keyword">return</span> [line.split() <span class="keyword">for</span> line <span class="keyword">in</span> lines]</span><br><span class="line">    <span class="keyword">elif</span> token == <span class="string">&#x27;char&#x27;</span>:</span><br><span class="line">        <span class="keyword">return</span> [<span class="built_in">list</span>(line) <span class="keyword">for</span> line <span class="keyword">in</span> lines]</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;错误：未知词元类型：&#x27;</span> + token)</span><br><span class="line"></span><br><span class="line">tokens = tokenize(lines)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">11</span>):</span><br><span class="line">    <span class="built_in">print</span>(tokens[i])</span><br></pre></td></tr></table></figure>
<h3 id="词表"><a href="#词表" class="headerlink" title="词表"></a>词表</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Vocab</span>:  <span class="comment">#@save</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;文本词表&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, tokens=<span class="literal">None</span>, min_freq=<span class="number">0</span>, reserved_tokens=<span class="literal">None</span></span>):</span><br><span class="line">        <span class="keyword">if</span> tokens <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            tokens = []</span><br><span class="line">        <span class="keyword">if</span> reserved_tokens <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            reserved_tokens = []</span><br><span class="line">        <span class="comment"># 按出现频率排序</span></span><br><span class="line">        counter = count_corpus(tokens)</span><br><span class="line">        self._token_freqs = <span class="built_in">sorted</span>(counter.items(), key=<span class="keyword">lambda</span> x: x[<span class="number">1</span>],</span><br><span class="line">                                   reverse=<span class="literal">True</span>)</span><br><span class="line">        <span class="comment"># 未知词元的索引为0</span></span><br><span class="line">        self.idx_to_token = [<span class="string">&#x27;&lt;unk&gt;&#x27;</span>] + reserved_tokens</span><br><span class="line">        self.token_to_idx = &#123;token: idx</span><br><span class="line">                             <span class="keyword">for</span> idx, token <span class="keyword">in</span> <span class="built_in">enumerate</span>(self.idx_to_token)&#125;</span><br><span class="line">        <span class="keyword">for</span> token, freq <span class="keyword">in</span> self._token_freqs:</span><br><span class="line">            <span class="keyword">if</span> freq &lt; min_freq:</span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line">            <span class="keyword">if</span> token <span class="keyword">not</span> <span class="keyword">in</span> self.token_to_idx:</span><br><span class="line">                self.idx_to_token.append(token)</span><br><span class="line">                self.token_to_idx[token] = <span class="built_in">len</span>(self.idx_to_token) - <span class="number">1</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(self.idx_to_token)</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__getitem__</span>(<span class="params">self, tokens</span>):</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> <span class="built_in">isinstance</span>(tokens, (<span class="built_in">list</span>, <span class="built_in">tuple</span>)):</span><br><span class="line">            <span class="keyword">return</span> self.token_to_idx.get(tokens, self.unk)</span><br><span class="line">        <span class="keyword">return</span> [self.__getitem__(token) <span class="keyword">for</span> token <span class="keyword">in</span> tokens]</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">to_tokens</span>(<span class="params">self, indices</span>):</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> <span class="built_in">isinstance</span>(indices, (<span class="built_in">list</span>, <span class="built_in">tuple</span>)):</span><br><span class="line">            <span class="keyword">return</span> self.idx_to_token[indices]</span><br><span class="line">        <span class="keyword">return</span> [self.idx_to_token[index] <span class="keyword">for</span> index <span class="keyword">in</span> indices]</span><br><span class="line"><span class="meta">    @property</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">unk</span>(<span class="params">self</span>):  <span class="comment"># 未知词元的索引为0</span></span><br><span class="line">        <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line"><span class="meta">    @property</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">token_freqs</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> self._token_freqs</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">count_corpus</span>(<span class="params">tokens</span>):  <span class="comment">#@save</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;统计词元的频率&quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># 这里的tokens是1D列表或2D列表</span></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">len</span>(tokens) == <span class="number">0</span> <span class="keyword">or</span> <span class="built_in">isinstance</span>(tokens[<span class="number">0</span>], <span class="built_in">list</span>):</span><br><span class="line">        <span class="comment"># 将词元列表展平成一个列表</span></span><br><span class="line">        tokens = [token <span class="keyword">for</span> line <span class="keyword">in</span> tokens <span class="keyword">for</span> token <span class="keyword">in</span> line]</span><br><span class="line">    <span class="keyword">return</span> collections.Counter(tokens)</span><br></pre></td></tr></table></figure>
使用时光机器数据集作为语料库来构建此表，然后打印前几个高频词元及其索引。<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">vocab = Vocab(tokens)</span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">list</span>(vocab.token_to_idx.items())[:<span class="number">10</span>])</span><br></pre></td></tr></table></figure>
</li>
</ol>
<h3 id="功能整合"><a href="#功能整合" class="headerlink" title="功能整合"></a>功能整合</h3><p>在使用上述函数时，我们将所有功能打包到<code>load_corpus_time_machine</code>函数中， 该函数返回<code>corpus</code>（词元索引列表）和<code>vocab</code>（时光机器语料库的词表）。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">load_corpus_time_machine</span>(<span class="params">max_tokens=-<span class="number">1</span></span>):  <span class="comment">#@save</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;返回时光机器数据集的词元索引列表和词表&quot;&quot;&quot;</span></span><br><span class="line">    lines = read_time_machine()</span><br><span class="line">    tokens = tokenize(lines, <span class="string">&#x27;char&#x27;</span>)</span><br><span class="line">    vocab = Vocab(tokens)</span><br><span class="line">    <span class="comment"># 因为时光机器数据集中的每个文本行不一定是一个句子或一个段落，</span></span><br><span class="line">    <span class="comment"># 所以将所有文本行展平到一个列表中</span></span><br><span class="line">    corpus = [vocab[token] <span class="keyword">for</span> line <span class="keyword">in</span> tokens <span class="keyword">for</span> token <span class="keyword">in</span> line]</span><br><span class="line">    <span class="keyword">if</span> max_tokens &gt; <span class="number">0</span>:</span><br><span class="line">        corpus = corpus[:max_tokens]</span><br><span class="line">    <span class="keyword">return</span> corpus, vocab</span><br><span class="line"></span><br><span class="line">corpus, vocab = load_corpus_time_machine()</span><br><span class="line"><span class="built_in">len</span>(corpus), <span class="built_in">len</span>(vocab)</span><br></pre></td></tr></table></figure></p>
<h3 id="练习-26"><a href="#练习-26" class="headerlink" title="练习"></a>练习</h3><ol>
<li>词元化是一个关键的预处理步骤，它因语言而异。尝试找到另外三种常用的词元化文本的方法。</li>
</ol>
<ul>
<li>BPE（Byte-Pair Encoding） WordPiece SentencePiece</li>
</ul>
<ol>
<li>在本节的实验中，将文本词元为单词和更改<code>Vocab</code>实例的<code>min_freq</code>参数。这对词表大小有何影响？ <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">lines = read_time_machine()</span><br><span class="line">tokens = tokenize(lines, <span class="string">&#x27;word&#x27;</span>)</span><br><span class="line">vocab = Vocab(tokens, min_freq=<span class="number">3</span>)</span><br><span class="line">corpus = [vocab[token] <span class="keyword">for</span> line <span class="keyword">in</span> tokens <span class="keyword">for</span> token <span class="keyword">in</span> line]</span><br><span class="line"><span class="keyword">if</span> -<span class="number">1</span> &gt; <span class="number">0</span>:</span><br><span class="line">    corpus = corpus[:-<span class="number">1</span>]</span><br><span class="line"><span class="built_in">len</span>(corpus), <span class="built_in">len</span>(vocab)</span><br></pre></td></tr></table></figure>
词表大小变为(32775, 1420)<h2 id="语言模型和数据集"><a href="#语言模型和数据集" class="headerlink" title="语言模型和数据集"></a>语言模型和数据集</h2>语言模型的目标是估计序列的联合概率<br>只需要抽取一个词元<br>一个理想的语言模型能够基于模型本身生成自然文本。<h3 id="练习-27"><a href="#练习-27" class="headerlink" title="练习"></a>练习</h3></li>
<li>假设训练数据集中有100,000个单词。一个四元语法需要存储多少个词频和相邻多词频率？<br> (N-3) <em> (N-2) </em> (N-1) <em> N词频和(N-3) </em> (N-2) * (N-1) 相邻多词词频</li>
<li>我们如何对一系列对话建模？<br> 将每一段对话视为一个序列数据，用RNN处理这些序列。</li>
<li>一元语法、二元语法和三元语法的齐普夫定律的指数是不一样的，能设法估计么？<br> 齐普夫定律描述了词频与词序之间的幂律关系。对于一元语法、二元语法和三元语法，齐普夫定律的指数可能会有所不同，这取决于语言的特性和语料库的特定性质。估计这些指数通常需要对大量文本数据进行统计分析，通过拟合词频分布的尾部来确定。</li>
<li>想一想读取长序列数据的其他方法？<br> 使用滑动窗口技术，将长序列分割成多个较短的子序列，或者使用分段技术，将序列分成多个部分分别处理。</li>
<li>考虑一下我们用于读取长序列的随机偏移量。<ol>
<li>为什么随机偏移量是个好主意？<br> 它允许模型在每次迭代中从长序列中随机选择一个子序列，从而增加训练过程中的多样性，并可能提高模型的泛化能力。</li>
<li>它真的会在文档的序列上实现完美的均匀分布吗？<br> 可能不会在文档的序列上实现完美的均匀分布，因为某些区域可能会被更频繁地采样。</li>
<li>要怎么做才能使分布更均匀？<br> 限制每个序列的重叠区域，或者使用更复杂的采样策略来引导模型关注序列的不同部分。</li>
</ol>
</li>
<li>如果我们希望一个序列样本是一个完整的句子，那么这在小批量抽样中会带来怎样的问题？如何解决？<br> 一个句子可能被分割成两个或多个小批量，从而破坏了句子的完整性。可以使用桶排序，它将具有相似长度的序列分组到同一个小批量中，以保持句子的完整性。</li>
</ol>
<h2 id="循环神经网络-1"><a href="#循环神经网络-1" class="headerlink" title="循环神经网络"></a>循环神经网络</h2><h3 id="练习-28"><a href="#练习-28" class="headerlink" title="练习"></a>练习</h3><ol>
<li>如果我们使用循环神经网络来预测文本序列中的下一个字符，那么任意输出所需的维度是多少？<br> 等于词汇表的大小</li>
<li>为什么循环神经网络可以基于文本序列中所有先前的词元，在某个时间步表示当前词元的条件概率？<br> 它的设计允许网络在每个实践部接受新的输入并更新其内部状态。这种状态能够捕捉并保留序列中的历史信息，使得网络可以根据先前观察到的词元来预测下一个词元。</li>
<li>如果基于一个长序列进行反向传播，梯度会发生什么状况？<br> 可能会梯度消失或者梯度爆炸。在反向传播时，梯度需要通过序列的每个时间步进行多次链式求导，如果序列很长，这些连续的乘积操作可能导致梯度非常小或非常大，影响网络学习过程。</li>
<li>与本节中描述的语言模型相关的问题有哪些？</li>
</ol>
<ul>
<li>如何选择和设计合适的词汇表，以便更好地捕捉语言的特性。</li>
<li>如何确定最佳的模型架构，包括隐藏层的大小和层数，以及激活函数的选择。</li>
<li>如何处理未知词汇，以及如何平衡词表的大小和模型的泛化能力。</li>
</ul>
<h2 id="循环神经网络的从零开始实现"><a href="#循环神经网络的从零开始实现" class="headerlink" title="循环神经网络的从零开始实现"></a>循环神经网络的从零开始实现</h2><h3 id="独热编码"><a href="#独热编码" class="headerlink" title="独热编码"></a>独热编码</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">F.one_hot(torch.tensor([<span class="number">0</span>,<span class="number">2</span>]),<span class="built_in">len</span>(vocab))</span><br></pre></td></tr></table></figure>
<p>我们每次采样的小批量数据形状是二维张量： （批量大小，时间步数）。 <code>one_hot</code>函数将这样一个小批量数据转换成三维张量， 张量的最后一个维度等于词表大小（<code>len(vocab)</code>）。 我们经常转换输入的维度，以便获得形状为 （时间步数，批量大小，词表大小）的输出。 这将使我们能够更方便地通过最外层的维度， 一步一步地更新小批量数据的隐状态。</p>
<h3 id="初始化模型参数"><a href="#初始化模型参数" class="headerlink" title="初始化模型参数"></a>初始化模型参数</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">get_params</span>(<span class="params">vocab_size, num_hiddens, device</span>):</span><br><span class="line">    num_inputs = num_outputs = vocab_size</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">normal</span>(<span class="params">shape</span>):</span><br><span class="line">        <span class="keyword">return</span> torch.randn(size=shape, device=device) * <span class="number">0.01</span></span><br><span class="line">    <span class="comment"># 隐藏层参数</span></span><br><span class="line">    W_xh = normal((num_inputs, num_hiddens))</span><br><span class="line">    W_hh = normal((num_hiddens, num_hiddens))</span><br><span class="line">    b_h = torch.zeros(num_hiddens, device=device)</span><br><span class="line">    <span class="comment"># 输出层参数</span></span><br><span class="line">    W_hq = normal((num_hiddens, num_outputs))</span><br><span class="line">    b_q = torch.zeros(num_outputs, device=device)</span><br><span class="line">    <span class="comment"># 附加梯度</span></span><br><span class="line">    params = [W_xh, W_hh, b_h, W_hq, b_q]</span><br><span class="line">    <span class="keyword">for</span> param <span class="keyword">in</span> params:</span><br><span class="line">        param.requires_grad_(<span class="literal">True</span>)</span><br><span class="line">    <span class="keyword">return</span> params</span><br></pre></td></tr></table></figure>
<h3 id="网络模型"><a href="#网络模型" class="headerlink" title="网络模型"></a>网络模型</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">init_rnn_state</span>(<span class="params">batch_size, num_hiddens, device</span>):</span><br><span class="line">    <span class="keyword">return</span> (torch.zeros((batch_size, num_hiddens), device=device), )</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">rnn</span>(<span class="params">inputs, state, params</span>):</span><br><span class="line">    <span class="comment"># inputs的形状：(时间步数量，批量大小，词表大小)</span></span><br><span class="line">    W_xh, W_hh, b_h, W_hq, b_q = params</span><br><span class="line">    H, = state</span><br><span class="line">    outputs = []</span><br><span class="line">    <span class="comment"># X的形状：(批量大小，词表大小)</span></span><br><span class="line">    <span class="keyword">for</span> X <span class="keyword">in</span> inputs:</span><br><span class="line">        H = torch.tanh(torch.mm(X, W_xh) + torch.mm(H, W_hh) + b_h)</span><br><span class="line">        Y = torch.mm(H, W_hq) + b_q</span><br><span class="line">        outputs.append(Y)</span><br><span class="line">    <span class="keyword">return</span> torch.cat(outputs, dim=<span class="number">0</span>), (H,)</span><br></pre></td></tr></table></figure>
<p>rnn函数定义了如何在一个时间步内计算因状态和输出。循环神经网络模型通过inputs最外层的维度实现循环，一边逐时间步更新批量数据的隐状态H，此外，这里使用tanh函数作为激活函数。<br>接下来需要创建一个类来包装这些函数：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">RNNModelScratch</span>: <span class="comment">#@save</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;从零开始实现的循环神经网络模型&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, vocab_size, num_hiddens, device,</span></span><br><span class="line"><span class="params">                 get_params, init_state, forward_fn</span>):</span><br><span class="line">        self.vocab_size, self.num_hiddens = vocab_size, num_hiddens</span><br><span class="line">        self.params = get_params(vocab_size, num_hiddens, device)</span><br><span class="line">        self.init_state, self.forward_fn = init_state, forward_fn</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__call__</span>(<span class="params">self, X, state</span>):</span><br><span class="line">        X = F.one_hot(X.T, self.vocab_size).<span class="built_in">type</span>(torch.float32)</span><br><span class="line">        <span class="keyword">return</span> self.forward_fn(X, state, self.params)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">begin_state</span>(<span class="params">self, batch_size, device</span>):</span><br><span class="line">        <span class="keyword">return</span> self.init_state(batch_size, self.num_hiddens, device)</span><br></pre></td></tr></table></figure></p>
<h3 id="训练-1"><a href="#训练-1" class="headerlink" title="训练"></a>训练</h3><ol>
<li>序列数据的不同采样方法（随机采样和顺序分区）将导致隐状态初始化的差异。</li>
<li>我们在更新模型参数之前裁剪梯度。 这样的操作的目的是，即使训练过程中某个点上发生了梯度爆炸，也能保证模型不会发散。</li>
<li>我们用困惑度来评价模型。所述， 这样的度量确保了不同长度的序列具有可比性。<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#@save</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">train_epoch_ch8</span>(<span class="params">net, train_iter, loss, updater, device, use_random_iter</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;训练网络一个迭代周期（定义见第8章）&quot;&quot;&quot;</span></span><br><span class="line">    state, timer = <span class="literal">None</span>, d2l.Timer()</span><br><span class="line">    metric = d2l.Accumulator(<span class="number">2</span>)  <span class="comment"># 训练损失之和,词元数量</span></span><br><span class="line">    <span class="keyword">for</span> X, Y <span class="keyword">in</span> train_iter:</span><br><span class="line">        <span class="keyword">if</span> state <span class="keyword">is</span> <span class="literal">None</span> <span class="keyword">or</span> use_random_iter:</span><br><span class="line">            <span class="comment"># 在第一次迭代或使用随机抽样时初始化state</span></span><br><span class="line">            state = net.begin_state(batch_size=X.shape[<span class="number">0</span>], device=device)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">isinstance</span>(net, nn.Module) <span class="keyword">and</span> <span class="keyword">not</span> <span class="built_in">isinstance</span>(state, <span class="built_in">tuple</span>):</span><br><span class="line">                <span class="comment"># state对于nn.GRU是个张量</span></span><br><span class="line">                state.detach_()</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="comment"># state对于nn.LSTM或对于我们从零开始实现的模型是个张量</span></span><br><span class="line">                <span class="keyword">for</span> s <span class="keyword">in</span> state:</span><br><span class="line">                    s.detach_()</span><br><span class="line">        y = Y.T.reshape(-<span class="number">1</span>)</span><br><span class="line">        X, y = X.to(device), y.to(device)</span><br><span class="line">        y_hat, state = net(X, state)</span><br><span class="line">        l = loss(y_hat, y.long()).mean()</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">isinstance</span>(updater, torch.optim.Optimizer):</span><br><span class="line">            updater.zero_grad()</span><br><span class="line">            l.backward()</span><br><span class="line">            grad_clipping(net, <span class="number">1</span>)</span><br><span class="line">            updater.step()</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            l.backward()</span><br><span class="line">            grad_clipping(net, <span class="number">1</span>)</span><br><span class="line">            updater(batch_size=<span class="number">1</span>)</span><br><span class="line">        metric.add(l * y.numel(), y.numel())</span><br><span class="line">    <span class="keyword">return</span> math.exp(metric[<span class="number">0</span>] / metric[<span class="number">1</span>]), metric[<span class="number">1</span>] / timer.stop()</span><br></pre></td></tr></table></figure>
</li>
</ol>
<h3 id="训练和预测"><a href="#训练和预测" class="headerlink" title="训练和预测"></a>训练和预测</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#@save</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">train_ch8</span>(<span class="params">net, train_iter, vocab, lr, num_epochs, device,</span></span><br><span class="line"><span class="params">              use_random_iter=<span class="literal">False</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;训练模型（定义见第8章）&quot;&quot;&quot;</span></span><br><span class="line">    loss = nn.CrossEntropyLoss()</span><br><span class="line">    animator = d2l.Animator(xlabel=<span class="string">&#x27;epoch&#x27;</span>, ylabel=<span class="string">&#x27;perplexity&#x27;</span>,</span><br><span class="line">                            legend=[<span class="string">&#x27;train&#x27;</span>], xlim=[<span class="number">10</span>, num_epochs])</span><br><span class="line">    <span class="comment"># 初始化</span></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">isinstance</span>(net, nn.Module):</span><br><span class="line">        updater = torch.optim.SGD(net.parameters(), lr)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        updater = <span class="keyword">lambda</span> batch_size: d2l.sgd(net.params, lr, batch_size)</span><br><span class="line">    predict = <span class="keyword">lambda</span> prefix: predict_ch8(prefix, <span class="number">50</span>, net, vocab, device)</span><br><span class="line">    <span class="comment"># 训练和预测</span></span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epochs):</span><br><span class="line">        ppl, speed = train_epoch_ch8(</span><br><span class="line">            net, train_iter, loss, updater, device, use_random_iter)</span><br><span class="line">        <span class="keyword">if</span> (epoch + <span class="number">1</span>) % <span class="number">10</span> == <span class="number">0</span>:</span><br><span class="line">            <span class="built_in">print</span>(predict(<span class="string">&#x27;time traveller&#x27;</span>))</span><br><span class="line">            animator.add(epoch + <span class="number">1</span>, [ppl])</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;困惑度 <span class="subst">&#123;ppl:<span class="number">.1</span>f&#125;</span>, <span class="subst">&#123;speed:<span class="number">.1</span>f&#125;</span> 词元/秒 <span class="subst">&#123;<span class="built_in">str</span>(device)&#125;</span>&#x27;</span>)</span><br><span class="line">    <span class="built_in">print</span>(predict(<span class="string">&#x27;time traveller&#x27;</span>))</span><br><span class="line">    <span class="built_in">print</span>(predict(<span class="string">&#x27;traveller&#x27;</span>))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">num_epochs, lr = <span class="number">500</span>, <span class="number">1</span></span><br><span class="line">train_ch8(net, train_iter, vocab, lr, num_epochs, d2l.try_gpu())</span><br></pre></td></tr></table></figure>
<h3 id="练习-29"><a href="#练习-29" class="headerlink" title="练习"></a>练习</h3><ol>
<li>尝试说明独热编码等价于为每个对象选择不同的嵌入表示。<br> 在文本处理中，每个词元（如单词或字符）都会被分配一个唯一的索引，独热编码就是根据这个索引创建一个与词汇表大小相同的向量，其中对应索引位置的元素为1，其余为0。这种表示方法等价于为每个对象选择一个不同的嵌入向量，其中嵌入向量是预先定义好的，而不是通过学习得到的。</li>
<li>在不裁剪梯度的情况下运行本节中的代码会发生什么？<br> 可能会导致数值不稳定，如梯度爆炸，这会影响模型的训练过程和最终性能。</li>
<li>更改顺序划分，使其不会从计算图中分离隐状态。运行时间会有变化吗？困惑度呢？<br> 可能会影响模型的训练效率和稳定性。这种改变可能会使隐状态在每次迭代中保持连续，从而有助于梯度的传播和模型的学习。</li>
<li>用ReLU替换本节中使用的激活函数，并重复本节中的实验。我们还需要梯度裁剪吗？为什么？<br> 可能会影响模型的性能。ReLU激活函数在正区间内保持线性，而在负区间内输出为0。这种特性使得ReLU在正向传播时能够保持梯度不衰减，有助于缓解梯度消失问题。在某些情况下，使用ReLU可以减少对梯度裁剪的需求，因为它相对于其他激活函数（如sigmoid或tanh）在正区间内具有恒定的梯度。</li>
</ol>
<h3 id="循环神经网络的简介实现"><a href="#循环神经网络的简介实现" class="headerlink" title="循环神经网络的简介实现"></a>循环神经网络的简介实现</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.nn <span class="keyword">import</span> functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br><span class="line"></span><br><span class="line">batch_size, num_steps = <span class="number">32</span>, <span class="number">35</span></span><br><span class="line">train_iter, vocab = d2l.load_data_time_machine(batch_size, num_steps)</span><br></pre></td></tr></table></figure>
<h3 id="模型定义"><a href="#模型定义" class="headerlink" title="模型定义"></a>模型定义</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">num_hiddens = <span class="number">256</span></span><br><span class="line">rnn_layer = nn.RNN(<span class="built_in">len</span>(vocab), num_hiddens)</span><br><span class="line"><span class="comment"># 我们使用张量来初始化隐状态，它的形状是（隐藏层数，批量大小，隐藏单元数）。</span></span><br><span class="line">state = torch.zeros((<span class="number">1</span>, batch_size, num_hiddens))</span><br><span class="line">X = torch.rand(size=(num_steps, batch_size, <span class="built_in">len</span>(vocab)))</span><br><span class="line">Y, state_new = rnn_layer(X, state)</span><br></pre></td></tr></table></figure>
<h3 id="定义一个RNN类"><a href="#定义一个RNN类" class="headerlink" title="定义一个RNN类"></a>定义一个RNN类</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#@save</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">RNNModel</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;循环神经网络模型&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, rnn_layer, vocab_size, **kwargs</span>):</span><br><span class="line">        <span class="built_in">super</span>(RNNModel, self).__init__(**kwargs)</span><br><span class="line">        self.rnn = rnn_layer</span><br><span class="line">        self.vocab_size = vocab_size</span><br><span class="line">        self.num_hiddens = self.rnn.hidden_size</span><br><span class="line">        <span class="comment"># 如果RNN是双向的（之后将介绍），num_directions应该是2，否则应该是1</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> self.rnn.bidirectional:</span><br><span class="line">            self.num_directions = <span class="number">1</span></span><br><span class="line">            self.linear = nn.Linear(self.num_hiddens, self.vocab_size)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.num_directions = <span class="number">2</span></span><br><span class="line">            self.linear = nn.Linear(self.num_hiddens * <span class="number">2</span>, self.vocab_size)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, inputs, state</span>):</span><br><span class="line">        X = F.one_hot(inputs.T.long(), self.vocab_size)</span><br><span class="line">        X = X.to(torch.float32)</span><br><span class="line">        Y, state = self.rnn(X, state)</span><br><span class="line">        <span class="comment"># 全连接层首先将Y的形状改为(时间步数*批量大小,隐藏单元数)</span></span><br><span class="line">        <span class="comment"># 它的输出形状是(时间步数*批量大小,词表大小)。</span></span><br><span class="line">        output = self.linear(Y.reshape((-<span class="number">1</span>, Y.shape[-<span class="number">1</span>])))</span><br><span class="line">        <span class="keyword">return</span> output, state</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">begin_state</span>(<span class="params">self, device, batch_size=<span class="number">1</span></span>):</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> <span class="built_in">isinstance</span>(self.rnn, nn.LSTM):</span><br><span class="line">            <span class="comment"># nn.GRU以张量作为隐状态</span></span><br><span class="line">            <span class="keyword">return</span>  torch.zeros((self.num_directions * self.rnn.num_layers,batch_size, self.num_hiddens),</span><br><span class="line">                                device=device)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="comment"># nn.LSTM以元组作为隐状态</span></span><br><span class="line">            <span class="keyword">return</span> (torch.zeros((</span><br><span class="line">                self.num_directions * self.rnn.num_layers,</span><br><span class="line">                batch_size, self.num_hiddens),device=device),</span><br><span class="line">                    torch.zeros((</span><br><span class="line">                        self.num_directions *self.rnn.num_layers,batch_size, self.num_hiddens), device=device))</span><br></pre></td></tr></table></figure>
<h3 id="训练与预测"><a href="#训练与预测" class="headerlink" title="训练与预测"></a>训练与预测</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">device = d2l.try_gpu()</span><br><span class="line">net = RNNModel(rnn_layer, vocab_size=<span class="built_in">len</span>(vocab))</span><br><span class="line">net = net.to(device)</span><br><span class="line">d2l.predict_ch8(<span class="string">&#x27;time traveller&#x27;</span>, <span class="number">10</span>, net, vocab, device)</span><br></pre></td></tr></table></figure>
<h3 id="练习-30"><a href="#练习-30" class="headerlink" title="练习"></a>练习</h3><ol>
<li>尝试使用高级API，能使循环神经网络模型过拟合吗？<br> 尤其是当数据集相对较小或者模型过于复杂时。高级API通常提供了更多的模型配置选项和优化技术，如正则化、dropout等，这些技术可以帮助减少过拟合的风险。然而，如果不正确使用这些技术，或者在训练过程中没有适当的验证和调整，模型仍然可能会在训练数据上过度拟合，导致泛化能力下降。</li>
<li>如果在循环神经网络模型中增加隐藏层的数量会发生什么？能使模型正常工作吗？<br> 可以增加模型的复杂度和学习能力，这在处理复杂任务时可能是有益的。然而，增加隐藏层的数量也可能导致几个问题：首先，它可能会增加模型训练的难度，因为更多的参数需要被优化；其次，它可能会增加过拟合的风险，因为模型可能会学习到数据中的噪声而不是潜在的模式。为了使增加隐藏层的模型正常工作，需要仔细调整超参数，可能还需要更多的训练数据和更复杂的正则化技术。</li>
</ol>
<h1 id="现代循环神经网络"><a href="#现代循环神经网络" class="headerlink" title="现代循环神经网络"></a>现代循环神经网络</h1><h2 id="门控循环单元（GRU）"><a href="#门控循环单元（GRU）" class="headerlink" title="门控循环单元（GRU）"></a>门控循环单元（GRU）</h2><h3 id="练习-31"><a href="#练习-31" class="headerlink" title="练习"></a>练习</h3><ol>
<li>假设我们只想使用时间步t′的输入来预测时间步t&gt;t′的输出。对于每个时间步，重置门和更新门的最佳值是什么？</li>
</ol>
<ul>
<li>重置门：rt​ 应该接近0，这表示我们不希望将过去的隐藏状态信息融入到当前的候选隐藏状态中。</li>
<li>更新门：zt​ 应该接近1，这表示我们希望保留大部分的当前输入信息，而不是过去的隐藏状态。</li>
</ul>
<ol>
<li>调整和分析超参数对运行时间、困惑度和输出顺序的影响。</li>
</ol>
<ul>
<li>学习率：较高的学习率可能导致模型快速收敛，但也可能导致在最优解附近的震荡或发散。较低的学习率则可能导致模型收敛速度缓慢。</li>
<li>隐藏层大小：较大的隐藏层可以捕捉更复杂的特征，但可能导致过拟合和更长的训练时间。较小的隐藏层可能无法捕捉所有必要的信息，导致欠拟合。</li>
<li>批次大小：较大的批次可以提供更稳定的梯度估计，但需要更多的内存，并且可能需要更多的迭代来训练模型。较小的批次可能需要更多的迭代，但每次迭代的计算量较小。</li>
<li>迭代次数：更多的迭代次数可以给模型更多的时间来学习数据，但也可能导致过拟合和不必要的计算。较少的迭代次数可能导致模型未能充分学习。</li>
</ul>
<ol>
<li>比较<code>rnn.RNN</code>和<code>rnn.GRU</code>的不同实现对运行时间、困惑度和输出字符串的影响。</li>
</ol>
<ul>
<li>运行时间：GRU通常具有更少的参数和更简单的结构，这可能导致在某些情况下训练速度更快。</li>
<li>困惑度：GRU由于其门控机制，通常能够更好地捕捉长距离依赖关系，这可能导致较低的困惑度和更好的模型性能。</li>
<li>输出字符串：GRU的输出可能更加流畅和连贯，因为它能够更有效地避免梯度消失问题，从而在长序列中保持信息的传递。</li>
</ul>
<ol>
<li>如果仅仅实现门控循环单元的一部分，例如，只有一个重置门或一个更新门会怎样？<br> 没有更新门，模型可能无法有效地决定何时应该更新其隐藏状态；没有重置门，模型可能无法决定何时应该忘记过去的信息。这可能导致模型无法捕捉到序列中的重要动态，从而影响其性能和预测能力。<h2 id="长短期记忆网络（LSTM）"><a href="#长短期记忆网络（LSTM）" class="headerlink" title="长短期记忆网络（LSTM）"></a>长短期记忆网络（LSTM）</h2>LSTM的每个单元包含以下几个关键的组成部分：</li>
<li><strong>遗忘门（Forget Gate）</strong>：决定哪些信息应该被丢弃或保留。它通过一个sigmoid函数来输出一个介于0到1之间的值，0表示完全遗忘，而1表示完全保留。</li>
<li><strong>输入门（Input Gate）</strong>：决定哪些新的信息应该被添加到细胞状态中。它由两部分组成：一个sigmoid层决定哪些值我们将要更新，和一个tanh层创建一个新的候选值向量，这些值将会被加入到状态中。</li>
<li><strong>细胞状态（Cell State）</strong>：是LSTM的核心，它在整个序列中传递相关的信息。细胞状态的更新是通过遗忘门和输入门的组合来实现的。</li>
<li><strong>输出门（Output Gate）</strong>：基于当前的细胞状态和输入，决定最终的输出。输出门的输出通过tanh函数处理，并将结果与sigmoid门的输出相乘，以决定最终输出的活跃度。<br>LSTM的这些门控机制使得网络能够有选择性地保留或遗忘信息，从而有效地处理长期依赖关系。这使得LSTM在许多序列数据任务中，如语言模型、机器翻译、语音识别等领域，都取得了显著的成功。<br>![[Pasted image 20240406190204.png]]<h3 id="练习-32"><a href="#练习-32" class="headerlink" title="练习"></a>练习</h3></li>
<li>调整和分析超参数对运行时间、困惑度和输出顺序的影响。<br> 增加隐藏层的神经元数量可能会导致模型的训练时间增加，因为需要更多的时间来计算和更新更多的权重。同时，这也可能影响模型的困惑度，即模型对测试数据的不确定性的度量。如果模型过于复杂，可能会导致过拟合，从而在训练数据上表现良好但在测试数据上表现不佳，从而增加困惑度。</li>
<li>如何更改模型以生成适当的单词，而不是字符序列？<br> 可以通过更改模型的输出层和训练数据来实现。<br> 首先，需要将文本数据预处理为单词级别的表示，而不是字符级别的表示。这意味着需要构建一个单词到整数的映射，并在训练数据中将每个单词转换为相应的整数序列。然后，模型的输出层应该设计为预测下一个单词的概率分布，而不是下一个字符。这通常通过在RNN的顶部添加一个全连接层来实现，该层的输出维度等于词汇表的大小。在训练过程中，使用单词级别的标签（即正确的下一个单词）作为目标，而不是字符级别的标签。</li>
<li>在给定隐藏层维度的情况下，比较门控循环单元、长短期记忆网络和常规循环神经网络的计算成本。要特别注意训练和推断成本。<br> 门控循环单元（GRU）和长短期记忆网络（LSTM）都是为了解决常规循环神经网络（RNN）在处理长序列时遇到的梯度消失问题而设计的。在隐藏层维度给定的情况下，LSTM和GRU通常比常规RNN具有更多的参数和更复杂的结构，因为它们引入了门控机制来控制信息的流动。这可能会导致它们的训练成本高于常规RNN，因为需要更多的计算资源来更新和优化这些额外的参数。然而，这种增加的计算成本可能会通过减少训练时间（因为模型能够更快地学习长距离依赖关系）和提高模型性能来补偿。在推断阶段，LSTM和GRU可能会比常规RNN慢，因为门控机制需要额外的计算来决定信息的保留和遗忘。</li>
<li>既然候选记忆元通过使用tanh函数来确保值范围在(−1,1)之间，那么为什么隐状态需要再次使用tanh函数来确保输出值范围在(−1,1)之间呢？<br> 在LSTM中，候选记忆元是通过tanh函数处理的，确保其值范围在[-1, 1]之间。然而，LSTM的隐状态是记忆元的一个加权和，其中包括旧的隐状态和新的候选记忆元。隐状态需要通过tanh函数处理，以确保其值范围在[0, 1]之间，这是因为隐状态在LSTM中充当着网络的“记忆”，并且需要反映为一个概率分布，表示信息的重要性和应该被保留的程度。tanh函数的输出范围与LSTM的设计目标相匹配，即保留重要的信息并遗忘不重要的信息。</li>
<li>实现一个能够基于时间序列进行预测而不是基于字符序列进行预测的长短期记忆网络模型。<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"><span class="comment"># x_train的形状应该是（样本数量，时间步长，特征数量）</span></span><br><span class="line"><span class="comment"># y_train的形状应该是（样本数量，输出长度）</span></span><br><span class="line"><span class="comment"># 定义LSTM模型</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">LSTMModel</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, input_size, hidden_size, num_layers, output_size</span>):</span><br><span class="line">        <span class="built_in">super</span>(LSTMModel, self).__init__()</span><br><span class="line">        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=<span class="literal">True</span>)</span><br><span class="line">        self.fc = nn.Linear(hidden_size, output_size)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        lstm_out, (hidden, cell) = self.lstm(x)</span><br><span class="line">        <span class="comment"># 只使用最后一个时间步的输出</span></span><br><span class="line">        output = self.fc(lstm_out[:, -<span class="number">1</span>, :])</span><br><span class="line">        <span class="keyword">return</span> output</span><br><span class="line"><span class="comment"># 设定超参数</span></span><br><span class="line">input_size = <span class="number">4</span>  <span class="comment"># 特征数量</span></span><br><span class="line">hidden_size = <span class="number">50</span>  <span class="comment"># LSTM隐藏层大小</span></span><br><span class="line">num_layers = <span class="number">1</span>  <span class="comment"># LSTM层数</span></span><br><span class="line">output_size = <span class="number">1</span>  <span class="comment"># 输出长度，对于回归问题通常为1</span></span><br><span class="line">learning_rate = <span class="number">0.01</span></span><br><span class="line">batch_size = <span class="number">32</span></span><br><span class="line">num_epochs = <span class="number">100</span></span><br><span class="line"><span class="comment"># 创建模型实例</span></span><br><span class="line">model = LSTMModel(input_size, hidden_size, num_layers, output_size)</span><br><span class="line"><span class="comment"># 损失函数和优化器</span></span><br><span class="line">criterion = nn.MSELoss()</span><br><span class="line">optimizer = optim.Adam(model.parameters(), lr=learning_rate)</span><br><span class="line"><span class="comment"># 训练模型</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epochs):</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, <span class="built_in">len</span>(x_train), batch_size):</span><br><span class="line">        batch_x = x_train[i:i+batch_size]</span><br><span class="line">        batch_y = y_train[i:i+batch_size]</span><br><span class="line">        inputs = torch.tensor(batch_x, dtype=torch.float32)</span><br><span class="line">        targets = torch.tensor(batch_y, dtype=torch.float32)</span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        outputs = model(inputs)</span><br><span class="line">        loss = criterion(outputs, targets)</span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line">    <span class="keyword">if</span> (epoch+<span class="number">1</span>) % <span class="number">10</span> == <span class="number">0</span>:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&#x27;Epoch [<span class="subst">&#123;epoch+<span class="number">1</span>&#125;</span>/<span class="subst">&#123;num_epochs&#125;</span>], Loss: <span class="subst">&#123;loss.item()&#125;</span>&#x27;</span>)</span><br><span class="line"><span class="comment"># 测试模型</span></span><br><span class="line"><span class="comment"># 假设x_test是你要进行预测的时间序列数据</span></span><br><span class="line">x_test = np.random.rand(<span class="number">1</span>, <span class="number">1</span>, <span class="number">4</span>)</span><br><span class="line">inputs = torch.tensor(x_test, dtype=torch.float32)</span><br><span class="line">predicted = model(inputs)</span><br><span class="line"><span class="built_in">print</span>(predicted)</span><br></pre></td></tr></table></figure>
</li>
</ol>
<h2 id="深度循环神经网络"><a href="#深度循环神经网络" class="headerlink" title="深度循环神经网络"></a>深度循环神经网络</h2><ol>
<li>尝试从零开始实现两层循环神经网络。<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">TwoLayerLSTM</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, input_size, hidden_size, num_layers, output_size</span>):</span><br><span class="line">        <span class="built_in">super</span>(TwoLayerLSTM, self).__init__()</span><br><span class="line">        self.hidden_size = hidden_size</span><br><span class="line">        self.num_layers = num_layers</span><br><span class="line">        self.lstm_layers = nn.ModuleList([nn.LSTM(input_size, hidden_size, num_layers, batch_first=<span class="literal">True</span>) <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">2</span>)])</span><br><span class="line">        self.output_layer = nn.Linear(hidden_size, output_size)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        lstm_out, (hidden, cell) = self.lstm_layers[<span class="number">0</span>](x)</span><br><span class="line">        lstm_out, (hidden, cell) = self.lstm_layers[<span class="number">1</span>](lstm_out)</span><br><span class="line">        output = self.output_layer(lstm_out[:, -<span class="number">1</span>, :])</span><br><span class="line">        <span class="keyword">return</span> output, (hidden, cell)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 假设输入数据的维度为10，隐藏层大小为20，输出大小为1</span></span><br><span class="line">model = TwoLayerLSTM(input_size=<span class="number">10</span>, hidden_size=<span class="number">20</span>, num_layers=<span class="number">2</span>, output_size=<span class="number">1</span>)</span><br></pre></td></tr></table></figure></li>
<li>在本节训练模型中，比较使用门控循环单元替换长短期记忆网络后模型的精确度和训练速度。<br> 在实际应用中，GRU通常比LSTM有更快的训练速度，因为GRU的门控机制相对简单，参数数量较少。然而，LSTM通常在处理长期依赖关系方面表现得更好，因为它具有更复杂的门控机制。在比较两者的精确度和训练速度时，需要根据具体任务和数据集进行实验。一般来说，如果任务对长期依赖关系的要求不高，GRU可能是一个更快、更高效的选择。如果任务需要捕捉复杂的长期依赖关系，LSTM可能是更好的选择。</li>
<li>如果增加训练数据，能够将困惑度降到多低？<br> 增加训练数据通常有助于提高模型的性能，包括降低困惑度。困惑度是衡量模型对测试数据的不确定性的指标，较低的困惑度意味着模型对数据的预测更准确。理论上，随着训练数据的增加，模型可以学习到更多的模式和依赖关系，从而提高其对测试数据的预测能力。然而，降低困惑度的幅度也受到数据质量、模型容量和训练策略等因素的影响。</li>
<li>在为文本建模时，是否可以将不同作者的源数据合并？有何优劣呢？<br> 可以增加数据的多样性和丰富性，有助于提高模型的泛化能力。然而，不同作者的写作风格和用词习惯可能存在显著差异，这可能会影响模型的训练效果。</li>
</ol>
<h2 id="双向循环神经网络"><a href="#双向循环神经网络" class="headerlink" title="双向循环神经网络"></a>双向循环神经网络</h2><h3 id="练习-33"><a href="#练习-33" class="headerlink" title="练习"></a>练习</h3><ol>
<li>设计一个具有多个隐藏层的双向循环神经网络。<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MultiLayerBiLSTM</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, input_size, hidden_size, num_layers, output_size</span>):</span><br><span class="line">        <span class="built_in">super</span>(MultiLayerBiLSTM, self).__init__()</span><br><span class="line">        self.hidden_size = hidden_size</span><br><span class="line">        self.num_layers = num_layers</span><br><span class="line">        self.bi_lstm = nn.ModuleList([</span><br><span class="line">            nn.LSTM(input_size, hidden_size, num_layers, batch_first=<span class="literal">True</span>, bidirectional=<span class="literal">True</span>)</span><br><span class="line">            <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(num_layers)</span><br><span class="line">        ])</span><br><span class="line">        self.output_layer = nn.Linear(hidden_size * <span class="number">2</span>, output_size)  <span class="comment"># 双向输出需要合并</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        outputs = []</span><br><span class="line">        <span class="keyword">for</span> lstm <span class="keyword">in</span> self.bi_lstm:</span><br><span class="line">            lstm_out, (hidden, cell) = lstm(x)</span><br><span class="line">            outputs.append(lstm_out)</span><br><span class="line">            x = lstm_out</span><br><span class="line">        <span class="comment"># 合并所有隐藏层的输出</span></span><br><span class="line">        combined_output = torch.cat(outputs, dim=-<span class="number">1</span>)</span><br><span class="line">        output = self.output_layer(combined_output[:, -<span class="number">1</span>, :])</span><br><span class="line">        <span class="keyword">return</span> output</span><br><span class="line"></span><br><span class="line"><span class="comment"># 假设输入数据的维度为10，隐藏层大小为20，输出大小为1</span></span><br><span class="line">model = MultiLayerBiLSTM(input_size=<span class="number">10</span>, hidden_size=<span class="number">20</span>, num_layers=<span class="number">2</span>, output_size=<span class="number">1</span>)</span><br></pre></td></tr></table></figure></li>
<li>在自然语言中一词多义很常见。例如，“bank”一词在不同的上下文“i went to the bank to deposit cash”和“i went to the bank to sit down”中有不同的含义。如何设计一个神经网络模型，使其在给定上下文序列和单词的情况下，返回该单词在此上下文中的向量表示？哪种类型的神经网络架构更适合处理一词多义？<br> 可以使用编码器-解码器（Encoder-Decoder）架构，其中编码器负责生成上下文的表示，解码器则利用这个表示来生成目标单词的向量。<br> 模型设计：<ul>
<li><strong>上下文编码器</strong>：使用Bi-LSTM或Transformer架构来编码输入句子的上下文信息。这种双向结构能够捕捉到每个词前后的依赖关系，从而为每个词生成一个包含上下文信息的向量表示。</li>
<li><strong>注意力机制</strong>：在解码器中引入注意力机制，它可以动态地聚焦于编码器输出的上下文表示中最相关的部分。这样，模型可以根据当前处理的单词来调整其对上下文的关注点。</li>
<li><strong>解码器</strong>：解码器可以是另一个LSTM或GRU层，它使用注意力加权的上下文向量来生成目标单词的表示。</li>
</ul>
</li>
</ol>
<h2 id="机器翻译与数据集"><a href="#机器翻译与数据集" class="headerlink" title="机器翻译与数据集"></a>机器翻译与数据集</h2><h3 id="练习-34"><a href="#练习-34" class="headerlink" title="练习"></a>练习</h3><ol>
<li>在<code>load_data_nmt</code>函数中尝试不同的<code>num_examples</code>参数值。这对源语言和目标语言的词表大小有何影响？<br> 在<code>load_data_nmt</code>函数中尝试不同的<code>num_examples</code>参数值会影响源语言和目标语言的词表大小。<code>num_examples</code>参数决定了从数据集中抽取多少个样本来构建词表。如果<code>num_examples</code>的值较小，那么词表可能会不够全面，因为它只考虑了较少的样本。这可能导致一些单词没有被包含在词表中，特别是那些较少出现的单词。相反，如果<code>num_examples</code>的值较大，词表将更加全面，因为它考虑了更多的样本，从而可能包含更多的单词。然而，一个非常大的<code>num_examples</code>值也可能导致词表过于庞大，包含许多在特定任务中并不重要的单词。</li>
<li>某些语言（例如中文和日语）的文本没有单词边界指示符（例如空格）。对于这种情况，单词级词元化仍然是个好主意吗？为什么？<br> 子词级词元化可以帮助模型更好地理解文本的结构，同时避免了将单词错误地分割的问题。此外，使用基于词典或基于统计的分词方法可以有效地处理这些语言的文本，因为这些方法能够识别出有意义的词汇单元，而不是简单地基于字符进行分割。这样，模型可以更准确地捕捉到语言的特征，并提高处理效果。</li>
</ol>
<h2 id="编码器-解码器结构"><a href="#编码器-解码器结构" class="headerlink" title="编码器-解码器结构"></a>编码器-解码器结构</h2><p>机器翻译是序列转换模型的一个核心问题， 其输入和输出都是长度可变的序列。 为了处理这种类型的输入和输出， 我们可以设计一个包含两个主要组件的架构： 第一个组件是一个<em>编码器</em>（encoder）： 它接受一个长度可变的序列作为输入， 并将其转换为具有固定形状的编码状态。 第二个组件是<em>解码器</em>（decoder）： 它将固定形状的编码状态映射到长度可变的序列。 这被称为编码器-解码器（encoder-decoder）架构。</p>
<h3 id="编码器"><a href="#编码器" class="headerlink" title="编码器"></a>编码器</h3><p>在编码器接口中，我们只指定长度可变的序列作为编码器的输入<code>X</code>。 任何继承这个<code>Encoder</code>基类的模型将完成代码实现。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="comment">#@save</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Encoder</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;编码器-解码器架构的基本编码器接口&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, **kwargs</span>):</span><br><span class="line">        <span class="built_in">super</span>(Encoder, self).__init__(**kwargs)</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, X, *args</span>):</span><br><span class="line">        <span class="keyword">raise</span> NotImplementedError</span><br></pre></td></tr></table></figure><br>解码器<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#@save</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Decoder</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;编码器-解码器架构的基本解码器接口&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, **kwargs</span>):</span><br><span class="line">        <span class="built_in">super</span>(Decoder, self).__init__(**kwargs)</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">init_state</span>(<span class="params">self, enc_outputs, *args</span>):</span><br><span class="line">        <span class="keyword">raise</span> NotImplementedError</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, X, state</span>):</span><br><span class="line">        <span class="keyword">raise</span> NotImplementedError</span><br></pre></td></tr></table></figure></p>
<h3 id="合并编码器和解码器"><a href="#合并编码器和解码器" class="headerlink" title="合并编码器和解码器"></a>合并编码器和解码器</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#@save</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">EncoderDecoder</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;编码器-解码器架构的基类&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, encoder, decoder, **kwargs</span>):</span><br><span class="line">        <span class="built_in">super</span>(EncoderDecoder, self).__init__(**kwargs)</span><br><span class="line">        self.encoder = encoder</span><br><span class="line">        self.decoder = decoder</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, enc_X, dec_X, *args</span>):</span><br><span class="line">        enc_outputs = self.encoder(enc_X, *args)</span><br><span class="line">        dec_state = self.decoder.init_state(enc_outputs, *args)</span><br><span class="line">        <span class="keyword">return</span> self.decoder(dec_X, dec_state)</span><br></pre></td></tr></table></figure>
<h3 id="练习-35"><a href="#练习-35" class="headerlink" title="练习"></a>练习</h3><ol>
<li>假设我们使用神经网络来实现“编码器－解码器”架构，那么编码器和解码器必须是同一类型的神经网络吗？<br> 不一定是同一类型的网络</li>
<li>除了机器翻译，还有其它可以适用于”编码器－解码器“架构的应用吗？<br> 语音识别，图像描述，视频内容理解，聊天对话模型</li>
</ol>
<h2 id="序列到序列学习（seq2seq）"><a href="#序列到序列学习（seq2seq）" class="headerlink" title="序列到序列学习（seq2seq）"></a>序列到序列学习（seq2seq）</h2><p>遵循编码器－解码器架构的设计原则， 循环神经网络编码器使用长度可变的序列作为输入， 将其转换为固定形状的隐状态。 换言之，输入序列的信息被<em>编码</em>到循环神经网络编码器的隐状态中。 为了连续生成输出序列的词元， 独立的循环神经网络解码器是基于输入序列的编码信息 和输出序列已经看见的或者生成的词元来预测下一个词元。</p>
<h3 id="练习-36"><a href="#练习-36" class="headerlink" title="练习"></a>练习</h3><ol>
<li>重新运行实验并在计算损失时不使用遮蔽，可以观察到什么结果？为什么会有这个结果？<br> 模型可能会在预测时错误地将注意力放在填充的位置上。这会导致模型学习到错误的依赖关系，因为它把填充的部分也当作了有效的输入。结果通常是模型性能下降，因为它不能正确地区分真实的序列数据和用于保持序列长度一致性的填充数据。遮蔽的目的是告诉模型哪些位置是真实的数据，哪些是填充数据，从而确保模型只在有效的数据上进行学习。</li>
<li>如果编码器和解码器的层数或者隐藏单元数不同，那么如何初始化解码器的隐状态？<br> 解码器的隐状态通常可以从编码器的最后一个时间步的隐状态进行初始化。这样做是因为编码器的最终状态被认为包含了整个输入序列的上下文信息，它可以作为解码器的初始上下文。</li>
<li>在训练中，如果用前一时间步的预测输入到解码器来代替强制教学，对性能有何影响？<br> 这可能会导致模型性能下降。teacher forcing是一种训练策略，它迫使模型在每个时间步使用真实的目标词作为输入，即使这些目标词在推理时可能不可用。如果移除这个策略，模型可能会生成不准确的预测，因为它不再依赖于真实的目标序列进行学习，而是依赖于自己的预测。这可能导致模型陷入错误累积的循环，从而降低生成序列的准确性。</li>
<li>有没有其他方法来设计解码器的输出层？</li>
</ol>
<ul>
<li>注意力机制：通过注意力机制，输出层可以聚焦于输入序列中的相关部分来生成每个输出。</li>
<li>层归一化：在输出层之前使用层归一化可以帮助稳定训练过程。</li>
<li>残差连接：引入残差连接可以帮助信息在网络中更有效地流动，防止梯度消失。</li>
<li>不同的激活函数：根据任务的不同，可以尝试使用不同的激活函数，如tanh或ReLU，来改善模型的性能。</li>
</ul>
<h2 id="束搜索"><a href="#束搜索" class="headerlink" title="束搜索"></a>束搜索</h2><h3 id="贪心搜索"><a href="#贪心搜索" class="headerlink" title="贪心搜索"></a>贪心搜索</h3><p>它在每一步都选择当前看起来最优的选择，而不考虑长远的后果。这种方法简单且易于实现，但在某些情况下可能无法找到全局最优解，因为它可能在早期步骤中就锁定了次优的路径。<br>在机器翻译中，贪心搜索可能会在每个时间步选择最有可能的单词作为翻译的一部分，直到生成完整的句子。然而，这种方法可能会导致输出的连贯性和准确性问题，因为它没有考虑整个句子的全局最优性。<br>在训练过程中，贪心搜索可以用于解码器的输出，其中模型在每个时间步生成一个输出，而不是等待整个序列生成完成。这种方法的优点是速度快，但可能导致生成的序列质量不高，因为它没有利用后续时间步的信息来优化当前的输出。</p>
<h3 id="穷举搜索"><a href="#穷举搜索" class="headerlink" title="穷举搜索"></a>穷举搜索</h3><p>如果目标是获得最优序列， 我们可以考虑使用<em>穷举搜索</em>（exhaustive search）： 穷举地列举所有可能的输出序列及其条件概率， 然后计算输出条件概率最高的一个。</p>
<h3 id="束搜索-1"><a href="#束搜索-1" class="headerlink" title="束搜索"></a>束搜索</h3><p><em>束搜索</em>（beam search）是贪心搜索的一个改进版本。 它有一个超参数，名为束宽（beam size）k。 在时间步1，我们选择具有最高条件概率的k个词元。 这k个词元将分别是k个候选输出序列的第一个词元。 在随后的每个时间步，基于上一时间步的k个候选输出序列， 我们将继续从k|y个可能的选择中 挑出具有最高条件概率的k个候选输出序列。<br>![[Pasted image 20240406202313.png]]</p>
<h3 id="练习-37"><a href="#练习-37" class="headerlink" title="练习"></a>练习</h3><ol>
<li>我们可以把穷举搜索看作一种特殊的束搜索吗？为什么？<br> 穷举搜索可以被看作是束宽为1的特殊束搜索。在穷举搜索中，搜索算法会考虑所有可能的候选解，直到找到最优解或满足某些条件为止。相比之下，束搜索在每一步只保留一定数量的最佳候选解（即束宽），并在此基础上继续搜索。束宽为1时，束搜索在每一步只保留一个候选解，这与穷举搜索中考虑所有可能解的方式非常相似。</li>
<li>在 <a target="_blank" rel="noopener" href="https://zh-v2.d2l.ai/chapter_recurrent-modern/seq2seq.html#sec-seq2seq">9.7节</a>的机器翻译问题中应用束搜索。 束宽是如何影响预测的速度和结果的？<br> 束宽决定了在每一步保留的候选解的数量：<ul>
<li>较小的束宽（如1）可以加快搜索速度，因为需要评估的候选解数量较少。但这也可能导致搜索过程错过一些优质的候选解，从而影响翻译质量。</li>
<li>较大的束宽可以增加找到高质量翻译的概率，因为它考虑了更多的候选解。然而，这也意味着需要更多的计算资源和时间来评估这些候选解，从而减慢搜索速度。 因此，选择合适的束宽需要在搜索质量和效率之间做出权衡。<h1 id="注意力机制"><a href="#注意力机制" class="headerlink" title="注意力机制"></a>注意力机制</h1>“是否包含自主性提示”将注意力机制与全连接层或汇聚层区别开来。 在注意力机制的背景下，自主性提示被称为查询（query）。 给定任何查询，注意力机制通过注意力汇聚（attention pooling） 将选择引导至<em>感官输入</em>（sensory inputs，例如中间特征表示）。 在注意力机制中，这些感官输入被称为值（value）。 更通俗的解释，每个值都与一个键（key）配对， 这可以想象为感官输入的非自主提示。<h3 id="练习-38"><a href="#练习-38" class="headerlink" title="练习"></a>练习</h3></li>
</ul>
</li>
<li>在机器翻译中通过解码序列词元时，其自主性提示可能是什么？非自主性提示和感官输入又是什么？<br> 在机器翻译的上下文中，自主性提示通常指的是模型在生成翻译时依赖于其先前生成的输出，而不是外部输入。例如，在解码过程中，模型可能会根据已经生成的词序列来决定下一个最合适的词。这种提示体现了模型在生成翻译时的自主性和连贯性，因为它完全依赖于内部状态和已生成的序列。</li>
<li>随机生成一个10×10矩阵并使用<code>softmax</code>运算来确保每行都是有效的概率分布，然后可视化输出注意力权重。<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="comment"># 随机生成一个3x5的矩阵，代表注意力权重</span></span><br><span class="line">attention_weights = np.random.rand(<span class="number">3</span>, <span class="number">5</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用softmax运算来归一化每行，确保它们是有效的概率分布</span></span><br><span class="line">attention_distribution = np.exp(attention_weights) / np.<span class="built_in">sum</span>(np.exp(attention_weights), axis=<span class="number">1</span>, keepdims=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 可视化注意力权重</span></span><br><span class="line">plt.figure(figsize=(<span class="number">10</span>, <span class="number">5</span>))</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(attention_distribution.shape[<span class="number">0</span>]):</span><br><span class="line">    plt.plot(<span class="built_in">range</span>(<span class="number">1</span>, attention_distribution.shape[<span class="number">1</span>] + <span class="number">1</span>), attention_distribution[i, :], label=<span class="string">f&#x27;Decoder Position <span class="subst">&#123;i+<span class="number">1</span>&#125;</span>&#x27;</span>)</span><br><span class="line">plt.xlabel(<span class="string">&#x27;Position&#x27;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;Attention Weight&#x27;</span>)</span><br><span class="line">plt.title(<span class="string">&#x27;Attention Weights Distribution&#x27;</span>)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<h2 id="注意力汇聚：Nadaraya-Watson核回归"><a href="#注意力汇聚：Nadaraya-Watson核回归" class="headerlink" title="注意力汇聚：Nadaraya-Watson核回归"></a>注意力汇聚：Nadaraya-Watson核回归</h2><h3 id="生成数据集"><a href="#生成数据集" class="headerlink" title="生成数据集"></a>生成数据集</h3>给定的成对的输入数据集，根据某非线性函数生成一个人工数据集，加入噪声项。生成50个训练样本和50个测试样本。<h3 id="平均汇聚"><a href="#平均汇聚" class="headerlink" title="平均汇聚"></a>平均汇聚</h3>先使用最简单的估计器来解决回归问题。 基于平均汇聚来计算所有训练样本输出值的平均值<h3 id="非参数注意力汇聚"><a href="#非参数注意力汇聚" class="headerlink" title="非参数注意力汇聚"></a>非参数注意力汇聚</h3>显然，平均汇聚忽略了输入xi。 于是Nadaraya 和 Watson提出了一个更好的想法，根据输入的位置对输出yi进行加权。<script type="math/tex; mode=display">f(x)=\sum_{i=1}^{n}\frac{K(x-x_i}{ {\textstyle \sum_{j=1}^{n}}K(x-x_j) }y_i</script>其中KI是核（kernel）<h3 id="带参数注意力汇聚"><a href="#带参数注意力汇聚" class="headerlink" title="带参数注意力汇聚"></a>带参数注意力汇聚</h3>非参数的Nadaraya-Watson核回归具有<em>一致性</em>（consistency）的优点： 如果有足够的数据，此模型会收敛到最优结果。 尽管如此，我们还是可以轻松地将可学习的参数集成到注意力汇聚中。<h3 id="批量矩阵乘法"><a href="#批量矩阵乘法" class="headerlink" title="批量矩阵乘法"></a>批量矩阵乘法</h3>为了更有效地计算小批量数据的注意力， 我们可以利用深度学习开发框架中提供的批量矩阵乘法。<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">X = torch.ones((<span class="number">2</span>, <span class="number">1</span>, <span class="number">4</span>))</span><br><span class="line">Y = torch.ones((<span class="number">2</span>, <span class="number">4</span>, <span class="number">6</span>))</span><br><span class="line">torch.bmm(X, Y).shape</span><br></pre></td></tr></table></figure>
在注意力机制的背景中，我们可以使用小批量矩阵乘法来计算小批量数据中的加权平均值。<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">weights = torch.ones((<span class="number">2</span>, <span class="number">10</span>)) * <span class="number">0.1</span></span><br><span class="line">values = torch.arange(<span class="number">20.0</span>).reshape((<span class="number">2</span>, <span class="number">10</span>))</span><br><span class="line">torch.bmm(weights.unsqueeze(<span class="number">1</span>), values.unsqueeze(-<span class="number">1</span>))</span><br></pre></td></tr></table></figure>
<h3 id="练习-39"><a href="#练习-39" class="headerlink" title="练习"></a>练习</h3></li>
<li>增加训练数据的样本数量，能否得到更好的非参数的Nadaraya-Watson核回归模型？<br> 增加训练数据的样本数量通常能够提升非参数的Nadaraya-Watson核回归模型的性能。非参数方法不对数据的基础结构做任何先验假设，因此它们能够从更多的数据中学习并捕捉到更复杂的模式。</li>
<li>在带参数的注意力汇聚的实验中学习得到的参数w的价值是什么？为什么在可视化注意力权重时，它会使加权区域更加尖锐？<br> 在带参数的注意力汇聚中，学习到的参数可以捕捉输入数据中的重要特征和结构。这些参数有助于模型更好地理解哪些部分的输入对于预测任务最为关键。注意力权重的可视化通常会显示出一个加权区域，这个区域突出了输入数据中与预测目标最相关的部分。当注意力权重集中在一个较小的、尖锐的区域时，这意味着模型已经学习到专注于输入数据中的特定部分，从而提高了预测的准确性和解释性。</li>
<li>如何将超参数添加到非参数的Nadaraya-Watson核回归中以实现更好地预测结果？<br> 可以考虑引入正则化项或者调整核函数的带宽。正则化可以帮助防止过拟合，而带宽参数则控制了模型对数据局部波动的平滑程度。通过调整这些超参数，可以优化模型以适应特定的预测任务。</li>
<li>为本节的核回归设计一个新的带参数的注意力汇聚模型。训练这个新模型并可视化其注意力权重。<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">	<span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">ParameterizedAttentionModel</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, input_dim, attention_dim</span>):</span><br><span class="line">        <span class="built_in">super</span>(ParameterizedAttentionModel, self).__init__()</span><br><span class="line">        self.query LinearLayer(input_dim, attention_dim)</span><br><span class="line">        self.key LinearLayer(input_dim, attention_dim)</span><br><span class="line">        self.value LinearLayer(input_dim, attention_dim)</span><br><span class="line">        self.output LinearLayer(attention_dim, <span class="number">1</span>)</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, inputs</span>):</span><br><span class="line">        queries = self.query(inputs)</span><br><span class="line">        keys = self.key(inputs)</span><br><span class="line">        values = self.value(inputs)</span><br><span class="line">        attention_scores = torch.matmul(queries, keys.transpose(-<span class="number">2</span>, -<span class="number">1</span>))</span><br><span class="line">        attention_weights = F.softmax(attention_scores, dim=-<span class="number">1</span>)</span><br><span class="line">        attended_values = torch.matmul(attention_weights, values)</span><br><span class="line">        output = self.output(attended_values)</span><br><span class="line">        <span class="keyword">return</span> output</span><br><span class="line"><span class="comment"># 假设输入维度为10，注意力维度为5</span></span><br><span class="line">model = ParameterizedAttentionModel(input_dim=<span class="number">10</span>, attention_dim=<span class="number">5</span>)</span><br></pre></td></tr></table></figure>
<h2 id="注意力评分函数"><a href="#注意力评分函数" class="headerlink" title="注意力评分函数"></a>注意力评分函数</h2><a target="_blank" rel="noopener" href="https://zh-v2.d2l.ai/chapter_attention-mechanisms/nadaraya-waston.html#equation-eq-nadaraya-watson-gaussian">(10.2.6)</a>中的 高斯核指数部分可以视为<em>注意力评分函数</em>（attention scoring function）， 简称评分函数（scoring function）， 然后把这个函数的输出结果输入到softmax函数中进行运算。 通过上述步骤，将得到与键对应的值的概率分布（即注意力权重）。 最后，注意力汇聚的输出就是基于这些注意力权重的值的加权和。<br>![[Pasted image 20240408043455.png]]<h3 id="掩蔽softmax操作"><a href="#掩蔽softmax操作" class="headerlink" title="掩蔽softmax操作"></a>掩蔽softmax操作</h3>softmax操作用于输出一个概率分布作为注意力权重。 在某些情况下，并非所有的值都应该被纳入到注意力汇聚中。<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#@save</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">masked_softmax</span>(<span class="params">X, valid_lens</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;通过在最后一个轴上掩蔽元素来执行softmax操作&quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># X:3D张量，valid_lens:1D或2D张量</span></span><br><span class="line">    <span class="keyword">if</span> valid_lens <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">        <span class="keyword">return</span> nn.functional.softmax(X, dim=-<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        shape = X.shape</span><br><span class="line">        <span class="keyword">if</span> valid_lens.dim() == <span class="number">1</span>:</span><br><span class="line">            valid_lens = torch.repeat_interleave(valid_lens, shape[<span class="number">1</span>])</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            valid_lens = valid_lens.reshape(-<span class="number">1</span>)</span><br><span class="line">        <span class="comment"># 最后一轴上被掩蔽的元素使用一个非常大的负值替换，从而其softmax输出为0</span></span><br><span class="line">        X = d2l.sequence_mask(X.reshape(-<span class="number">1</span>, shape[-<span class="number">1</span>]), valid_lens,</span><br><span class="line">                              value=-<span class="number">1e6</span>)</span><br><span class="line">        <span class="keyword">return</span> nn.functional.softmax(X.reshape(shape), dim=-<span class="number">1</span>)</span><br></pre></td></tr></table></figure>
为了演示此函数是如何工作的， 考虑由两个2×4矩阵表示的样本， 这两个样本的有效长度分别为2和3。 经过掩蔽softmax操作，超出有效长度的值都被掩蔽为0。<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">masked_softmax(torch.rand(<span class="number">2</span>, <span class="number">2</span>, <span class="number">4</span>), torch.tensor([<span class="number">2</span>, <span class="number">3</span>]))</span><br></pre></td></tr></table></figure>
<h3 id="加性注意力"><a href="#加性注意力" class="headerlink" title="加性注意力"></a>加性注意力</h3>一般来说，当查询和键是不同长度的矢量时，可以使用加性注意力作为评分函数。<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#@save</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">AdditiveAttention</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;加性注意力&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, key_size, query_size, num_hiddens, dropout, **kwargs</span>):</span><br><span class="line">        <span class="built_in">super</span>(AdditiveAttention, self).__init__(**kwargs)</span><br><span class="line">        self.W_k = nn.Linear(key_size, num_hiddens, bias=<span class="literal">False</span>)</span><br><span class="line">        self.W_q = nn.Linear(query_size, num_hiddens, bias=<span class="literal">False</span>)</span><br><span class="line">        self.w_v = nn.Linear(num_hiddens, <span class="number">1</span>, bias=<span class="literal">False</span>)</span><br><span class="line">        self.dropout = nn.Dropout(dropout)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, queries, keys, values, valid_lens</span>):</span><br><span class="line">        queries, keys = self.W_q(queries), self.W_k(keys)</span><br><span class="line">        <span class="comment"># 在维度扩展后，</span></span><br><span class="line">        <span class="comment"># queries的形状：(batch_size，查询的个数，1，num_hidden)</span></span><br><span class="line">        <span class="comment"># key的形状：(batch_size，1，“键－值”对的个数，num_hiddens)</span></span><br><span class="line">        <span class="comment"># 使用广播方式进行求和</span></span><br><span class="line">        features = queries.unsqueeze(<span class="number">2</span>) + keys.unsqueeze(<span class="number">1</span>)</span><br><span class="line">        features = torch.tanh(features)</span><br><span class="line">        <span class="comment"># self.w_v仅有一个输出，因此从形状中移除最后那个维度。</span></span><br><span class="line">        <span class="comment"># scores的形状：(batch_size，查询的个数，“键-值”对的个数)</span></span><br><span class="line">        scores = self.w_v(features).squeeze(-<span class="number">1</span>)</span><br><span class="line">        self.attention_weights = masked_softmax(scores, valid_lens)</span><br><span class="line">        <span class="comment"># values的形状：(batch_size，“键－值”对的个数，值的维度)</span></span><br><span class="line">        <span class="keyword">return</span> torch.bmm(self.dropout(self.attention_weights), values)</span><br></pre></td></tr></table></figure>
用一个小例子来演示上面的<code>AdditiveAttention</code>类， 其中查询、键和值的形状为（批量大小，步数或词元序列长度，特征大小）， 实际输出为(2,1,20)、(2,10,2)和(2,10,4)。 注意力汇聚输出的形状为（批量大小，查询的步数，值的维度）。<h3 id="练习-40"><a href="#练习-40" class="headerlink" title="练习"></a>练习</h3></li>
<li>修改小例子中的键，并且可视化注意力权重。可加性注意力和缩放的“点－积”注意力是否仍然产生相同的结果？为什么？<br> 我们首先需要了解注意力权重是如何计算的。在典型的注意力机制中，如“点-积”注意力，权重是通过查询（Q）和键（K）的点积来计算的，然后通常会除以一个缩放因子，最后应用softmax函数来获得概率分布。如果我们要修改键，我们需要确保修改后的键仍然能够与查询进行有效的交互，以便计算出有意义的注意力权重。可视化注意力权重通常涉及到将权重矩阵以图形的形式展示出来，这样可以直观地看到模型在处理输入序列时对不同位置的关注度。</li>
<li>只使用矩阵乘法，能否为具有不同矢量长度的查询和键设计新的评分函数？<br> 只使用矩阵乘法可能不足以处理具有不同矢量长度的查询和键。在标准的注意力机制中，查询和键的长度通常是相同的，以便进行点积操作。如果查询和键的长度不同，可能需要引入额外的步骤来调整它们的长度，或者设计一个能够处理不同长度输入的评分函数。例如，可以通过嵌入层将查询和键映射到相同长度的空间，或者设计一个基于对齐或匹配的评分函数，而不是简单的点积。</li>
<li>当查询和键具有相同的矢量长度时，矢量求和作为评分函数是否比“点－积”更好？为什么？<br> 当查询和键具有相同的矢量长度时，矢量求和作为评分函数可能不如“点-积”注意力有效。原因在于点-积能够更好地捕捉查询和键之间的相似性，因为它考虑了查询和键的每个维度之间的相互作用。而矢量求和可能会忽略这些维度之间的相互作用，导致无法有效地捕捉查询和键之间的关系。<h2 id="Bahdanau-注意力"><a href="#Bahdanau-注意力" class="headerlink" title="Bahdanau 注意力"></a>Bahdanau 注意力</h2><h3 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h3>![[Pasted image 20240408043946.png]]<h3 id="定义注意力解码器"><a href="#定义注意力解码器" class="headerlink" title="定义注意力解码器"></a>定义注意力解码器</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#@save</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">AttentionDecoder</span>(d2l.Decoder):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;带有注意力机制解码器的基本接口&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, **kwargs</span>):</span><br><span class="line">        <span class="built_in">super</span>(AttentionDecoder, self).__init__(**kwargs)</span><br><span class="line"></span><br><span class="line"><span class="meta">    @property</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">attention_weights</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">raise</span> NotImplementedError</span><br></pre></td></tr></table></figure>
接下来，让我们在接下来的<code>Seq2SeqAttentionDecoder</code>类中 实现带有Bahdanau注意力的循环神经网络解码器。 首先，初始化解码器的状态，需要下面的输入：</li>
<li>编码器在所有时间步的最终层隐状态，将作为注意力的键和值；</li>
<li>上一时间步的编码器全层隐状态，将作为初始化解码器的隐状态；</li>
<li>编码器有效长度（排除在注意力池中填充词元）。<br>在每个解码时间步骤中，解码器上一个时间步的最终层隐状态将用作查询。 因此，注意力输出和输入嵌入都连结为循环神经网络解码器的输入。<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Seq2SeqAttentionDecoder</span>(<span class="title class_ inherited__">AttentionDecoder</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, vocab_size, embed_size, num_hiddens, num_layers,</span></span><br><span class="line"><span class="params">                 dropout=<span class="number">0</span>, **kwargs</span>):</span><br><span class="line">        <span class="built_in">super</span>(Seq2SeqAttentionDecoder, self).__init__(**kwargs)</span><br><span class="line">        self.attention = d2l.AdditiveAttention(</span><br><span class="line">            num_hiddens, num_hiddens, num_hiddens, dropout)</span><br><span class="line">        self.embedding = nn.Embedding(vocab_size, embed_size)</span><br><span class="line">        self.rnn = nn.GRU(</span><br><span class="line">            embed_size + num_hiddens, num_hiddens, num_layers,</span><br><span class="line">            dropout=dropout)</span><br><span class="line">        self.dense = nn.Linear(num_hiddens, vocab_size)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">init_state</span>(<span class="params">self, enc_outputs, enc_valid_lens, *args</span>):</span><br><span class="line">        <span class="comment"># outputs的形状为(batch_size，num_steps，num_hiddens).</span></span><br><span class="line">        <span class="comment"># hidden_state的形状为(num_layers，batch_size，num_hiddens)</span></span><br><span class="line">        outputs, hidden_state = enc_outputs</span><br><span class="line">        <span class="keyword">return</span> (outputs.permute(<span class="number">1</span>, <span class="number">0</span>, <span class="number">2</span>), hidden_state, enc_valid_lens)</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, X, state</span>):</span><br><span class="line">        <span class="comment"># enc_outputs的形状为(batch_size,num_steps,num_hiddens).</span></span><br><span class="line">        <span class="comment"># hidden_state的形状为(num_layers,batch_size,</span></span><br><span class="line">        <span class="comment"># num_hiddens)</span></span><br><span class="line">        enc_outputs, hidden_state, enc_valid_lens = state</span><br><span class="line">        <span class="comment"># 输出X的形状为(num_steps,batch_size,embed_size)</span></span><br><span class="line">        X = self.embedding(X).permute(<span class="number">1</span>, <span class="number">0</span>, <span class="number">2</span>)</span><br><span class="line">        outputs, self._attention_weights = [], []</span><br><span class="line">        <span class="keyword">for</span> x <span class="keyword">in</span> X:</span><br><span class="line">            <span class="comment"># query的形状为(batch_size,1,num_hiddens)</span></span><br><span class="line">            query = torch.unsqueeze(hidden_state[-<span class="number">1</span>], dim=<span class="number">1</span>)</span><br><span class="line">            <span class="comment"># context的形状为(batch_size,1,num_hiddens)</span></span><br><span class="line">            context = self.attention(</span><br><span class="line">                query, enc_outputs, enc_outputs, enc_valid_lens)</span><br><span class="line">            <span class="comment"># 在特征维度上连结</span></span><br><span class="line">            x = torch.cat((context, torch.unsqueeze(x, dim=<span class="number">1</span>)), dim=-<span class="number">1</span>)</span><br><span class="line">            <span class="comment"># 将x变形为(1,batch_size,embed_size+num_hiddens)</span></span><br><span class="line">            out, hidden_state = self.rnn(x.permute(<span class="number">1</span>, <span class="number">0</span>, <span class="number">2</span>), hidden_state)</span><br><span class="line">            outputs.append(out)</span><br><span class="line">            self._attention_weights.append(self.attention.attention_weights)</span><br><span class="line">        <span class="comment"># 全连接层变换后，outputs的形状为</span></span><br><span class="line">        <span class="comment"># (num_steps,batch_size,vocab_size)</span></span><br><span class="line">        outputs = self.dense(torch.cat(outputs, dim=<span class="number">0</span>))</span><br><span class="line">        <span class="keyword">return</span> outputs.permute(<span class="number">1</span>, <span class="number">0</span>, <span class="number">2</span>), [enc_outputs, hidden_state,</span><br><span class="line">                                          enc_valid_lens]</span><br><span class="line"><span class="meta">    @property</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">attention_weights</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> self._attention_weights</span><br></pre></td></tr></table></figure>
<h2 id="多头注意力"><a href="#多头注意力" class="headerlink" title="多头注意力"></a>多头注意力</h2>多头注意力（Multi-head Attention）是一种用于增强神经网络模型对序列数据（如文本或时间序列）建模能力的注意力机制。它是 Transformer 模型中的关键组件之一。</li>
</ol>
<p>在传统的注意力机制中，模型通过计算查询（query）、键（key）和值（value）之间的关联来为每个查询选择相关的值。而多头注意力通过引入多组查询-键-值投影矩阵，从而使模型能够在不同的表示空间中学习关注不同方面的信息。每个头产生的注意力权重矩阵被独立地计算，然后这些矩阵被拼接在一起并经过另一个线性变换，最终得到多头注意力的输出。</p>
<p>多头注意力的主要优势在于它可以让模型在不同的表示子空间中学习到不同的语义信息，从而提高模型对复杂关系的建模能力。在 Transformer 中，多头注意力被用来同时捕捉不同位置的关系和不同层次的语义信息，使得模型能够更好地处理长距离依赖性。</p>
<h3 id="实现"><a href="#实现" class="headerlink" title="实现"></a>实现</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#@save</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MultiHeadAttention</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;多头注意力&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, key_size, query_size, value_size, num_hiddens,</span></span><br><span class="line"><span class="params">                 num_heads, dropout, bias=<span class="literal">False</span>, **kwargs</span>):</span><br><span class="line">        <span class="built_in">super</span>(MultiHeadAttention, self).__init__(**kwargs)</span><br><span class="line">        self.num_heads = num_heads</span><br><span class="line">        self.attention = d2l.DotProductAttention(dropout)</span><br><span class="line">        self.W_q = nn.Linear(query_size, num_hiddens, bias=bias)</span><br><span class="line">        self.W_k = nn.Linear(key_size, num_hiddens, bias=bias)</span><br><span class="line">        self.W_v = nn.Linear(value_size, num_hiddens, bias=bias)</span><br><span class="line">        self.W_o = nn.Linear(num_hiddens, num_hiddens, bias=bias)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, queries, keys, values, valid_lens</span>):</span><br><span class="line">        <span class="comment"># queries，keys，values的形状:</span></span><br><span class="line">        <span class="comment"># (batch_size，查询或者“键－值”对的个数，num_hiddens)</span></span><br><span class="line">        <span class="comment"># valid_lens　的形状:</span></span><br><span class="line">        <span class="comment"># (batch_size，)或(batch_size，查询的个数)</span></span><br><span class="line">        <span class="comment"># 经过变换后，输出的queries，keys，values　的形状:</span></span><br><span class="line">        <span class="comment"># (batch_size*num_heads，查询或者“键－值”对的个数，</span></span><br><span class="line">        <span class="comment"># num_hiddens/num_heads)</span></span><br><span class="line">        queries = transpose_qkv(self.W_q(queries), self.num_heads)</span><br><span class="line">        keys = transpose_qkv(self.W_k(keys), self.num_heads)</span><br><span class="line">        values = transpose_qkv(self.W_v(values), self.num_heads)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> valid_lens <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="comment"># 在轴0，将第一项（标量或者矢量）复制num_heads次，</span></span><br><span class="line">            <span class="comment"># 然后如此复制第二项，然后诸如此类。</span></span><br><span class="line">            valid_lens = torch.repeat_interleave(</span><br><span class="line">                valid_lens, repeats=self.num_heads, dim=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># output的形状:(batch_size*num_heads，查询的个数，</span></span><br><span class="line">        <span class="comment"># num_hiddens/num_heads)</span></span><br><span class="line">        output = self.attention(queries, keys, values, valid_lens)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># output_concat的形状:(batch_size，查询的个数，num_hiddens)</span></span><br><span class="line">        output_concat = transpose_output(output, self.num_heads)</span><br><span class="line">        <span class="keyword">return</span> self.W_o(output_concat)</span><br></pre></td></tr></table></figure>
<p>同时定义transpose_qkv,转置函数<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#@save</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">transpose_qkv</span>(<span class="params">X, num_heads</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;为了多注意力头的并行计算而变换形状&quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># 输入X的形状:(batch_size，查询或者“键－值”对的个数，num_hiddens)</span></span><br><span class="line">    <span class="comment"># 输出X的形状:(batch_size，查询或者“键－值”对的个数，num_heads，</span></span><br><span class="line">    <span class="comment"># num_hiddens/num_heads)</span></span><br><span class="line">    X = X.reshape(X.shape[<span class="number">0</span>], X.shape[<span class="number">1</span>], num_heads, -<span class="number">1</span>)</span><br><span class="line">    <span class="comment"># 输出X的形状:(batch_size，num_heads，查询或者“键－值”对的个数,</span></span><br><span class="line">    <span class="comment"># num_hiddens/num_heads)</span></span><br><span class="line">    X = X.permute(<span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">3</span>)</span><br><span class="line">    <span class="comment"># 最终输出的形状:(batch_size*num_heads,查询或者“键－值”对的个数,</span></span><br><span class="line">    <span class="comment"># num_hiddens/num_heads)</span></span><br><span class="line">    <span class="keyword">return</span> X.reshape(-<span class="number">1</span>, X.shape[<span class="number">2</span>], X.shape[<span class="number">3</span>])</span><br><span class="line"><span class="comment">#@save</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">transpose_output</span>(<span class="params">X, num_heads</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;逆转transpose_qkv函数的操作&quot;&quot;&quot;</span></span><br><span class="line">    X = X.reshape(-<span class="number">1</span>, num_heads, X.shape[<span class="number">1</span>], X.shape[<span class="number">2</span>])</span><br><span class="line">    X = X.permute(<span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">3</span>)</span><br><span class="line">    <span class="keyword">return</span> X.reshape(X.shape[<span class="number">0</span>], X.shape[<span class="number">1</span>], -<span class="number">1</span>)</span><br></pre></td></tr></table></figure></p>
<h3 id="练习-41"><a href="#练习-41" class="headerlink" title="练习"></a>练习</h3><ol>
<li><p>分别可视化这个实验中的多个头的注意力权重。</p>
</li>
<li><p>假设有一个完成训练的基于多头注意力的模型，现在希望修剪最不重要的注意力头以提高预测速度。如何设计实验来衡量注意力头的重要性呢？<br> 为了衡量注意力头的重要性并设计实验进行修剪，我们可以采用以下方法：<br> 首先，我们需要定义一个性能指标，如准确率、F1分数或模型的损失值，来评估模型在不同任务上的表现。<br> 逐个或逐步关闭（修剪）注意力头，并观察性能指标的变化。每次关闭一个或多个头后，重新训练模型，并记录性能变化。<br> 分析性能变化与修剪的头之间的关系。如果关闭某些头后性能下降不明显，这可能表明这些头不是很重要。相反，如果性能显著下降，则表明这些头对模型的预测至关重要。<br> 根据重要性分析的结果，迭代地调整模型结构，移除不重要的头，并重新训练模型。这个过程可能需要多次迭代，直到找到一个性能和效率之间的最佳平衡点。</p>
</li>
</ol>
<h2 id="自注意力与位置编码"><a href="#自注意力与位置编码" class="headerlink" title="自注意力与位置编码"></a>自注意力与位置编码</h2><p>想象一下，有了注意力机制之后，我们将词元序列输入注意力池化中， 以便同一组词元同时充当查询、键和值。 具体来说，每个查询都会关注所有的键－值对并生成一个注意力输出。 由于查询、键和值来自同一组输入，因此被称为 <em>自注意力</em>（self-attention）</p>
<h3 id="位置编码"><a href="#位置编码" class="headerlink" title="位置编码"></a>位置编码</h3><p>在处理词元序列时，循环神经网络是逐个的重复地处理词元的， 而自注意力则因为并行计算而放弃了顺序操作。 为了使用序列的顺序信息，通过在输入表示中添加 <em>位置编码</em>（positional encoding）来注入绝对的或相对的位置信息。 位置编码可以通过学习得到也可以直接固定得到。基于正弦函数和余弦函数的固定位置编码：<br>假设输入表示$X∈R^{n<em>d}$ 包含一个序列中n个词元的d维嵌入表示。 位置编码使用相同形状的位置嵌入矩阵 $P∈R^{n</em>d}$输出X+P， 矩阵第i行、第2j列和2j+1列上的元素为：</p>
<script type="math/tex; mode=display">p_{i,2j}=sin(\frac{i}{10000^{2j/d}})</script><script type="math/tex; mode=display">p_{i,2j+1}=cos(\frac{i}{10000^{2j/d}})</script><p>在位置嵌入矩阵P中， 行代表词元在序列中的位置，列代表位置编码的不同维度。 从下面的例子中可以看到位置嵌入矩阵的第6列和第7列的频率高于第8列和第9列。 第6列和第7列之间的偏移量（第8列和第9列相同）是由于正弦函数和余弦函数的交替。</p>
<h3 id="练习-42"><a href="#练习-42" class="headerlink" title="练习"></a>练习</h3><ol>
<li>假设设计一个深度架构，通过堆叠基于位置编码的自注意力层来表示序列。可能会存在什么问题？<br> 位置编码的固定性，难以捕捉序列中的长期依赖关系，计算复杂度增加，过拟合风险。</li>
<li>请设计一种可学习的位置编码方法。<br>  在可学习的位置编码中，位置编码不再是固定的，而是作为模型参数的一部分。这意味着位置编码可以通过训练过程进行优化。在上述代码示例中，<code>self.positional_embedding</code>是一个可学习的参数，其形状为<code>[max_position, num_features]</code>。这个矩阵存储了从位置0到<code>max_position</code>的每个位置的编码。<br> 位置编码的自适应性： 由于位置编码是可学习的，模型可以根据数据中的模式和序列的特点自适应地调整位置编码。这使得模型能够更好地处理各种长度的序列，并且能够捕捉到序列中位置信息的复杂性。<h2 id="Transformer"><a href="#Transformer" class="headerlink" title="Transformer"></a>Transformer</h2>![[Pasted image 20240408050453.png]]</li>
</ol>
<h3 id="基于位置的前馈网络"><a href="#基于位置的前馈网络" class="headerlink" title="基于位置的前馈网络"></a>基于位置的前馈网络</h3><p>基于位置的前馈网络对序列中的所有位置的表示进行变换时使用的是同一个多层感知机（MLP）。输入<code>X</code>的形状（批量大小，时间步数或序列长度，隐单元数或特征维度）将被一个两层的感知机转换成形状为（批量大小，时间步数，<code>ffn_num_outputs</code>）的输出张量。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#@save</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">PositionWiseFFN</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;基于位置的前馈网络&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, ffn_num_input, ffn_num_hiddens, ffn_num_outputs,</span></span><br><span class="line"><span class="params">                 **kwargs</span>):</span><br><span class="line">        <span class="built_in">super</span>(PositionWiseFFN, self).__init__(**kwargs)</span><br><span class="line">        self.dense1 = nn.Linear(ffn_num_input, ffn_num_hiddens)</span><br><span class="line">        self.relu = nn.ReLU()</span><br><span class="line">        self.dense2 = nn.Linear(ffn_num_hiddens, ffn_num_outputs)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, X</span>):</span><br><span class="line">        <span class="keyword">return</span> self.dense2(self.relu(self.dense1(X)))</span><br></pre></td></tr></table></figure><br>用同一个多层感知机对所有位置上的输入进行变换，当这些位置的输入相同时，输出也是相同的。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">ffn = PositionWiseFFN(<span class="number">4</span>, <span class="number">4</span>, <span class="number">8</span>)</span><br><span class="line">ffn.<span class="built_in">eval</span>()</span><br><span class="line">ffn(torch.ones((<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>)))[<span class="number">0</span>]</span><br></pre></td></tr></table></figure></p>
<h3 id="残差连接和层规范化"><a href="#残差连接和层规范化" class="headerlink" title="残差连接和层规范化"></a>残差连接和层规范化</h3><p>以下代码对比不同维度的层规范化和批量规范化的效果。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">ln = nn.LayerNorm(<span class="number">2</span>)</span><br><span class="line">bn = nn.BatchNorm1d(<span class="number">2</span>)</span><br><span class="line">X = torch.tensor([[<span class="number">1</span>, <span class="number">2</span>], [<span class="number">2</span>, <span class="number">3</span>]], dtype=torch.float32)</span><br><span class="line"><span class="comment"># 在训练模式下计算X的均值和方差</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;layer norm:&#x27;</span>, ln(X), <span class="string">&#x27;\nbatch norm:&#x27;</span>, bn(X))</span><br></pre></td></tr></table></figure><br>现在可以使用残差连接和层规范化来实现<code>AddNorm</code>类。暂退法也被作为正则化方法使用。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#@save</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">AddNorm</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;残差连接后进行层规范化&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, normalized_shape, dropout, **kwargs</span>):</span><br><span class="line">        <span class="built_in">super</span>(AddNorm, self).__init__(**kwargs)</span><br><span class="line">        self.dropout = nn.Dropout(dropout)</span><br><span class="line">        self.ln = nn.LayerNorm(normalized_shape)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, X, Y</span>):</span><br><span class="line">        <span class="keyword">return</span> self.ln(self.dropout(Y) + X)</span><br></pre></td></tr></table></figure><br>残差连接要求两个输入的形状相同，以便加法操作后输出张量的形状相同。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">add_norm = AddNorm([<span class="number">3</span>, <span class="number">4</span>], <span class="number">0.5</span>)</span><br><span class="line">add_norm.<span class="built_in">eval</span>()</span><br><span class="line">add_norm(torch.ones((<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>)), torch.ones((<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>))).shape</span><br></pre></td></tr></table></figure></p>
<h3 id="编码器-1"><a href="#编码器-1" class="headerlink" title="编码器"></a>编码器</h3><p>有了组成Transformer编码器的基础组件，现在可以先实现编码器中的一个层。下面的<code>EncoderBlock</code>类包含两个子层：多头自注意力和基于位置的前馈网络，这两个子层都使用了残差连接和紧随的层规范化。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#@save</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">EncoderBlock</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Transformer编码器块&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, key_size, query_size, value_size, num_hiddens,</span></span><br><span class="line"><span class="params">                 norm_shape, ffn_num_input, ffn_num_hiddens, num_heads,</span></span><br><span class="line"><span class="params">                 dropout, use_bias=<span class="literal">False</span>, **kwargs</span>):</span><br><span class="line">        <span class="built_in">super</span>(EncoderBlock, self).__init__(**kwargs)</span><br><span class="line">        self.attention = d2l.MultiHeadAttention(</span><br><span class="line">            key_size, query_size, value_size, num_hiddens, num_heads, dropout,</span><br><span class="line">            use_bias)</span><br><span class="line">        self.addnorm1 = AddNorm(norm_shape, dropout)</span><br><span class="line">        self.ffn = PositionWiseFFN(</span><br><span class="line">            ffn_num_input, ffn_num_hiddens, num_hiddens)</span><br><span class="line">        self.addnorm2 = AddNorm(norm_shape, dropout)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, X, valid_lens</span>):</span><br><span class="line">        Y = self.addnorm1(X, self.attention(X, X, X, valid_lens))</span><br><span class="line">        <span class="keyword">return</span> self.addnorm2(Y, self.ffn(Y))</span><br></pre></td></tr></table></figure><br>验证Transformer编码器中任何层都不会改变骑输入的形状：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">X = torch.ones((<span class="number">2</span>, <span class="number">100</span>, <span class="number">24</span>))</span><br><span class="line">valid_lens = torch.tensor([<span class="number">3</span>, <span class="number">2</span>])</span><br><span class="line">encoder_blk = EncoderBlock(<span class="number">24</span>, <span class="number">24</span>, <span class="number">24</span>, <span class="number">24</span>, [<span class="number">100</span>, <span class="number">24</span>], <span class="number">24</span>, <span class="number">48</span>, <span class="number">8</span>, <span class="number">0.5</span>)</span><br><span class="line">encoder_blk.<span class="built_in">eval</span>()</span><br><span class="line">encoder_blk(X, valid_lens).shape</span><br><span class="line"><span class="comment"># torch.Size([2, 100, 24])</span></span><br></pre></td></tr></table></figure><br>下面实现的Transformer编码器的代码中，堆叠了<code>num_layers</code>个<code>EncoderBlock</code>类的实例。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#@save</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">TransformerEncoder</span>(d2l.Encoder):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Transformer编码器&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, vocab_size, key_size, query_size, value_size,</span></span><br><span class="line"><span class="params">                 num_hiddens, norm_shape, ffn_num_input, ffn_num_hiddens,</span></span><br><span class="line"><span class="params">                 num_heads, num_layers, dropout, use_bias=<span class="literal">False</span>, **kwargs</span>):</span><br><span class="line">        <span class="built_in">super</span>(TransformerEncoder, self).__init__(**kwargs)</span><br><span class="line">        self.num_hiddens = num_hiddens</span><br><span class="line">        self.embedding = nn.Embedding(vocab_size, num_hiddens)</span><br><span class="line">        self.pos_encoding = d2l.PositionalEncoding(num_hiddens, dropout)</span><br><span class="line">        self.blks = nn.Sequential()</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(num_layers):</span><br><span class="line">            self.blks.add_module(<span class="string">&quot;block&quot;</span>+<span class="built_in">str</span>(i),</span><br><span class="line">                EncoderBlock(key_size, query_size, value_size, num_hiddens,</span><br><span class="line">                             norm_shape, ffn_num_input, ffn_num_hiddens,</span><br><span class="line">                             num_heads, dropout, use_bias))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, X, valid_lens, *args</span>):</span><br><span class="line">        <span class="comment"># 因为位置编码值在-1和1之间，</span></span><br><span class="line">        <span class="comment"># 因此嵌入值乘以嵌入维度的平方根进行缩放，</span></span><br><span class="line">        <span class="comment"># 然后再与位置编码相加。</span></span><br><span class="line">        X = self.pos_encoding(self.embedding(X) * math.sqrt(self.num_hiddens))</span><br><span class="line">        self.attention_weights = [<span class="literal">None</span>] * <span class="built_in">len</span>(self.blks)</span><br><span class="line">        <span class="keyword">for</span> i, blk <span class="keyword">in</span> <span class="built_in">enumerate</span>(self.blks):</span><br><span class="line">            X = blk(X, valid_lens)</span><br><span class="line">            self.attention_weights[</span><br><span class="line">                i] = blk.attention.attention.attention_weights</span><br><span class="line">        <span class="keyword">return</span> X</span><br></pre></td></tr></table></figure><br>下面我们指定了超参数来创建一个两层的Transformer编码器。 Transformer编码器输出的形状是（批量大小，时间步数目，<code>num_hiddens</code>）。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">encoder = TransformerEncoder(</span><br><span class="line">    <span class="number">200</span>, <span class="number">24</span>, <span class="number">24</span>, <span class="number">24</span>, <span class="number">24</span>, [<span class="number">100</span>, <span class="number">24</span>], <span class="number">24</span>, <span class="number">48</span>, <span class="number">8</span>, <span class="number">2</span>, <span class="number">0.5</span>)</span><br><span class="line">encoder.<span class="built_in">eval</span>()</span><br><span class="line">encoder(torch.ones((<span class="number">2</span>, <span class="number">100</span>), dtype=torch.long), valid_lens).shape</span><br><span class="line"><span class="comment"># torch.Size([2, 100, 24])</span></span><br></pre></td></tr></table></figure></p>
<h3 id="解码器"><a href="#解码器" class="headerlink" title="解码器"></a>解码器</h3><p>Transformer解码器也是由多个相同的层组成。在<code>DecoderBlock</code>类中实现的每个层包含了三个子层：解码器自注意力、“编码器-解码器”注意力和基于位置的前馈网络。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">DecoderBlock</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;解码器中第i个块&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, key_size, query_size, value_size, num_hiddens,</span></span><br><span class="line"><span class="params">                 norm_shape, ffn_num_input, ffn_num_hiddens, num_heads,</span></span><br><span class="line"><span class="params">                 dropout, i, **kwargs</span>):</span><br><span class="line">        <span class="built_in">super</span>(DecoderBlock, self).__init__(**kwargs)</span><br><span class="line">        self.i = i</span><br><span class="line">        self.attention1 = d2l.MultiHeadAttention(</span><br><span class="line">            key_size, query_size, value_size, num_hiddens, num_heads, dropout)</span><br><span class="line">        self.addnorm1 = AddNorm(norm_shape, dropout)</span><br><span class="line">        self.attention2 = d2l.MultiHeadAttention(</span><br><span class="line">            key_size, query_size, value_size, num_hiddens, num_heads, dropout)</span><br><span class="line">        self.addnorm2 = AddNorm(norm_shape, dropout)</span><br><span class="line">        self.ffn = PositionWiseFFN(ffn_num_input, ffn_num_hiddens,</span><br><span class="line">                                   num_hiddens)</span><br><span class="line">        self.addnorm3 = AddNorm(norm_shape, dropout)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, X, state</span>):</span><br><span class="line">        enc_outputs, enc_valid_lens = state[<span class="number">0</span>], state[<span class="number">1</span>]</span><br><span class="line">        <span class="comment"># 训练阶段，输出序列的所有词元都在同一时间处理，</span></span><br><span class="line">        <span class="comment"># 因此state[2][self.i]初始化为None。</span></span><br><span class="line">        <span class="comment"># 预测阶段，输出序列是通过词元一个接着一个解码的，</span></span><br><span class="line">        <span class="comment"># 因此state[2][self.i]包含着直到当前时间步第i个块解码的输出表示</span></span><br><span class="line">        <span class="keyword">if</span> state[<span class="number">2</span>][self.i] <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            key_values = X</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            key_values = torch.cat((state[<span class="number">2</span>][self.i], X), axis=<span class="number">1</span>)</span><br><span class="line">        state[<span class="number">2</span>][self.i] = key_values</span><br><span class="line">        <span class="keyword">if</span> self.training:</span><br><span class="line">            batch_size, num_steps, _ = X.shape</span><br><span class="line">            <span class="comment"># dec_valid_lens的开头:(batch_size,num_steps),</span></span><br><span class="line">            <span class="comment"># 其中每一行是[1,2,...,num_steps]</span></span><br><span class="line">            dec_valid_lens = torch.arange(</span><br><span class="line">                <span class="number">1</span>, num_steps + <span class="number">1</span>, device=X.device).repeat(batch_size, <span class="number">1</span>)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            dec_valid_lens = <span class="literal">None</span></span><br><span class="line">        <span class="comment"># 自注意力</span></span><br><span class="line">        X2 = self.attention1(X, key_values, key_values, dec_valid_lens)</span><br><span class="line">        Y = self.addnorm1(X, X2)</span><br><span class="line">        <span class="comment"># 编码器－解码器注意力。</span></span><br><span class="line">        <span class="comment"># enc_outputs的开头:(batch_size,num_steps,num_hiddens)</span></span><br><span class="line">        Y2 = self.attention2(Y, enc_outputs, enc_outputs, enc_valid_lens)</span><br><span class="line">        Z = self.addnorm2(Y, Y2)</span><br><span class="line">        <span class="keyword">return</span> self.addnorm3(Z, self.ffn(Z)), state</span><br></pre></td></tr></table></figure><br>为了便于在“编码器－解码器”注意力中进行缩放点积计算和残差连接中进行加法计算，编码器和解码器的特征维度都是<code>num_hiddens</code>。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">decoder_blk = DecoderBlock(<span class="number">24</span>, <span class="number">24</span>, <span class="number">24</span>, <span class="number">24</span>, [<span class="number">100</span>, <span class="number">24</span>], <span class="number">24</span>, <span class="number">48</span>, <span class="number">8</span>, <span class="number">0.5</span>, <span class="number">0</span>)</span><br><span class="line">decoder_blk.<span class="built_in">eval</span>()</span><br><span class="line">X = torch.ones((<span class="number">2</span>, <span class="number">100</span>, <span class="number">24</span>))</span><br><span class="line">state = [encoder_blk(X, valid_lens), valid_lens, [<span class="literal">None</span>]]</span><br><span class="line">decoder_blk(X, state)[<span class="number">0</span>].shape</span><br><span class="line"><span class="comment"># torch.Size([2, 100, 24])</span></span><br></pre></td></tr></table></figure><br>将多个block拼接成decoder<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">TransformerDecoder</span>(d2l.AttentionDecoder):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, vocab_size, key_size, query_size, value_size,</span></span><br><span class="line"><span class="params">                 num_hiddens, norm_shape, ffn_num_input, ffn_num_hiddens,</span></span><br><span class="line"><span class="params">                 num_heads, num_layers, dropout, **kwargs</span>):</span><br><span class="line">        <span class="built_in">super</span>(TransformerDecoder, self).__init__(**kwargs)</span><br><span class="line">        self.num_hiddens = num_hiddens</span><br><span class="line">        self.num_layers = num_layers</span><br><span class="line">        self.embedding = nn.Embedding(vocab_size, num_hiddens)</span><br><span class="line">        self.pos_encoding = d2l.PositionalEncoding(num_hiddens, dropout)</span><br><span class="line">        self.blks = nn.Sequential()</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(num_layers):</span><br><span class="line">            self.blks.add_module(<span class="string">&quot;block&quot;</span>+<span class="built_in">str</span>(i),</span><br><span class="line">                DecoderBlock(key_size, query_size, value_size, num_hiddens,</span><br><span class="line">                             norm_shape, ffn_num_input, ffn_num_hiddens,</span><br><span class="line">                             num_heads, dropout, i))</span><br><span class="line">        self.dense = nn.Linear(num_hiddens, vocab_size)</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">init_state</span>(<span class="params">self, enc_outputs, enc_valid_lens, *args</span>):</span><br><span class="line">        <span class="keyword">return</span> [enc_outputs, enc_valid_lens, [<span class="literal">None</span>] * self.num_layers]</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, X, state</span>):</span><br><span class="line">        X = self.pos_encoding(self.embedding(X) * math.sqrt(self.num_hiddens))</span><br><span class="line">        self._attention_weights = [[<span class="literal">None</span>] * <span class="built_in">len</span>(self.blks) <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span> (<span class="number">2</span>)]</span><br><span class="line">        <span class="keyword">for</span> i, blk <span class="keyword">in</span> <span class="built_in">enumerate</span>(self.blks):</span><br><span class="line">            X, state = blk(X, state)</span><br><span class="line">            <span class="comment"># 解码器自注意力权重</span></span><br><span class="line">            self._attention_weights[<span class="number">0</span>][</span><br><span class="line">                i] = blk.attention1.attention.attention_weights</span><br><span class="line">            <span class="comment"># “编码器－解码器”自注意力权重</span></span><br><span class="line">            self._attention_weights[<span class="number">1</span>][</span><br><span class="line">                i] = blk.attention2.attention.attention_weights</span><br><span class="line">        <span class="keyword">return</span> self.dense(X), state</span><br><span class="line"><span class="meta">    @property</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">attention_weights</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> self._attention_weights</span><br></pre></td></tr></table></figure></p>
<h3 id="练习-43"><a href="#练习-43" class="headerlink" title="练习"></a>练习</h3><ol>
<li>在实验中训练更深的Transformer将如何影响训练速度和翻译效果？<br> 可能会增加训练时间和计算资源的需求。随着层数的增加，模型的参数数量也会增加，这可能导致梯度下降过程中的优化变得更加复杂和困难。同时，更多的层意味着更多的注意力计算，这会增加内存消耗和计算时间。</li>
<li>在Transformer中使用加性注意力取代缩放点积注意力是不是个好办法？为什么？<br> 加性注意力通过一个加性而非点积的方式来计算注意力分数。这种方法可能有助于模型捕捉不同类型的依赖关系，因为它提供了一种不同的机制来组合查询和键的信息。</li>
<li>对于语言模型，应该使用Transformer的编码器还是解码器，或者两者都用？如何设计？<br> 对于语言模型，通常使用Transformer的解码器部分，因为解码器能够处理序列数据并生成下一个词元的预测。然而，在某些情况下，也可以结合编码器和解码器，例如在机器翻译任务中。编码器可以处理源语言文本，而解码器则生成目标语言的翻译。</li>
<li>如果输入序列很长，Transformer会面临什么挑战？为什么？<br> 首先，长序列会增加自注意力层的计算复杂度，因为每个时间步都需要与序列中的所有其他时间步进行交互。这会导致显著的计算和内存开销。其次，长序列可能导致梯度消失或爆炸问题，影响模型的训练稳定性和效率。</li>
<li>如何提高Transformer的计算速度和内存使用效率？提示：可以参考论文 (<a target="_blank" rel="noopener" href="https://zh-v2.d2l.ai/chapter_references/zreferences.html#id166" title="Tay, Y., Dehghani, M., Bahri, D., &amp; Metzler, D. (2020). Efficient transformers: a survey. arXiv preprint arXiv:2009.06732.">Tay <em>et al.</em>, 2020</a>)。<br> 可以使用量化来减少模型参数的位宽，从而减少计算和存储需求。还可以使用模型剪枝技术去除不重要的权重，简化模型结构。或使用局部注意力或稀疏注意力。</li>
<li>如果不使用卷积神经网络，如何设计基于Transformer模型的图像分类任务？提示：可以参考Vision Transformer<br> 可以设计Vision Transformer（ViT）来进行图像分类任务。ViT将图像分割成一系列的小块（patches），然后将这些小块线性嵌入到一个序列中，就像处理文本序列一样。然后，使用标准的Transformer架构来处理这个序列，包括自注意力层和前馈网络层。ViT能够捕捉图像中的全局依赖关系，并且在多个图像分类任务上取得了良好的性能。设计时，可以调整块的大小、序列的长度以及Transformer层的配置来适应不同的图像和任务需求。</li>
</ol>
<h1 id="优化算法"><a href="#优化算法" class="headerlink" title="优化算法"></a>优化算法</h1><h2 id="优化和深度学习"><a href="#优化和深度学习" class="headerlink" title="优化和深度学习"></a>优化和深度学习</h2><h3 id="优化的目标"><a href="#优化的目标" class="headerlink" title="优化的目标"></a>优化的目标</h3><p>主要关注最小化目标，由于优化算法发目标函数通常是基于训练数据集发损失函数，因此优化发目标时减少训练误差。但深度学习的目标是减少泛化误差。为了实现后者，除了使用优化算法来减少训练误差之外，还要注意过拟合。</p>
<h3 id="深度学习中的优化挑战"><a href="#深度学习中的优化挑战" class="headerlink" title="深度学习中的优化挑战"></a>深度学习中的优化挑战</h3><p>对于任何目标函数f(x)，如果在x处对应的f(x)值小于在x附近任意其他点的f(x)值，那么f(x)可能是局部最小值。如果f(x)在x处的值是整个域中目标函数的最小值，那么f(x)是全局最小值。</p>
<h3 id="鞍点"><a href="#鞍点" class="headerlink" title="鞍点"></a>鞍点</h3><p>除了局部最小值之外，鞍点是梯度消失的另一个原因。<em>鞍点</em>（saddle point）是指函数的所有梯度都消失但既不是全局最小值也不是局部最小值的任何位置。考虑这个函数$f(x)=x^3$。它的一阶和二阶导数在x=0时消失。这时优化可能会停止，尽管它不是最小值。</p>
<h3 id="练习-44"><a href="#练习-44" class="headerlink" title="练习"></a>练习</h3><ol>
<li>考虑一个简单的MLP，它有一个隐藏层，比如，隐藏层中维度为d和一个输出。证明对于任何局部最小值，至少有d’个等效方案。<br> 我们至少可以构造 d 个等效方案，每个方案对应于隐藏层权重的一种调整。但由于输出层的权重和偏置也可以调整，实际上存在无限多个等效方案。这里的 d′ 可以理解为 d 个隐藏单元提供的基础对称性，而实际的等效方案数量是无限的。</li>
<li>你能想到深度学习优化还涉及哪些其他挑战？<br> 非凸优化问题，训练与验证的偏差，计算资源限制，超参数调整</li>
<li>假设你想在（真实的）鞍上平衡一个（真实的）球。<br> 为什么这很难？<br> <strong>不稳定的平衡点</strong>：鞍点是一个既不是最高点也不是最低点的位置，球在这一点上容易受到扰动而滚动到其他位置，类似于优化过程中的鞍点，模型参数的微小变化可能导致性能显著下降。<br>  <strong>难以识别</strong>：鞍点可能在参数空间中不明显，难以被识别和区分，特别是在高维空间中。<br> <strong>局部最小值的干扰</strong>：鞍点附近可能存在局部最小值，优化算法可能会被这些局部最小值吸引，从而陷入鞍点并难以逃脱。<br> 能利用这种效应来优化算法吗？<br> <strong>使用动量</strong>：动量可以帮助模型在参数空间中保持速度，避免在鞍点附近停滞不前。<br> <strong>学习率调整策略</strong>：通过调整学习率，可以在鞍点附近进行更细致的搜索，有助于模型越过鞍点继续向全局最小值前进。<br> <strong>正则化技术</strong>：正则化可以帮助模型学习更平滑的权重配置，减少在鞍点附近震荡的可能性。<h2 id="凸性"><a href="#凸性" class="headerlink" title="凸性"></a>凸性</h2><h3 id="凸集"><a href="#凸集" class="headerlink" title="凸集"></a>凸集</h3>对于任何$a,b\in X$连接a和b的线段也位于X中，则向量空间中的集合X是凸的。<h3 id="凸函数"><a href="#凸函数" class="headerlink" title="凸函数"></a>凸函数</h3>给定一个凸集X，对于所有x，x‘属于X，和所有$\lambda \in [0,1]$，函数f是凸的，我们可以得到：<script type="math/tex; mode=display">\lambda f(x)+(1-\lambda )f(x')>=f(\lambda x+(1-\lambda)x')</script>余弦函数是非凸的，抛物线和指数函数是凸的。<h3 id="詹森不等式"><a href="#詹森不等式" class="headerlink" title="詹森不等式"></a>詹森不等式</h3><script type="math/tex; mode=display">\sum_{i}^{}a_if(x_i)>=f(\sum_{i}^{}a_ix_i)and E_X[f(X)]>=f(E_X[X])</script>凸函数的期望不小于期望的凸函数，其中后者通常是一个更简单的表达式。<br>应用：用一个较简单的表达式约束一个较复杂的表达式。<h3 id="性质"><a href="#性质" class="headerlink" title="性质"></a>性质</h3>局部极小值是全局极小值<br>凸函数的下水平集是凸的<h3 id="拉格朗日函数"><a href="#拉格朗日函数" class="headerlink" title="拉格朗日函数"></a>拉格朗日函数</h3>通常，求解一个有约束的优化问题是困难的，解决这个问题的一种方法来自物理中相当简单的直觉。 想象一个球在一个盒子里，球会滚到最低的地方，重力将与盒子两侧对球施加的力平衡。 简而言之，目标函数（即重力）的梯度将被约束函数的梯度所抵消（由于墙壁的“推回”作用，需要保持在盒子内）。 请注意，任何不起作用的约束（即球不接触壁）都将无法对球施加任何力。<br>这里我们简略拉格朗日函数L的推导，上述推理可以通过以下鞍点优化问题来表示：<script type="math/tex; mode=display">L(x,a1,a2,...,an)=f(x)+\sum_{i=1}^{n}aici(x) where ai>=0</script><h3 id="练习-45"><a href="#练习-45" class="headerlink" title="练习"></a>练习</h3></li>
<li>假设我们想要通过绘制集合内点之间的所有直线并检查这些直线是否包含来验证集合的凸性。i.证明只检查边界上的点是充分的。ii.证明只检查集合的顶点是充分的。<br> i. 证明只检查边界上的点是充分的：<br> 集合的凸性意味着对于集合中的任意两点，这两点之间的线段都完全位于集合内。要验证一个集合是否为凸集，我们可以从集合内任选两点，并检查这两点之间的线段是否完全在集合内。边界上的点是集合中最接近“边缘”的点，如果集合是凸的，那么边界上的任意两点之间的线段都将完全在集合内，因为任何偏离这条线段的点都将在集合外。因此，如果边界上任意两点之间的线段都在集合内，那么集合内任意两点之间的线段也必然在集合内，因为它们是由边界上的点“夹”在中间的。所以，只检查边界上的点足以验证集合的凸性。<br> ii. 证明只检查集合的顶点是充分的：<br> 顶点是集合边界上的特定点，它们是集合边界的“角”或最尖锐的部分。在凸集的定义中，如果集合是凸的，那么通过集合中任意两点的线段都将完全位于集合内。顶点是确定集合形状的关键点，因为它们定义了集合边界的转向。<br> 如果只检查顶点，我们可以观察顶点之间的连接线段是否完全在集合内。如果所有顶点之间的线段都在集合内，那么可以推断出集合是凸的，因为这些线段覆盖了集合的所有边界。此外，任何不在这些顶点线段上的点都位于这些线段之间，因此也必然在集合内。因此，只检查顶点足以验证集合的凸性。</li>
</ol>
<h2 id="梯度下降"><a href="#梯度下降" class="headerlink" title="梯度下降"></a>梯度下降</h2><h2 id="随机梯度下降"><a href="#随机梯度下降" class="headerlink" title="随机梯度下降"></a>随机梯度下降</h2><h2 id="小批量随机梯度下降"><a href="#小批量随机梯度下降" class="headerlink" title="小批量随机梯度下降"></a>小批量随机梯度下降</h2><h2 id="动量法"><a href="#动量法" class="headerlink" title="动量法"></a>动量法</h2><p>本节将探讨更有效的优化算法，尤其是针对实验中常见的某些类型的优化问题。<br>它旨在帮助加速梯度下降算法在相关方向上的收敛，并抑制在不相关方向上的震荡。动量法通过累积过去梯度的指数衰减平均来实现这一目标。</p>
<h3 id="从零开始实现-1"><a href="#从零开始实现-1" class="headerlink" title="从零开始实现"></a>从零开始实现</h3><p>相比于小批量随机梯度下降，动量方法需要维护一组辅助变量，即速度。 它与梯度以及优化问题的变量具有相同的形状。 在下面的实现中，我们称这些变量为<code>states</code>。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">init_momentum_states</span>(<span class="params">feature_dim</span>):</span><br><span class="line">    v_w = torch.zeros((feature_dim, <span class="number">1</span>))</span><br><span class="line">    v_b = torch.zeros(<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">return</span> (v_w, v_b)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">sgd_momentum</span>(<span class="params">params, states, hyperparams</span>):</span><br><span class="line">    <span class="keyword">for</span> p, v <span class="keyword">in</span> <span class="built_in">zip</span>(params, states):</span><br><span class="line">        <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">            v[:] = hyperparams[<span class="string">&#x27;momentum&#x27;</span>] * v + p.grad</span><br><span class="line">            p[:] -= hyperparams[<span class="string">&#x27;lr&#x27;</span>] * v</span><br><span class="line">        p.grad.data.zero_()</span><br></pre></td></tr></table></figure><br>在实验中运作：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">train_momentum</span>(<span class="params">lr, momentum, num_epochs=<span class="number">2</span></span>):</span><br><span class="line">    d2l.train_ch11(sgd_momentum, init_momentum_states(feature_dim),</span><br><span class="line">                   &#123;<span class="string">&#x27;lr&#x27;</span>: lr, <span class="string">&#x27;momentum&#x27;</span>: momentum&#125;, data_iter,</span><br><span class="line">                   feature_dim, num_epochs)</span><br><span class="line"></span><br><span class="line">data_iter, feature_dim = d2l.get_data_ch11(batch_size=<span class="number">10</span>)</span><br><span class="line">train_momentum(<span class="number">0.02</span>, <span class="number">0.5</span>)</span><br></pre></td></tr></table></figure><br>![[Pasted image 20240408230734.png]]</p>
<h3 id="简介实现-1"><a href="#简介实现-1" class="headerlink" title="简介实现"></a>简介实现</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">trainer = torch.optim.SGD</span><br><span class="line">d2l.train_concise_ch11(trainer, &#123;<span class="string">&#x27;lr&#x27;</span>: <span class="number">0.005</span>, <span class="string">&#x27;momentum&#x27;</span>: <span class="number">0.9</span>&#125;, data_iter)</span><br></pre></td></tr></table></figure>
<h3 id="练习-46"><a href="#练习-46" class="headerlink" title="练习"></a>练习</h3><ol>
<li>当我们执行带动量法的随机梯度下降时会有什么变化？当我们使用带动量法的小批量随机梯度下降时会发生什么？试验参数如何？<br> 带动量法的随机梯度下降（SGD）和小批量随机梯度下降（Mini-batch SGD）是两种常见的优化算法，它们在标准SGD的基础上引入了动量项，以加速训练过程并提高收敛性。下面是这两种方法的特点和变化：</li>
</ol>
<ul>
<li><strong>带动量法的随机梯度下降（SGD with Momentum）</strong>：<ul>
<li>在每次迭代中，动量法不仅考虑当前梯度，还考虑之前梯度的加权平均，这有助于平滑梯度更新路径。</li>
<li>动量项 vt​ 根据当前梯度和之前动量的加权平均进行更新，然后用于更新参数。</li>
<li>这种方法可以减少SGD在优化过程中的震荡，特别是在面对噪声或非平稳目标函数时。</li>
<li>动量法的SGD通常需要调整额外的超参数，即动量系数 γ，以及学习率 α。</li>
</ul>
</li>
<li><strong>带动量法的小批量随机梯度下降（Momentum SGD with Mini-batches）</strong>：<ul>
<li>在这种方法中，每次迭代使用一个小批量数据来计算梯度，然后结合动量项来更新参数。</li>
<li>小批量方法可以提供更稳定的梯度估计，并且可以更有效地利用并行计算资源，如GPU。</li>
<li>动量项同样有助于平滑梯度更新，减少震荡，并加速训练过程。</li>
<li>使用小批量数据时，动量项的更新可能会受到批量大小的影响，因此可能需要根据批量大小调整动量系数 γ 和学习率 α。<h2 id="AdaGrad算法"><a href="#AdaGrad算法" class="headerlink" title="AdaGrad算法"></a>AdaGrad算法</h2>AdaGrad的核心思想是根据每个参数的历史梯度信息来调整其学习率，使得模型在训练过程中能够针对不同的参数采取不同的更新策略。<br>AdaGrad算法的主要特点如下：</li>
</ul>
</li>
</ul>
<ol>
<li><strong>参数特定的学习率</strong>：AdaGrad会为模型中的每个参数分配一个单独的学习率，这样不同的参数可以以不同的速度进行更新。</li>
<li><strong>累积梯度平方和</strong>：对于每个参数，AdaGrad会累积其梯度的平方和，这有助于捕捉到每个参数的更新频率和幅度。</li>
<li><strong>自适应调整</strong>：随着训练的进行，每个参数的学习率会根据其累积的梯度信息自动调整。如果一个参数的梯度在训练过程中变化很大，其学习率会降低；反之，如果梯度变化较小，学习率会增加。</li>
<li><strong>适用性</strong>：AdaGrad特别适合处理稀疏数据集，因为它可以针对数据中出现频率不同的特征采取不同的更新策略。<h3 id="简介实现-2"><a href="#简介实现-2" class="headerlink" title="简介实现"></a>简介实现</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">trainer = torch.optim.Adagrad</span><br><span class="line">d2l.train_concise_ch11(trainer, &#123;<span class="string">&#x27;lr&#x27;</span>: <span class="number">0.1</span>&#125;, data_iter)</span><br></pre></td></tr></table></figure>
<h3 id="练习-47"><a href="#练习-47" class="headerlink" title="练习"></a>练习</h3></li>
<li>要如何修改AdaGrad算法，才能使其在学习率方面的衰减不那么激进？<br> 引入衰减因子，使用学习率预热，动态调整学习率<h2 id="RMSProp算法"><a href="#RMSProp算法" class="headerlink" title="RMSProp算法"></a>RMSProp算法</h2>![[Pasted image 20240408231934.png]]<h3 id="简介实现-3"><a href="#简介实现-3" class="headerlink" title="简介实现"></a>简介实现</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">trainer = torch.optim.RMSprop</span><br><span class="line">d2l.train_concise_ch11(trainer, &#123;<span class="string">&#x27;lr&#x27;</span>: <span class="number">0.01</span>, <span class="string">&#x27;alpha&#x27;</span>: <span class="number">0.9</span>&#125;,</span><br><span class="line">                       data_iter)</span><br></pre></td></tr></table></figure>
<h2 id="Adadelta"><a href="#Adadelta" class="headerlink" title="Adadelta"></a>Adadelta</h2>Adadelta是AdaGrad的另一种变体， 主要区别在于前者减少了学习率适应坐标的数量。 此外，广义上Adadelta被称为没有学习率，因为它使用变化量本身作为未来变化的校准。 <h3 id="简介实现-4"><a href="#简介实现-4" class="headerlink" title="简介实现"></a>简介实现</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">trainer = torch.optim.Adadelta</span><br><span class="line">d2l.train_concise_ch11(trainer, &#123;<span class="string">&#x27;rho&#x27;</span>: <span class="number">0.9</span>&#125;, data_iter)</span><br></pre></td></tr></table></figure>
<h3 id="练习-48"><a href="#练习-48" class="headerlink" title="练习"></a>练习</h3></li>
<li>将Adadelta的收敛行为与AdaGrad和RMSProp进行比较。<br> Adadelta算法是对AdaGrad算法的改进，旨在解决AdaGrad学习率单调递减可能导致过早收敛的问题。<br> AdaGrad通过累积梯度平方来自适应学习率，但可能导致学习率过早下降；RMSProp通过指数加权移动平均来调整学习率，改善了AdaGrad的这一缺点；而Adadelta则进一步改进了自适应学习率机制，通过两个指数加权移动平均来同时控制学习和更新的幅度，使得算法在训练过程中更加稳定和有效。<h2 id="Adam算法"><a href="#Adam算法" class="headerlink" title="Adam算法"></a>Adam算法</h2>Adam（Adaptive Moment Estimation）算法是一种用于优化神经网络的随机梯度下降算法的变种，它结合了动量法（momentum）和自适应学习率的思想。<br>Adam算法的核心思想是根据梯度的一阶矩估计（mean）和二阶矩估计（uncentered variance）来动态调整每个参数的学习率。具体而言，Adam算法会维护两个指数加权移动平均变量，分别表示梯度的一阶矩估计和二阶矩估计。这两个变量分别用来校正梯度的偏差和动态调整学习率。<br>算法步骤如下：</li>
<li>初始化参数θ，一阶矩估计变量m和二阶矩估计变量v。</li>
<li>在每个时间步t：<ul>
<li>计算当前时间步的梯度gt。</li>
<li>更新一阶矩估计变量m和二阶矩估计变量v：<figure class="highlight scss"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">mt = β<span class="number">1</span> * mt-<span class="number">1</span> + (<span class="number">1</span> - β<span class="number">1</span>) * gt</span><br><span class="line">vt = β<span class="number">2</span> * vt-<span class="number">1</span> + (<span class="number">1</span> - β<span class="number">2</span>) * gt^<span class="number">2</span></span><br></pre></td></tr></table></figure></li>
</ul>
</li>
</ol>
<ul>
<li>其中，β1和β2分别是控制一阶矩估计和二阶矩估计的指数衰减率。</li>
<li>根据一阶矩估计的偏差修正mt和vt：<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">m̂t = mt / (<span class="number">1</span> - β<span class="number">1</span>^t)</span><br><span class="line">v̂t = vt / (<span class="number">1</span> - β<span class="number">2</span>^t)</span><br></pre></td></tr></table></figure>
更新参数θ：<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">θt+<span class="number">1</span> = θt - α * m̂t / (sqrt(v̂t) + ε)</span><br></pre></td></tr></table></figure>
<h3 id="练习-49"><a href="#练习-49" class="headerlink" title="练习"></a>练习</h3></li>
</ul>
<ol>
<li>试着重写动量和二次矩更新，从而使其不需要偏差校正。</li>
<li>收敛时为什么需要降低学习率$\eta$？<br> 当模型接近收敛时，学习率的降低有助于使模型在最优解附近更加稳定地收敛。在开始阶段，较大的学习率可以帮助模型更快地接近最优解，但随着训练的进行，学习率的减小可以使模型在最优解周围更精细地调整参数，避免在最优解附近波动。因此，逐渐降低学习率可以提高模型的收敛速度和稳定性。<h2 id="学习率调度器"><a href="#学习率调度器" class="headerlink" title="学习率调度器"></a>学习率调度器</h2></li>
</ol>
<ul>
<li>首先，学习率的大小很重要。如果它太大，优化就会发散；如果它太小，训练就会需要过长时间，或者我们最终只能得到次优的结果。直观地说，这是最不敏感与最敏感方向的变化量的比率。</li>
<li>其次，衰减速率同样很重要。如果学习率持续过高，我们可能最终会在最小值附近弹跳，从而无法达到最优解。</li>
<li>在训练期间逐步降低学习率可以提高准确性，并且减少模型的过拟合。</li>
<li>在实验中，每当进展趋于稳定时就降低学习率，这是很有效的。从本质上说，这可以确保我们有效地收敛到一个适当的解，也只有这样才能通过降低学习率来减小参数的固有方差。</li>
<li>余弦调度器在某些计算机视觉问题中很受欢迎。</li>
<li>优化之前的预热期可以防止发散。</li>
<li>优化在深度学习中有多种用途。对于同样的训练误差而言，选择不同的优化算法和学习率调度，除了最大限度地减少训练时间，可以导致测试集上不同的泛化和过拟合量。<h3 id="练习-50"><a href="#练习-50" class="headerlink" title="练习"></a>练习</h3></li>
</ul>
<ol>
<li>如果改变学习率下降的指数，收敛性会如何改变？在实验中方便起见，使用<code>PolyScheduler</code>。<br> 如果改变学习率下降的指数，收敛性会受到影响。较大的指数会导致学习率下降得更快，可能会导致模型在训练过程中跳过最优解附近的区域，从而影响最终的收敛性。较小的指数会导致学习率下降得更慢，可能需要更多的训练时间才能达到最优解。因此，选择合适的学习率下降指数是重要的，通常需要根据具体问题和模型来调整。</li>
<li>将余弦调度器应用于大型计算机视觉问题，例如训练ImageNet数据集。与其他调度器相比，它如何影响性能？<br> 将余弦调度器应用于大型计算机视觉问题（如训练ImageNet数据集）可以带来一些好处。余弦调度器在训练初期使用较大的学习率，有助于快速收敛到一个比较好的解，然后在训练后期逐渐降低学习率，以更精细地调整参数并提高模型的泛化能力。与其他调度器相比，余弦调度器可以在一定程度上提高模型的性能和泛化能力。</li>
<li>预热应该持续多长时间？<br> 预热的持续时间应该根据具体情况来确定。预热的目的是在训练开始时使用较大的学习率，有助于快速收敛到一个比较好的解。预热时间不宜过长，通常在几个epoch内即可。预热时间过长可能会导致模型在训练初期过度调整参数，影响最终的收敛性能。</li>
<li>可以试着把优化和采样联系起来吗？首先，在随机梯度朗之万动力学上使用的结果。<br> 优化和采样可以通过随机梯度朗之万动力学（SGRLD）来联系。SGRLD是一种融合了随机梯度下降和朗之万动力学的优化算法，它将梯度下降的更新规则与朗之万动力学的随机性相结合，可以更好地处理带噪声的优化问题。在SGRLD中，采样被用来引入随机性，以避免陷入局部最优解，并且可以更好地探索参数空间。<h1 id="计算性能"><a href="#计算性能" class="headerlink" title="计算性能"></a>计算性能</h1>python是一种解释性语言，按顺序执行函数体的操作。通过对e = add(a,b)求值，将结果存储为变量和e。<br>尽管命令式编程很方便，但可能效率不高，一方面因为python会单独执行这三个函数的调用，而没有考虑add函数在fancy_func中倍重复调用。如果在一个GPU上执行这些命令，那么python解释器产生的凯西奥可能会非常大。此外，它需要保存e和f的值，指导函数中所有语句都执行完毕，这是因为程序不知道在执行语句e = add（a,b)和f = add(c,d)之后，其他部分是否会使用变量e和f。<h3 id="符号式编程"><a href="#符号式编程" class="headerlink" title="符号式编程"></a>符号式编程</h3>代码只有在完全定义了过程之后才执行计算：</li>
<li>定义计算流程</li>
<li>将流程编译成可执行的程序</li>
<li>给定输入，调用编译好的程序执行</li>
</ol>
<ul>
<li>命令式编程使得新模型的设计变得容易，因为可以依据控制流编写代码，并拥有相对成熟的Python软件生态。</li>
<li>符号式编程要求我们先定义并且编译程序，然后再执行程序，其好处是提高了计算性能。<h2 id="异步计算"><a href="#异步计算" class="headerlink" title="异步计算"></a>异步计算</h2></li>
<li><p>深度学习框架可以将Python前端的控制与后端的执行解耦，使得命令可以快速地异步插入后端、并行执行。</p>
</li>
<li><p>异步产生了一个相当灵活的前端，但请注意：过度填充任务队列可能会导致内存消耗过多。建议对每个小批量进行同步，以保持前端和后端大致同步。</p>
</li>
<li><p>芯片供应商提供了复杂的性能分析工具，以获得对深度学习效率更精确的洞察。</p>
<h3 id="自动并行"><a href="#自动并行" class="headerlink" title="自动并行"></a>自动并行</h3><h3 id="基于GPU的并行计算"><a href="#基于GPU的并行计算" class="headerlink" title="基于GPU的并行计算"></a>基于GPU的并行计算</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">devices = d2l.try_all_gpus()</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">run</span>(<span class="params">x</span>):</span><br><span class="line">    <span class="keyword">return</span> [x.mm(x) <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">50</span>)]</span><br><span class="line"></span><br><span class="line">x_gpu1 = torch.rand(size=(<span class="number">4000</span>, <span class="number">4000</span>), device=devices[<span class="number">0</span>])</span><br><span class="line">x_gpu2 = torch.rand(size=(<span class="number">4000</span>, <span class="number">4000</span>), device=devices[<span class="number">1</span>])</span><br></pre></td></tr></table></figure>
<h3 id="练习-51"><a href="#练习-51" class="headerlink" title="练习"></a>练习</h3></li>
</ul>
<ol>
<li>设计一个实验，在CPU和GPU这两种设备上使用并行计算和通信。<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建随机矩阵</span></span><br><span class="line">size = <span class="number">1000</span></span><br><span class="line">A = np.random.rand(size, size)</span><br><span class="line">B = np.random.rand(size, size)</span><br><span class="line"></span><br><span class="line"><span class="comment"># CPU上的串行矩阵乘法</span></span><br><span class="line">start_time = time.time()</span><br><span class="line">C_cpu_serial = np.dot(A, B)</span><br><span class="line">cpu_serial_time = time.time() - start_time</span><br><span class="line"></span><br><span class="line"><span class="comment"># GPU上的并行矩阵乘法</span></span><br><span class="line">A_gpu = torch.tensor(A, dtype=torch.float32).cuda()</span><br><span class="line">B_gpu = torch.tensor(B, dtype=torch.float32).cuda()</span><br><span class="line">start_time = time.time()</span><br><span class="line">C_gpu_parallel = torch.mm(A_gpu, B_gpu)</span><br><span class="line">gpu_parallel_time = time.time() - start_time</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将结果从GPU复制回CPU</span></span><br><span class="line">C_gpu_parallel = C_gpu_parallel.cpu().numpy()</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;CPU串行计算时间：<span class="subst">&#123;cpu_serial_time&#125;</span>秒&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;GPU并行计算时间：<span class="subst">&#123;gpu_parallel_time&#125;</span>秒&quot;</span>)</span><br></pre></td></tr></table></figure>
<h2 id="多GPU训练"><a href="#多GPU训练" class="headerlink" title="多GPU训练"></a>多GPU训练</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br><span class="line"><span class="comment">#@save</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">resnet18</span>(<span class="params">num_classes, in_channels=<span class="number">1</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;稍加修改的ResNet-18模型&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">resnet_block</span>(<span class="params">in_channels, out_channels, num_residuals,</span></span><br><span class="line"><span class="params">                     first_block=<span class="literal">False</span></span>):</span><br><span class="line">        blk = []</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(num_residuals):</span><br><span class="line">            <span class="keyword">if</span> i == <span class="number">0</span> <span class="keyword">and</span> <span class="keyword">not</span> first_block:</span><br><span class="line">                blk.append(d2l.Residual(in_channels, out_channels,</span><br><span class="line">                                        use_1x1conv=<span class="literal">True</span>, strides=<span class="number">2</span>))</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                blk.append(d2l.Residual(out_channels, out_channels))</span><br><span class="line">        <span class="keyword">return</span> nn.Sequential(*blk)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 该模型使用了更小的卷积核、步长和填充，而且删除了最大汇聚层</span></span><br><span class="line">    net = nn.Sequential(</span><br><span class="line">        nn.Conv2d(in_channels, <span class="number">64</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>),</span><br><span class="line">        nn.BatchNorm2d(<span class="number">64</span>),</span><br><span class="line">        nn.ReLU())</span><br><span class="line">    net.add_module(<span class="string">&quot;resnet_block1&quot;</span>, resnet_block(</span><br><span class="line">        <span class="number">64</span>, <span class="number">64</span>, <span class="number">2</span>, first_block=<span class="literal">True</span>))</span><br><span class="line">    net.add_module(<span class="string">&quot;resnet_block2&quot;</span>, resnet_block(<span class="number">64</span>, <span class="number">128</span>, <span class="number">2</span>))</span><br><span class="line">    net.add_module(<span class="string">&quot;resnet_block3&quot;</span>, resnet_block(<span class="number">128</span>, <span class="number">256</span>, <span class="number">2</span>))</span><br><span class="line">    net.add_module(<span class="string">&quot;resnet_block4&quot;</span>, resnet_block(<span class="number">256</span>, <span class="number">512</span>, <span class="number">2</span>))</span><br><span class="line">    net.add_module(<span class="string">&quot;global_avg_pool&quot;</span>, nn.AdaptiveAvgPool2d((<span class="number">1</span>,<span class="number">1</span>)))</span><br><span class="line">    net.add_module(<span class="string">&quot;fc&quot;</span>, nn.Sequential(nn.Flatten(),</span><br><span class="line">                                       nn.Linear(<span class="number">512</span>, num_classes)))</span><br><span class="line">    <span class="keyword">return</span> net</span><br><span class="line"></span><br><span class="line">net = resnet18(<span class="number">10</span>)</span><br><span class="line"><span class="comment"># 获取GPU列表</span></span><br><span class="line">devices = d2l.try_all_gpus()</span><br><span class="line"><span class="comment"># 我们将在训练代码实现中初始化网络</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">train</span>(<span class="params">net, num_gpus, batch_size, lr</span>):</span><br><span class="line">    train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)</span><br><span class="line">    devices = [d2l.try_gpu(i) <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(num_gpus)]</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">init_weights</span>(<span class="params">m</span>):</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">type</span>(m) <span class="keyword">in</span> [nn.Linear, nn.Conv2d]:</span><br><span class="line">            nn.init.normal_(m.weight, std=<span class="number">0.01</span>)</span><br><span class="line">    net.apply(init_weights)</span><br><span class="line">    <span class="comment"># 在多个GPU上设置模型</span></span><br><span class="line">    net = nn.DataParallel(net, device_ids=devices)</span><br><span class="line">    trainer = torch.optim.SGD(net.parameters(), lr)</span><br><span class="line">    loss = nn.CrossEntropyLoss()</span><br><span class="line">    timer, num_epochs = d2l.Timer(), <span class="number">10</span></span><br><span class="line">    animator = d2l.Animator(<span class="string">&#x27;epoch&#x27;</span>, <span class="string">&#x27;test acc&#x27;</span>, xlim=[<span class="number">1</span>, num_epochs])</span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epochs):</span><br><span class="line">        net.train()</span><br><span class="line">        timer.start()</span><br><span class="line">        <span class="keyword">for</span> X, y <span class="keyword">in</span> train_iter:</span><br><span class="line">            trainer.zero_grad()</span><br><span class="line">            X, y = X.to(devices[<span class="number">0</span>]), y.to(devices[<span class="number">0</span>])</span><br><span class="line">            l = loss(net(X), y)</span><br><span class="line">            l.backward()</span><br><span class="line">            trainer.step()</span><br><span class="line">        timer.stop()</span><br><span class="line">        animator.add(epoch + <span class="number">1</span>, (d2l.evaluate_accuracy_gpu(net, test_iter),))</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;测试精度：<span class="subst">&#123;animator.Y[<span class="number">0</span>][-<span class="number">1</span>]:<span class="number">.2</span>f&#125;</span>，<span class="subst">&#123;timer.avg():<span class="number">.1</span>f&#125;</span>秒/轮，&#x27;</span></span><br><span class="line">          <span class="string">f&#x27;在<span class="subst">&#123;<span class="built_in">str</span>(devices)&#125;</span>&#x27;</span>)</span><br><span class="line"><span class="comment"># 使用1个</span></span><br><span class="line">train(net, num_gpus=<span class="number">1</span>, batch_size=<span class="number">256</span>, lr=<span class="number">0.1</span>)</span><br><span class="line"><span class="comment"># 使用2个</span></span><br><span class="line">train(net, num_gpus=<span class="number">2</span>, batch_size=<span class="number">512</span>, lr=<span class="number">0.2</span>)</span><br></pre></td></tr></table></figure>
<h2 id="参数服务器"><a href="#参数服务器" class="headerlink" title="参数服务器"></a>参数服务器</h2>当我们从一个GPU迁移到多个GPU时，以及再迁移到包含多个GPU的多个服务器时（可能所有服务器的分布跨越了多个机架和多个网络交换机），分布式并行训练算法也需要变得更加复杂。</li>
</ol>
<ul>
<li>同步需要高度适应特定的网络基础设施和服务器内的连接，这种适应会严重影响同步所需的时间。</li>
<li>环同步对于p3和DGX-2服务器是最佳的，而对于其他服务器则未必。</li>
<li>当添加多个参数服务器以增加带宽时，分层同步策略可以工作的很好。<h3 id="练习-52"><a href="#练习-52" class="headerlink" title="练习"></a>练习</h3></li>
</ul>
<ol>
<li>请尝试进一步提高环同步的性能吗。（提示：可以双向发送消息。）<br> 可以尝试使用双向发送消息。在传统的环同步中，每个进程在每个阶段只能发送或接收消息，而双向发送消息可以使得进程在每个阶段既可以发送也可以接收消息，从而提高了通信的效率。这样可以减少通信的次数，加快算法的收敛速度。</li>
<li>在计算仍在进行中，可否允许执行异步通信？它将如何影响性能？<br> 允许在计算仍在进行中执行异步通信可以提高性能，因为它可以使得计算和通信重叠进行，减少了计算和通信之间的等待时间。但是，需要注意的是，在使用异步通信时需要确保通信操作不会影响计算的正确性。</li>
<li>怎样处理在长时间运行的计算过程中丢失了一台服务器这种问题？尝试设计一种容错机制来避免重启计算这种解决方案？<br> 可以通过设计容错机制来避免重启计算。一种常见的容错机制是使用检查点和恢复技术，定期保存计算状态到持久存储器中，以便在发生故障时能够重新启动计算。另一种方法是使用冗余计算节点，在计算过程中同时在多个节点上执行相同的计算任务，当某个节点发生故障时，可以从其他节点恢复计算。<h1 id="计算机视觉"><a href="#计算机视觉" class="headerlink" title="计算机视觉"></a>计算机视觉</h1></li>
</ol>
<h2 id="图像增广"><a href="#图像增广" class="headerlink" title="图像增广"></a>图像增广</h2><p>我们可以以不同的方式裁剪图像，使感兴趣的对象出现在不同的位置，减少模型对于对象出现位置的依赖。 我们还可以调整亮度、颜色等因素来降低模型对颜色的敏感度。</p>
<h3 id="常用的方法"><a href="#常用的方法" class="headerlink" title="常用的方法"></a>常用的方法</h3><p>翻转和裁剪<br>改变颜色：亮度，对比度，饱和度，色调。</p>
<h3 id="使用图像增广进行训练"><a href="#使用图像增广进行训练" class="headerlink" title="使用图像增广进行训练"></a>使用图像增广进行训练</h3><h3 id="练习-53"><a href="#练习-53" class="headerlink" title="练习"></a>练习</h3><ol>
<li>在不使用图像增广的情况下训练模型：<code>train_with_data_aug(no_aug, no_aug)</code>。比较使用和不使用图像增广的训练结果和测试精度。这个对比实验能支持图像增广可以减轻过拟合的论点吗？为什么？<br> 对比实验的结果可以支持图像增广可以减轻过拟合的论点。在实验中，使用图像增广的模型通常会在训练集上表现更好，同时在测试集上也能取得更好的泛化性能，即使在没有使用图像增广的情况下，模型可能会出现过拟合的现象。<br> 图像增广可以减轻过拟合的原因在于它可以增加训练数据的多样性，从而使得模型更加鲁棒。通过对训练图像进行随机变换，图像增广可以生成更多样化的训练样本，使得模型不容易记住训练集中的特定样本，从而降低过拟合的风险。</li>
<li>在基于CIFAR-10数据集的模型训练中结合多种不同的图像增广方法。它能提高测试准确性吗？<br> 在基于CIFAR-10数据集的模型训练中结合多种不同的图像增广方法可以提高测试准确性。通过使用多种不同的图像增广方法，可以进一步增加训练数据的多样性，使得模型更加鲁棒，从而提高模型在测试集上的泛化能力。</li>
<li>参阅深度学习框架的在线文档。它还提供了哪些其他的图像增广方法？<br> 平移（仿射变换），随即擦除，尺度变换<h2 id="微调"><a href="#微调" class="headerlink" title="微调"></a>微调</h2>使用迁移学习，从元数据学到的知识迁移到目标数据集。尽管Imagenet上数据集大多数与意思无关，但在此数据集上训练的模型可能会提取更通用的图像特征，这有助于识别边缘、纹理、形状和对象组合。<br>微调包含以下步骤：</li>
<li>在源数据集（例如ImageNet数据集）上预训练神经网络模型，即源模型。</li>
<li>创建一个新的神经网络模型，即目标模型。这将复制源模型上的所有模型设计及其参数（输出层除外）。我们假定这些模型参数包含从源数据集中学到的知识，这些知识也将适用于目标数据集。我们还假设源模型的输出层与源数据集的标签密切相关；因此不在目标模型中使用该层。</li>
<li>向目标模型添加输出层，其输出数是目标数据集中的类别数。然后随机初始化该层的模型参数。</li>
<li>在目标数据集（如椅子数据集）上训练目标模型。输出层将从头开始进行训练，而所有其他层的参数将根据源模型的参数进行微调。<h2 id="目标检测和边界框"><a href="#目标检测和边界框" class="headerlink" title="目标检测和边界框"></a>目标检测和边界框</h2><h3 id="边界框"><a href="#边界框" class="headerlink" title="边界框"></a>边界框</h3>在目标检测中，我们通常使用边界框来描述对象的空间位置。边界框是举行的，由鞠总左上角和右下角的xy坐标来决定另一种是边界框中心坐标和框的宽度和高度。<br>可以设计函数将两种表示方法进行转换，并在图像中可视化。<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">x = torch.tensor([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>])</span><br><span class="line">y = torch.tensor([<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>])</span><br><span class="line"><span class="comment"># 在新维度上堆叠张量</span></span><br><span class="line">stacked = torch.stack([x, y])</span><br><span class="line"><span class="comment"># 输出: tensor([[1, 2, 3],</span></span><br><span class="line"><span class="comment">#                [4, 5, 6]])</span></span><br><span class="line">stacked = torch.stack([x, y], dim=<span class="number">1</span>) </span><br><span class="line"><span class="built_in">print</span>(stacked) </span><br><span class="line"><span class="comment"># 输出: tensor([[1, 4], </span></span><br><span class="line"><span class="comment"># [2, 5], </span></span><br><span class="line"><span class="comment"># [3, 6]])</span></span><br></pre></td></tr></table></figure></li>
<li><code>torch.stack</code>会在新维度上堆叠张量，而<code>torch.cat</code>会在现有维度上拼接张量。</li>
<li><code>torch.stack</code>要求所有要堆叠的张量具有相同的形状，而<code>torch.cat</code>要求除了沿着指定维度之外的其他维度具有相同的形状。<h2 id="锚框"><a href="#锚框" class="headerlink" title="锚框"></a>锚框</h2>目标检测算法通常会在输入图像中采样大量的区域，然后判断这些区域中是否包含我们感兴趣的目标，并调整区域边界从而更准确地预测目标的真实边界框（ground-truth bounding box）。 不同的模型使用的区域采样方法可能不同。 这里我们介绍其中的一种方法：以每个像素为中心，生成多个缩放比和宽高比（aspect ratio）不同的边界框。 这些边界框被称为<em>锚框</em>（anchor box）。<h3 id="生成多个锚框"><a href="#生成多个锚框" class="headerlink" title="生成多个锚框"></a>生成多个锚框</h3>要生成多个不同形状的锚框，让我们设置许多缩放比（scale）取值s1,…,sn和许多宽高比（aspect ratio）取值r1,…,rm。 当使用这些比例和长宽比的所有组合以每个像素为中心时，输入图像将总共有wℎnm个锚框。 尽管这些锚框可能会覆盖所有真实边界框，但计算复杂性很容易过高。 在实践中，我们只考虑包含s1或r1的组合：<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#@save</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">multibox_prior</span>(<span class="params">data, sizes, ratios</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;生成以每个像素为中心具有不同形状的锚框&quot;&quot;&quot;</span></span><br><span class="line">    in_height, in_width = data.shape[-<span class="number">2</span>:]</span><br><span class="line">    device, num_sizes, num_ratios = data.device, <span class="built_in">len</span>(sizes), <span class="built_in">len</span>(ratios)</span><br><span class="line">    boxes_per_pixel = (num_sizes + num_ratios - <span class="number">1</span>)</span><br><span class="line">    size_tensor = torch.tensor(sizes, device=device)</span><br><span class="line">    ratio_tensor = torch.tensor(ratios, device=device)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 为了将锚点移动到像素的中心，需要设置偏移量。</span></span><br><span class="line">    <span class="comment"># 因为一个像素的高为1且宽为1，我们选择偏移我们的中心0.5</span></span><br><span class="line">    offset_h, offset_w = <span class="number">0.5</span>, <span class="number">0.5</span></span><br><span class="line">    steps_h = <span class="number">1.0</span> / in_height  <span class="comment"># 在y轴上缩放步长</span></span><br><span class="line">    steps_w = <span class="number">1.0</span> / in_width  <span class="comment"># 在x轴上缩放步长</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 生成锚框的所有中心点</span></span><br><span class="line">    center_h = (torch.arange(in_height, device=device) + offset_h) * steps_h</span><br><span class="line">    center_w = (torch.arange(in_width, device=device) + offset_w) * steps_w</span><br><span class="line">    shift_y, shift_x = torch.meshgrid(center_h, center_w, indexing=<span class="string">&#x27;ij&#x27;</span>)</span><br><span class="line">    shift_y, shift_x = shift_y.reshape(-<span class="number">1</span>), shift_x.reshape(-<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 生成“boxes_per_pixel”个高和宽，</span></span><br><span class="line">    <span class="comment"># 之后用于创建锚框的四角坐标(xmin,xmax,ymin,ymax)</span></span><br><span class="line">    w = torch.cat((size_tensor * torch.sqrt(ratio_tensor[<span class="number">0</span>]),</span><br><span class="line">                   sizes[<span class="number">0</span>] * torch.sqrt(ratio_tensor[<span class="number">1</span>:])))\</span><br><span class="line">                   * in_height / in_width  <span class="comment"># 处理矩形输入</span></span><br><span class="line">    h = torch.cat((size_tensor / torch.sqrt(ratio_tensor[<span class="number">0</span>]),</span><br><span class="line">                   sizes[<span class="number">0</span>] / torch.sqrt(ratio_tensor[<span class="number">1</span>:])))</span><br><span class="line">    <span class="comment"># 除以2来获得半高和半宽</span></span><br><span class="line">    anchor_manipulations = torch.stack((-w, -h, w, h)).T.repeat(</span><br><span class="line">                                        in_height * in_width, <span class="number">1</span>) / <span class="number">2</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 每个中心点都将有“boxes_per_pixel”个锚框，</span></span><br><span class="line">    <span class="comment"># 所以生成含所有锚框中心的网格，重复了“boxes_per_pixel”次</span></span><br><span class="line">    out_grid = torch.stack([shift_x, shift_y, shift_x, shift_y],</span><br><span class="line">                dim=<span class="number">1</span>).repeat_interleave(boxes_per_pixel, dim=<span class="number">0</span>)</span><br><span class="line">    output = out_grid + anchor_manipulations</span><br><span class="line">    <span class="keyword">return</span> output.unsqueeze(<span class="number">0</span>)</span><br></pre></td></tr></table></figure>
<h3 id="交并比（IoU）"><a href="#交并比（IoU）" class="headerlink" title="交并比（IoU）"></a>交并比（IoU）</h3>相交面积除以相并面积<h3 id="练习-54"><a href="#练习-54" class="headerlink" title="练习"></a>练习</h3></li>
<li>在multibox_prior函数中更改sizes和ratios的值。生成的锚框有什么变化？<br> 在<code>multibox_prior</code>函数中更改<code>sizes</code>和<code>ratios</code>的值会改变生成的锚框的大小和长宽比。<code>sizes</code>控制锚框的大小，<code>ratios</code>控制锚框的长宽比。更改这些参数会导致生成的锚框在大小和形状上有所变化。</li>
<li>构建并可视化两个IoU为0.5的边界框。它们是怎样重叠的？ <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.patches <span class="keyword">as</span> patches</span><br><span class="line"><span class="comment"># 创建两个边界框的坐标</span></span><br><span class="line">bbox1 = [<span class="number">0.3</span>, <span class="number">0.3</span>, <span class="number">0.6</span>, <span class="number">0.6</span>]  <span class="comment"># (xmin, ymin, xmax, ymax)</span></span><br><span class="line">bbox2 = [<span class="number">0.5</span>, <span class="number">0.5</span>, <span class="number">0.8</span>, <span class="number">0.8</span>]</span><br><span class="line"><span class="comment"># 计算两个边界框的重叠部分</span></span><br><span class="line">overlap_xmin = <span class="built_in">max</span>(bbox1[<span class="number">0</span>], bbox2[<span class="number">0</span>])</span><br><span class="line">overlap_ymin = <span class="built_in">max</span>(bbox1[<span class="number">1</span>], bbox2[<span class="number">1</span>])</span><br><span class="line">overlap_xmax = <span class="built_in">min</span>(bbox1[<span class="number">2</span>], bbox2[<span class="number">2</span>])</span><br><span class="line">overlap_ymax = <span class="built_in">min</span>(bbox1[<span class="number">3</span>], bbox2[<span class="number">3</span>])</span><br><span class="line"><span class="comment"># 计算重叠部分的面积</span></span><br><span class="line">overlap_area = <span class="built_in">max</span>(<span class="number">0</span>, overlap_xmax - overlap_xmin) * <span class="built_in">max</span>(<span class="number">0</span>, overlap_ymax - overlap_ymin)</span><br><span class="line"><span class="comment"># 计算两个边界框的面积</span></span><br><span class="line">area1 = (bbox1[<span class="number">2</span>] - bbox1[<span class="number">0</span>]) * (bbox1[<span class="number">3</span>] - bbox1[<span class="number">1</span>])</span><br><span class="line">area2 = (bbox2[<span class="number">2</span>] - bbox2[<span class="number">0</span>]) * (bbox2[<span class="number">3</span>] - bbox2[<span class="number">1</span>])</span><br><span class="line"><span class="comment"># 计算IoU</span></span><br><span class="line">iou = overlap_area / (area1 + area2 - overlap_area)</span><br><span class="line"><span class="comment"># 可视化边界框</span></span><br><span class="line">fig, ax = plt.subplots()</span><br><span class="line">ax.add_patch(patches.Rectangle((bbox1[<span class="number">0</span>], bbox1[<span class="number">1</span>]), bbox1[<span class="number">2</span>] - bbox1[<span class="number">0</span>], bbox1[<span class="number">3</span>] - bbox1[<span class="number">1</span>], fill=<span class="literal">False</span>))</span><br><span class="line">ax.add_patch(patches.Rectangle((bbox2[<span class="number">0</span>], bbox2[<span class="number">1</span>]), bbox2[<span class="number">2</span>] - bbox2[<span class="number">0</span>], bbox2[<span class="number">3</span>] - bbox2[<span class="number">1</span>], fill=<span class="literal">False</span>))</span><br><span class="line">plt.xlim(<span class="number">0</span>, <span class="number">1</span>)</span><br><span class="line">plt.ylim(<span class="number">0</span>, <span class="number">1</span>)</span><br><span class="line">plt.gca().invert_yaxis()  <span class="comment"># 反转y轴，使得原点在左上角</span></span><br><span class="line">plt.show()</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;IoU: <span class="subst">&#123;iou&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure></li>
<li>非极大值抑制是一种贪心算法，它通过移除来抑制预测的边界框。是否存在一种可能，被移除的一些框实际上是有用的？<br> 对于非极大值抑制（NMS），确实存在一些情况下被移除的边界框实际上是有用的。为了柔和地抑制，可以使用Soft-NMS算法，该算法通过减少重叠边界框的得分而不是完全删除它们来实现更平滑的抑制。Soft-NMS的关键思想是根据重叠的程度逐渐降低边界框的得分，而不是直接将其移除。</li>
<li>如何修改这个算法来柔和地抑制？可以参考Soft-NMS (Bodla et al., 2017)。 如果非手动，非最大限度的抑制可以被学习吗？<br> 非手动、非最大限度的抑制可以被学习，这通常通过训练一个边界框回归器来实现。边界框回归器的目标是预测每个边界框的位置和大小，从而使得最终的检测结果更加准确。这种方法可以在训练过程中学习到更有效的抑制策略，而不是简单地使用固定的阈值或规则来选择最佳边界框。<h2 id="多尺度目标检测"><a href="#多尺度目标检测" class="headerlink" title="多尺度目标检测"></a>多尺度目标检测</h2>如果为每个像素都生成的锚框，我们最终可能会得到太多需要计算的锚框。<br>减少图像上的锚框数量并不困难。 比如，我们可以在输入图像中均匀采样一小部分像素，并以它们为中心生成锚框。 此外，在不同尺度下，我们可以生成不同数量和不同大小的锚框。 直观地说，比起较大的目标，较小的目标在图像上出现的可能性更多样。 例如，1×1、1×2和2×2的目标可以分别以4、2和1种可能的方式出现在2×2图像上。 因此，当使用较小的锚框检测较小的物体时，我们可以采样更多的区域，而对于较大的物体，我们可以采样较少的区域。</li>
</ol>
<ul>
<li>在多个尺度下，我们可以生成不同尺寸的锚框来检测不同尺寸的目标。</li>
<li>通过定义特征图的形状，我们可以决定任何图像上均匀采样的锚框的中心。</li>
<li>我们使用输入图像在某个感受野区域内的信息，来预测输入图像上与该区域位置相近的锚框类别和偏移量。</li>
<li>我们可以通过深入学习，在多个层次上的图像分层表示进行多尺度目标检测。<h3 id="练习-55"><a href="#练习-55" class="headerlink" title="练习"></a>练习</h3></li>
</ul>
<ol>
<li>给定形状为1×c×ℎ×w的特征图变量，其中c、ℎ和w分别是特征图的通道数、高度和宽度。怎样才能将这个变量转换为锚框类别和偏移量？输出的形状是什么？<br> 使用一个1×1的卷积层将特征图变量转换为形状为1×(4k+c)×h×w的特征图。这个卷积层的输出通道数应为4k+c。将这个特征图展平为形状为1×((4k+c)×h×w)的向量。<br> 使用一个全连接层将展平后的向量转换为形状为1×((4k+1)×h×w)的向量。这个全连接层的输出大小应为(4k+1)×h×w，其中4k是偏移量的数量，1是类别预测的数量。<br> 将全连接层的输出重新整形为形状为1×(4k+1)×h×w的特征图，其中4k+1是每个位置的类别和偏移量的总数。<br> 最终输出的形状是1×(4k+1)×h×w，其中1表示批次大小为1。这个特征图包含每个位置的锚框类别和偏移量预测。<h2 id="目标检测数据集"><a href="#目标检测数据集" class="headerlink" title="目标检测数据集"></a>目标检测数据集</h2><h2 id="单发多框检测"><a href="#单发多框检测" class="headerlink" title="单发多框检测"></a>单发多框检测</h2>![[Pasted image 20240409202220.png]]<h2 id="区域卷积神经网络"><a href="#区域卷积神经网络" class="headerlink" title="区域卷积神经网络"></a>区域卷积神经网络</h2>区域卷积神经网络（R-CNN）系列是一系列用于目标检测的深度学习模型，主要包括R-CNN、Fast R-CNN、Faster R-CNN和Mask R-CNN等。这些模型在目标检测领域取得了重大突破，成为了目标检测任务中的经典模型。</li>
<li>R-CNN（Region-based Convolutional Neural Network）：R-CNN是最早的一种区域卷积神经网络模型，它通过选择性搜索（Selective Search）算法提取候选区域，并对每个候选区域进行卷积神经网络的特征提取和目标分类，从而实现目标检测。但是，R-CNN的速度较慢，因为它需要对每个候选区域分别进行卷积运算。</li>
<li>Fast R-CNN：Fast R-CNN对R-CNN进行了改进，提出了候选区域池化（RoI Pooling）层，将整个图像的特征图输入到卷积神经网络中，然后通过RoI Pooling层将每个候选区域映射到特征图上，并提取固定大小的特征。这样可以避免对每个候选区域都进行卷积运算，提高了速度和效率。</li>
<li>Faster R-CNN：Faster R-CNN在Fast R-CNN的基础上进一步改进，引入了区域提议网络（Region Proposal Network，RPN），用于生成候选区域。RPN通过在特征图上滑动一个小窗口来生成候选区域，并利用分类分支和回归分支对候选区域进行分类和精细化定位。这样可以将目标检测任务分为两个阶段：生成候选区域和目标分类定位，从而进一步提高了速度和效率。</li>
<li>Mask R-CNN：Mask R-CNN在Faster R-CNN的基础上添加了一个额外的分支用于实例分割。在目标检测的基础上，Mask R-CNN可以生成每个检测到的目标的精确掩码，从而实现目标的精确分割。<br>R-CNN系列模型的发展，使得目标检测在准确率和效率上取得了巨大的提升，成为了目标检测领域的重要里程碑。<h3 id="练习-56"><a href="#练习-56" class="headerlink" title="练习"></a>练习</h3></li>
<li><p>我们能否将目标检测视为回归问题（例如预测边界框和类别的概率）？可以参考YOLO模型 (<a target="_blank" rel="noopener" href="https://zh-v2.d2l.ai/chapter_references/zreferences.html#id135" title="Redmon, J., Divvala, S., Girshick, R., &amp; Farhadi, A. (2016). You only look once: unified, real-time object detection. Proceedings of the IEEE conference on computer vision and pattern recognition (pp. 779–788).">Redmon <em>et al.</em>, 2016</a>)的设计。<br> 目标检测可以被视为回归问题，其中目标是预测边界框的位置和大小，以及每个边界框内物体类别的概率。YOLO模型（You Only Look Once）是一个将目标检测视为回归问题的经典模型，它在单个神经网络中同时预测多个边界框的位置和类别概率。</p>
<p> 与YOLO相比，单发多框检测（Single Shot MultiBox Detection，SSD）也是一种将目标检测视为回归问题的方法，但它采用了不同的设计思路。主要区别包括：</p>
<ol>
<li><p>网络结构：YOLO采用全卷积网络结构，将整个图像作为输入并直接输出预测边界框和类别概率的特征图。而SSD在特征图上应用一系列卷积和池化操作，然后在不同层次上预测不同尺度和长宽比的边界框。</p>
</li>
<li><p>多尺度特征：SSD利用不同层次的特征图来检测不同尺度的物体，从而提高了检测的准确性。而YOLO将所有预测都放在单个特征图上，可能会导致对小物体的检测效果不佳。</p>
</li>
<li><p>预测方式：YOLO使用单个全连接层来预测边界框和类别概率，而SSD在不同层次上使用卷积层来预测，这样可以提高模型对不同尺度物体的检测能力。</p>
</li>
</ol>
</li>
</ol>
<h2 id="语义分割和数据集"><a href="#语义分割和数据集" class="headerlink" title="语义分割和数据集"></a>语义分割和数据集</h2><p>计算机视觉领域还有2个与语义分割相似的重要问题，即<em>图像分割</em>（image segmentation）和<em>实例分割</em>（instance segmentation）。 我们在这里将它们同语义分割简单区分一下。</p>
<ul>
<li>图像分割：将图像划分为若干组成区域，这类问题的方法通常利用图像中像素之间的相关性。它在训练时不需要有关图像像素的标签信息，在预测时也无法保证分割出的区域具有我们希望得到的语义。</li>
<li>实例分割也叫同时检测并分割（simultaneous detection and segmentation），它研究如何识别图像中各个目标实例的像素级区域。与语义分割不同，实例分割不仅需要区分语义，还要区分不同的目标实例。例如，如果图像中有两条狗，则实例分割需要区分像素属于的两条狗中的哪一条。<h3 id="数据集"><a href="#数据集" class="headerlink" title="数据集"></a>数据集</h3>Pascal VOC2012 语义分割数据集<h3 id="练习-57"><a href="#练习-57" class="headerlink" title="练习"></a>练习</h3></li>
</ul>
<ol>
<li>如何在自动驾驶和医疗图像诊断中应用语义分割？还能想到其他领域的应用吗？<br> 语义分割可以帮助自动驾驶系统理解道路上不同区域的含义，如车道线、行人、车辆和路标等。这对于决策制定和车辆行驶路径规划非常重要。<br> 语义分割可以帮助医生准确地识别和分割出不同的组织结构或病变区域，如肿瘤、器官等，从而提高诊断的准确性和效率。<br> 农业，建筑与城市规划，环境保护</li>
<li>回想一下 <a target="_blank" rel="noopener" href="https://zh-v2.d2l.ai/chapter_computer-vision/image-augmentation.html#sec-image-augmentation">13.1节</a>中对数据增强的描述。图像分类中使用的哪种图像增强方法是难以用于语义分割的？<br> 随机裁剪和随机翻转，对于语义分割来说并不适用。因为这些方法改变了图像的空间信息和像素分布，可能会导致语义分割结果不准确。<h2 id="转置卷积"><a href="#转置卷积" class="headerlink" title="转置卷积"></a>转置卷积</h2>用于逆转下采样导致的空间尺寸减少<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">tconv = nn.ConvTranspose2d(<span class="number">1</span>, <span class="number">1</span>, kernel_size=<span class="number">2</span>, padding=<span class="number">1</span>, bias=<span class="literal">False</span>)</span><br><span class="line">tconv.weight.data = K</span><br><span class="line">tconv(X)</span><br></pre></td></tr></table></figure>
与常规卷积不同，在转置卷积中，填充被应用于的输出（常规卷积将填充应用于输入）。 例如，当将高和宽两侧的填充数指定为1时，转置卷积的输出中将删除第一和最后的行与列。<h3 id="练习-58"><a href="#练习-58" class="headerlink" title="练习"></a>练习</h3></li>
<li>在 <a target="_blank" rel="noopener" href="https://zh-v2.d2l.ai/chapter_computer-vision/transposed-conv.html#subsec-connection-to-mat-transposition">13.10.3节</a>中，卷积输入<code>X</code>和转置的卷积输出<code>Z</code>具有相同的形状。他们的数值也相同吗？为什么？<br> 不相同</li>
<li>使用矩阵乘法来实现卷积是否有效率？为什么？<br> 使用矩阵乘法来实现卷积在某些情况下可能是有效率的，但在一般情况下通常不是最优的选择。这是因为卷积操作通常涉及大量的参数和稀疏权重，而矩阵乘法则会将这些稀疏性转换为密集计算，导致计算量增加。<h2 id="全卷积网络"><a href="#全卷积网络" class="headerlink" title="全卷积网络"></a>全卷积网络</h2>如 <a target="_blank" rel="noopener" href="https://zh-v2.d2l.ai/chapter_computer-vision/semantic-segmentation-and-dataset.html#sec-semantic-segmentation">13.9节</a>中所介绍的那样，语义分割是对图像中的每个像素分类。 <em>全卷积网络</em>（fully convolutional network，FCN）采用卷积神经网络实现了从图像像素到像素类别的变换 (<a target="_blank" rel="noopener" href="https://zh-v2.d2l.ai/chapter_references/zreferences.html#id100" title="Long, J., Shelhamer, E., &amp; Darrell, T. (2015). Fully convolutional networks for semantic segmentation. Proceedings of the IEEE conference on computer vision and pattern recognition (pp. 3431–3440).">Long <em>et al.</em>, 2015</a>)。 与我们之前在图像分类或目标检测部分介绍的卷积神经网络不同，全卷积网络将中间层特征图的高和宽变换回输入图像的尺寸：这是通过在 <a target="_blank" rel="noopener" href="https://zh-v2.d2l.ai/chapter_computer-vision/transposed-conv.html#sec-transposed-conv">13.10节</a>中引入的<em>转置卷积</em>（transposed convolution）实现的。 因此，输出的类别预测与输入图像在像素级别上具有一一对应关系：通道维的输出即该位置对应像素的类别预测。<h3 id="模型-1"><a href="#模型-1" class="headerlink" title="模型"></a>模型</h3>全卷积最基本的设计：<br>全卷积网络先使用卷积神经网络抽取图像特征，然后通过1×1卷积层将通道数变换为类别个数。<h3 id="练习-59"><a href="#练习-59" class="headerlink" title="练习"></a>练习</h3></li>
<li>如果将转置卷积层改用Xavier随机初始化，结果有什么变化？<br> 通过使用Xavier随机初始化，转置卷积层的权重将以一种更合理的方式初始化，有助于加速模型的收敛速度，并可能提高模型的性能。具体效果取决于网络的结构、数据集和其他超参数的设置。<h2 id="风格迁移"><a href="#风格迁移" class="headerlink" title="风格迁移"></a>风格迁移</h2>首先，我们初始化合成图像，例如将其初始化为内容图像。 该合成图像是风格迁移过程中唯一需要更新的变量，即风格迁移所需迭代的模型参数。 然后，我们选择一个预训练的卷积神经网络来抽取图像的特征，其中的模型参数在训练中无须更新。 这个深度卷积神经网络凭借多个层逐级抽取图像的特征，我们可以选择其中某些层的输出作为内容特征或风格特征。这里选取的预训练的神经网络含有3个卷积层，其中第二层输出内容特征，第一层和第三层输出风格特征。<br>![[Pasted image 20240409223638.png]]<h3 id="练习-60"><a href="#练习-60" class="headerlink" title="练习"></a>练习</h3></li>
<li>选择不同的内容和风格层，输出有什么变化？<br> 选择更靠近网络底层的内容层可以保留更多的图像内容，而选择更靠近网络顶层的风格层可以强调更多的风格特征。</li>
<li>调整损失函数中的权重超参数。输出是否保留更多内容或减少更多噪点？<br> 增加内容损失的权重可能会使输出更接近于内容图像，减少噪点；而增加风格损失的权重可能会使输出更接近于风格图像，但也可能导致一些失真或不自然的效果。</li>
<li>替换实验中的内容图像和风格图像，能创作出更有趣的合成图像吗？<br> 可以</li>
<li>我们可以对文本使用风格迁移吗？<br> 在文本领域，风格迁移可以应用于文本生成、翻译和修改等任务，以生成具有不同风格的文本。<br>我的实验：<br>希望将如下两张图片进行风格迁移：<br>![[3.png]]<br>![[2.jpg]]<br>![[Pasted image 20240410004055.png]]</li>
</ol>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="http://wwffyy.life">whisper</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="http://wwffyy.life/posts/1c7f6819.html">http://wwffyy.life/posts/1c7f6819.html</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="http://wwffyy.life" target="_blank">启蛰海</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"></div><div class="post_share"><div class="social-share" data-image="https://s21.ax1x.com/2024/03/29/pFosCVS.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="next-post pull-full"><a href="/posts/11203c58.html" title="sam入门"><img class="cover" src="https://pic.imgdb.cn/item/659a2491871b83018a4100da.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">sam入门</div></div></a></div></nav></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="https://s21.ax1x.com/2024/03/29/pFosCVS.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">whisper</div><div class="author-info__description">like : 摄影、篮球、钢琴</div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">9</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">0</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">0</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/gitcat-404"><i class="fab fa-github"></i><span>Follow Me</span></a></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">努力成为技术大佬</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E9%A2%84%E5%A4%87%E7%9F%A5%E8%AF%86"><span class="toc-number">1.</span> <span class="toc-text">预备知识</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E6%93%8D%E4%BD%9C"><span class="toc-number">1.1.</span> <span class="toc-text">数据操作</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%B9%BF%E6%92%AD%E6%9C%BA%E5%88%B6"><span class="toc-number">1.1.1.</span> <span class="toc-text">广播机制</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%8A%82%E7%9C%81%E5%86%85%E5%AD%98"><span class="toc-number">1.1.2.</span> <span class="toc-text">节省内存</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%BD%AC%E6%8D%A2numpy%E5%AF%B9%E8%B1%A1"><span class="toc-number">1.1.3.</span> <span class="toc-text">转换numpy对象</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%BB%83%E4%B9%A0"><span class="toc-number">1.1.4.</span> <span class="toc-text">练习</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86"><span class="toc-number">1.2.</span> <span class="toc-text">数据预处理</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E9%9B%86%E8%AF%BB%E5%8F%96"><span class="toc-number">1.2.1.</span> <span class="toc-text">数据集读取</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%BC%BA%E5%A4%B1%E5%80%BC%E5%A4%84%E7%90%86"><span class="toc-number">1.2.2.</span> <span class="toc-text">缺失值处理</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%BB%83%E4%B9%A0-1"><span class="toc-number">1.2.3.</span> <span class="toc-text">练习</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0"><span class="toc-number">1.3.</span> <span class="toc-text">线性代数</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%9F%A9%E9%98%B5%E7%9A%84%E8%BD%AC%E7%BD%AE"><span class="toc-number">1.3.1.</span> <span class="toc-text">矩阵的转置</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%BC%A0%E9%87%8F%E5%9F%BA%E6%9C%AC%E7%AE%97%E6%B3%95"><span class="toc-number">1.3.2.</span> <span class="toc-text">张量基本算法</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%99%8D%E7%BB%B4"><span class="toc-number">1.3.3.</span> <span class="toc-text">降维</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%9D%9E%E9%99%8D%E7%BB%B4%E6%B1%82%E5%92%8C"><span class="toc-number">1.3.4.</span> <span class="toc-text">非降维求和</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%82%B9%E7%A7%AF"><span class="toc-number">1.3.5.</span> <span class="toc-text">点积</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%9F%A9%E9%98%B5%E4%B9%98%E6%B3%95"><span class="toc-number">1.3.6.</span> <span class="toc-text">矩阵乘法</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%8C%83%E6%95%B0"><span class="toc-number">1.3.7.</span> <span class="toc-text">范数</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%BB%83%E4%B9%A0-2"><span class="toc-number">1.3.8.</span> <span class="toc-text">练习</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%BE%AE%E7%A7%AF%E5%88%86"><span class="toc-number">1.4.</span> <span class="toc-text">微积分</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%BB%83%E4%B9%A0-3"><span class="toc-number">1.4.1.</span> <span class="toc-text">练习</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%87%AA%E5%8A%A8%E5%BE%AE%E5%88%86"><span class="toc-number">1.5.</span> <span class="toc-text">自动微分</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%9D%9E%E6%A0%87%E9%87%8F%E5%8F%98%E9%87%8F%E7%9A%84%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD"><span class="toc-number">1.5.1.</span> <span class="toc-text">非标量变量的反向传播</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%88%86%E7%A6%BB%E8%AE%A1%E7%AE%97"><span class="toc-number">1.5.2.</span> <span class="toc-text">分离计算</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%BB%83%E4%B9%A0-4"><span class="toc-number">1.5.3.</span> <span class="toc-text">练习</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%9F%A5%E9%98%85%E6%96%87%E6%A1%A3"><span class="toc-number">1.6.</span> <span class="toc-text">查阅文档</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%9F%A5%E6%89%BE%E6%A8%A1%E5%9D%97%E4%B8%AD%E7%9A%84%E6%89%80%E6%9C%89%E5%87%BD%E6%95%B0%E5%92%8C%E7%B1%BB"><span class="toc-number">1.6.1.</span> <span class="toc-text">查找模块中的所有函数和类</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%9F%A5%E6%89%BE%E7%89%B9%E5%AE%9A%E5%87%BD%E6%95%B0%E5%92%8C%E7%B1%BB%E7%9A%84%E7%94%A8%E6%B3%95"><span class="toc-number">1.6.2.</span> <span class="toc-text">查找特定函数和类的用法</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%BA%BF%E6%80%A7%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="toc-number">2.</span> <span class="toc-text">线性神经网络</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92"><span class="toc-number">2.1.</span> <span class="toc-text">线性回归</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B"><span class="toc-number">2.1.1.</span> <span class="toc-text">线性模型</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0"><span class="toc-number">2.1.2.</span> <span class="toc-text">损失函数</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BB%8E%E9%9B%B6%E5%AE%9E%E7%8E%B0%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92"><span class="toc-number">2.2.</span> <span class="toc-text">从零实现线性回归</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%BB%83%E4%B9%A0-5"><span class="toc-number">2.2.1.</span> <span class="toc-text">练习</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E7%9A%84%E7%AE%80%E6%B4%81%E5%AE%9E%E7%8E%B0"><span class="toc-number">2.3.</span> <span class="toc-text">线性回归的简洁实现</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%AE%9A%E4%B9%89%E6%A8%A1%E5%9E%8B"><span class="toc-number">2.3.1.</span> <span class="toc-text">定义模型</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%AE%9A%E4%B9%89%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E4%B8%8E%E4%BC%98%E5%8C%96%E5%99%A8"><span class="toc-number">2.3.2.</span> <span class="toc-text">定义损失函数与优化器</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%AE%AD%E7%BB%83"><span class="toc-number">2.3.3.</span> <span class="toc-text">训练</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%BB%83%E4%B9%A0-6"><span class="toc-number">2.3.4.</span> <span class="toc-text">练习</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%A4%9A%E5%B1%82%E6%84%9F%E7%9F%A5%E6%9C%BA"><span class="toc-number">3.</span> <span class="toc-text">多层感知机</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%A4%9A%E5%B1%82%E6%84%9F%E7%9F%A5%E6%9C%BA-1"><span class="toc-number">3.1.</span> <span class="toc-text">多层感知机</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0"><span class="toc-number">3.1.1.</span> <span class="toc-text">激活函数</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%BB%83%E4%B9%A0-7"><span class="toc-number">3.1.2.</span> <span class="toc-text">练习</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%A4%9A%E5%B1%82%E6%84%9F%E7%9F%A5%E6%9C%BA%E7%AE%80%E6%B4%81%E5%AE%9E%E7%8E%B0"><span class="toc-number">3.2.</span> <span class="toc-text">多层感知机简洁实现</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%BB%83%E4%B9%A0-8"><span class="toc-number">3.2.1.</span> <span class="toc-text">练习</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%A8%A1%E5%9E%8B%E9%80%89%E6%8B%A9%EF%BC%8C%E6%AC%A0%E6%8B%9F%E5%90%88%EF%BC%8C%E8%BF%87%E6%8B%9F%E5%90%88"><span class="toc-number">3.3.</span> <span class="toc-text">模型选择，欠拟合，过拟合</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%BB%83%E4%B9%A0-9"><span class="toc-number">3.3.1.</span> <span class="toc-text">练习</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%9A%82%E9%80%80%E6%B3%95%EF%BC%88Dropout%EF%BC%89"><span class="toc-number">3.4.</span> <span class="toc-text">暂退法（Dropout）</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BB%8E%E9%9B%B6%E5%BC%80%E5%A7%8B%E5%AE%9E%E7%8E%B0"><span class="toc-number">3.4.1.</span> <span class="toc-text">从零开始实现</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%AE%9A%E4%B9%89%E6%A8%A1%E5%9E%8B-1"><span class="toc-number">3.4.2.</span> <span class="toc-text">定义模型</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%AE%80%E4%BB%8B%E5%AE%9E%E7%8E%B0"><span class="toc-number">3.4.3.</span> <span class="toc-text">简介实现</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%BB%83%E4%B9%A0-10"><span class="toc-number">3.4.4.</span> <span class="toc-text">练习</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%95%B0%E5%80%BC%E7%A8%B3%E5%AE%9A%E6%80%A7%E4%B8%8E%E6%A8%A1%E5%9E%8B%E5%88%9D%E5%A7%8B%E5%8C%96"><span class="toc-number">3.5.</span> <span class="toc-text">数值稳定性与模型初始化</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%A2%AF%E5%BA%A6%E6%B6%88%E5%A4%B1%E5%92%8C%E6%A2%AF%E5%BA%A6%E7%88%86%E7%82%B8"><span class="toc-number">3.5.1.</span> <span class="toc-text">梯度消失和梯度爆炸</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%AE%A1%E7%AE%97"><span class="toc-number">4.</span> <span class="toc-text">深度学习计算</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%B1%82%E5%92%8C%E5%9D%97"><span class="toc-number">4.1.</span> <span class="toc-text">层和块</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%87%AA%E5%AE%9A%E4%B9%89%E5%9D%97"><span class="toc-number">4.1.1.</span> <span class="toc-text">自定义块</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%A1%BA%E5%BA%8F%E5%9D%97"><span class="toc-number">4.1.2.</span> <span class="toc-text">顺序块</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%BB%83%E4%B9%A0-11"><span class="toc-number">4.1.3.</span> <span class="toc-text">练习</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%8F%82%E6%95%B0%E7%AE%A1%E7%90%86"><span class="toc-number">4.2.</span> <span class="toc-text">参数管理</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8F%82%E6%95%B0%E8%AE%BF%E9%97%AE"><span class="toc-number">4.2.1.</span> <span class="toc-text">参数访问</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%B8%80%E6%AC%A1%E6%80%A7%E8%AE%BF%E9%97%AE%E6%89%80%E6%9C%89%E5%8F%82%E6%95%B0"><span class="toc-number">4.2.2.</span> <span class="toc-text">一次性访问所有参数</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8F%82%E6%95%B0%E7%BB%91%E5%AE%9A"><span class="toc-number">4.2.3.</span> <span class="toc-text">参数绑定</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%BB%83%E4%B9%A0-12"><span class="toc-number">4.2.4.</span> <span class="toc-text">练习</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%87%AA%E5%AE%9A%E4%B9%89%E5%B1%82"><span class="toc-number">4.3.</span> <span class="toc-text">自定义层</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%AE%BE%E8%AE%A1%E5%B8%A6%E5%8F%82%E6%95%B0%E7%9A%84%E5%B1%82"><span class="toc-number">4.3.1.</span> <span class="toc-text">设计带参数的层</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%BB%83%E4%B9%A0-13"><span class="toc-number">4.3.2.</span> <span class="toc-text">练习</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%AF%BB%E5%86%99%E6%96%87%E4%BB%B6"><span class="toc-number">4.4.</span> <span class="toc-text">读写文件</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%9F%BA%E6%9C%ACload%EF%BC%8Csave%E6%93%8D%E4%BD%9C"><span class="toc-number">4.4.1.</span> <span class="toc-text">基本load，save操作</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%8A%A0%E8%BD%BD%E4%B8%8E%E4%BF%9D%E5%AD%98"><span class="toc-number">4.4.2.</span> <span class="toc-text">模型的加载与保存</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%BB%83%E4%B9%A0-14"><span class="toc-number">4.4.3.</span> <span class="toc-text">练习</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#GPU"><span class="toc-number">4.5.</span> <span class="toc-text">GPU</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%BB%83%E4%B9%A0-15"><span class="toc-number">4.5.1.</span> <span class="toc-text">练习</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="toc-number">5.</span> <span class="toc-text">卷积神经网络</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%85%A8%E8%BF%9E%E6%8E%A5%E5%88%B0%E5%8D%B7%E7%A7%AF"><span class="toc-number">5.1.</span> <span class="toc-text">全连接到卷积</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%9B%BE%E5%83%8F%E5%8D%B7%E7%A7%AF"><span class="toc-number">5.2.</span> <span class="toc-text">图像卷积</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BA%92%E7%9B%B8%E5%85%B3%E8%BF%90%E7%AE%97"><span class="toc-number">5.2.1.</span> <span class="toc-text">互相关运算</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8D%B7%E7%A7%AF%E5%B1%82"><span class="toc-number">5.2.2.</span> <span class="toc-text">卷积层</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%84%9F%E5%8F%97%E9%87%8E"><span class="toc-number">5.2.3.</span> <span class="toc-text">感受野</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%BB%83%E4%B9%A0-16"><span class="toc-number">5.2.4.</span> <span class="toc-text">练习</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%A1%AB%E5%85%85%E4%B8%8E%E6%AD%A5%E5%B9%85"><span class="toc-number">5.3.</span> <span class="toc-text">填充与步幅</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%80%9A%E7%94%A8%E5%85%AC%E5%BC%8F"><span class="toc-number">5.3.1.</span> <span class="toc-text">通用公式</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%B1%95%E5%BC%80%E5%85%AC%E5%BC%8F"><span class="toc-number">5.3.2.</span> <span class="toc-text">展开公式</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%A4%9A%E8%BE%93%E5%85%A5%E5%A4%9A%E8%BE%93%E5%87%BA%E9%80%9A%E9%81%93"><span class="toc-number">5.4.</span> <span class="toc-text">多输入多输出通道</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%A4%9A%E8%BE%93%E5%85%A5%E9%80%9A%E9%81%93"><span class="toc-number">5.4.1.</span> <span class="toc-text">多输入通道</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%A4%9A%E8%BE%93%E5%87%BA%E9%80%9A%E9%81%93"><span class="toc-number">5.4.2.</span> <span class="toc-text">多输出通道</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-1%E5%8D%B7%E7%A7%AF"><span class="toc-number">5.4.3.</span> <span class="toc-text">1 * 1卷积</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%B1%87%E8%81%9A%E5%B1%82%EF%BC%88pooling%E6%B1%A0%E5%8C%96%E5%B1%82%EF%BC%89"><span class="toc-number">5.5.</span> <span class="toc-text">汇聚层（pooling池化层）</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%9C%80%E5%A4%A7%E6%B1%87%E8%81%9A%E5%B1%82%E5%92%8C%E5%B9%B3%E5%9D%87%E6%B1%87%E8%81%9A%E5%B1%82"><span class="toc-number">5.5.1.</span> <span class="toc-text">最大汇聚层和平均汇聚层</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%BB%83%E4%B9%A0-17"><span class="toc-number">5.5.2.</span> <span class="toc-text">练习</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88LeNet%EF%BC%89"><span class="toc-number">5.6.</span> <span class="toc-text">卷积神经网络（LeNet）</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%BB%83%E4%B9%A0-18"><span class="toc-number">5.6.1.</span> <span class="toc-text">练习</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%8E%B0%E4%BB%A3%E5%8D%B7%E7%A7%AF%E7%BD%91%E7%BB%9C"><span class="toc-number">6.</span> <span class="toc-text">现代卷积网络</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%B7%B1%E5%BA%A6%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="toc-number">6.1.</span> <span class="toc-text">深度卷积神经网络</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Alexnet"><span class="toc-number">6.1.1.</span> <span class="toc-text">Alexnet</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%BB%83%E4%B9%A0-19"><span class="toc-number">6.1.2.</span> <span class="toc-text">练习</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BD%BF%E7%94%A8%E5%9D%97%E7%9A%84%E7%BD%91%E7%BB%9C%EF%BC%88VGG%EF%BC%89"><span class="toc-number">6.2.</span> <span class="toc-text">使用块的网络（VGG）</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#VGG%E5%9D%97"><span class="toc-number">6.2.1.</span> <span class="toc-text">VGG块</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%BD%91%E7%BB%9C%E4%B8%AD%E7%9A%84%E7%BD%91%E7%BB%9C%EF%BC%88NiN%EF%BC%89"><span class="toc-number">6.3.</span> <span class="toc-text">网络中的网络（NiN）</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#NiN%E5%9D%97"><span class="toc-number">6.3.1.</span> <span class="toc-text">NiN块</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%BB%83%E4%B9%A0-20"><span class="toc-number">6.3.2.</span> <span class="toc-text">练习</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%90%AB%E5%B9%B6%E8%A1%8C%E8%BF%9E%E7%BB%93%E7%9A%84%E7%BD%91%E7%BB%9C%EF%BC%88googleNet%EF%BC%89"><span class="toc-number">6.4.</span> <span class="toc-text">含并行连结的网络（googleNet）</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#googlenet%E6%A8%A1%E5%9E%8B"><span class="toc-number">6.4.1.</span> <span class="toc-text">googlenet模型</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%BB%83%E4%B9%A0-21"><span class="toc-number">6.4.2.</span> <span class="toc-text">练习</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%89%B9%E9%87%8F%E8%A7%84%E8%8C%83%E5%8C%96"><span class="toc-number">6.5.</span> <span class="toc-text">批量规范化</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BB%8E%E9%9B%B6%E5%AE%9E%E7%8E%B0"><span class="toc-number">6.5.1.</span> <span class="toc-text">从零实现</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%AE%80%E6%98%8E%E5%AE%9E%E7%8E%B0"><span class="toc-number">6.5.2.</span> <span class="toc-text">简明实现</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BA%89%E8%AE%AE"><span class="toc-number">6.5.3.</span> <span class="toc-text">争议</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%B3%A8%EF%BC%9A"><span class="toc-number">6.5.4.</span> <span class="toc-text">注：</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%BB%83%E4%B9%A0-22"><span class="toc-number">6.5.5.</span> <span class="toc-text">练习</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%AE%8B%E5%B7%AE%E7%BD%91%E7%BB%9C"><span class="toc-number">6.6.</span> <span class="toc-text">残差网络</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%87%BD%E6%95%B0%E7%B1%BB"><span class="toc-number">6.6.1.</span> <span class="toc-text">函数类</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%AE%8B%E5%B7%AE%E5%9D%97"><span class="toc-number">6.6.2.</span> <span class="toc-text">残差块</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#resnet%E6%A8%A1%E5%9E%8B"><span class="toc-number">6.6.3.</span> <span class="toc-text">resnet模型</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%BB%83%E4%B9%A0-23"><span class="toc-number">6.6.4.</span> <span class="toc-text">练习</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%A8%A0%E5%AF%86%E8%BF%9E%E6%8E%A5%E7%BD%91%E7%BB%9C"><span class="toc-number">6.7.</span> <span class="toc-text">稠密连接网络</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BB%8Eresnet%E5%88%B0densenet"><span class="toc-number">6.7.1.</span> <span class="toc-text">从resnet到densenet</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%A8%A0%E5%AF%86%E5%9D%97%E4%BD%93"><span class="toc-number">6.7.2.</span> <span class="toc-text">稠密块体</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%BB%83%E4%B9%A0-24"><span class="toc-number">6.7.3.</span> <span class="toc-text">练习</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="toc-number">7.</span> <span class="toc-text">循环神经网络</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%BA%8F%E5%88%97%E6%A8%A1%E5%9E%8B"><span class="toc-number">7.1.</span> <span class="toc-text">序列模型</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E6%A8%A1%E5%9E%8B"><span class="toc-number">7.1.1.</span> <span class="toc-text">马尔可夫模型</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%BB%83%E4%B9%A0-25"><span class="toc-number">7.1.2.</span> <span class="toc-text">练习</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%96%87%E6%9C%AC%E9%A2%84%E5%A4%84%E7%90%86"><span class="toc-number">7.2.</span> <span class="toc-text">文本预处理</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%AD%A5%E9%AA%A4"><span class="toc-number">7.2.1.</span> <span class="toc-text">步骤</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E9%9B%86%E8%AF%BB%E5%8F%96%E4%B8%8E%E8%AF%8D%E5%85%83%E5%8C%96"><span class="toc-number">7.2.2.</span> <span class="toc-text">数据集读取与词元化</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%AF%8D%E8%A1%A8"><span class="toc-number">7.2.3.</span> <span class="toc-text">词表</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8A%9F%E8%83%BD%E6%95%B4%E5%90%88"><span class="toc-number">7.2.4.</span> <span class="toc-text">功能整合</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%BB%83%E4%B9%A0-26"><span class="toc-number">7.2.5.</span> <span class="toc-text">练习</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E5%92%8C%E6%95%B0%E6%8D%AE%E9%9B%86"><span class="toc-number">7.3.</span> <span class="toc-text">语言模型和数据集</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%BB%83%E4%B9%A0-27"><span class="toc-number">7.3.1.</span> <span class="toc-text">练习</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C-1"><span class="toc-number">7.4.</span> <span class="toc-text">循环神经网络</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%BB%83%E4%B9%A0-28"><span class="toc-number">7.4.1.</span> <span class="toc-text">练习</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E4%BB%8E%E9%9B%B6%E5%BC%80%E5%A7%8B%E5%AE%9E%E7%8E%B0"><span class="toc-number">7.5.</span> <span class="toc-text">循环神经网络的从零开始实现</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%8B%AC%E7%83%AD%E7%BC%96%E7%A0%81"><span class="toc-number">7.5.1.</span> <span class="toc-text">独热编码</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%88%9D%E5%A7%8B%E5%8C%96%E6%A8%A1%E5%9E%8B%E5%8F%82%E6%95%B0"><span class="toc-number">7.5.2.</span> <span class="toc-text">初始化模型参数</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B"><span class="toc-number">7.5.3.</span> <span class="toc-text">网络模型</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%AE%AD%E7%BB%83-1"><span class="toc-number">7.5.4.</span> <span class="toc-text">训练</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%AE%AD%E7%BB%83%E5%92%8C%E9%A2%84%E6%B5%8B"><span class="toc-number">7.5.5.</span> <span class="toc-text">训练和预测</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%BB%83%E4%B9%A0-29"><span class="toc-number">7.5.6.</span> <span class="toc-text">练习</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E7%AE%80%E4%BB%8B%E5%AE%9E%E7%8E%B0"><span class="toc-number">7.5.7.</span> <span class="toc-text">循环神经网络的简介实现</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%A8%A1%E5%9E%8B%E5%AE%9A%E4%B9%89"><span class="toc-number">7.5.8.</span> <span class="toc-text">模型定义</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%AE%9A%E4%B9%89%E4%B8%80%E4%B8%AARNN%E7%B1%BB"><span class="toc-number">7.5.9.</span> <span class="toc-text">定义一个RNN类</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%AE%AD%E7%BB%83%E4%B8%8E%E9%A2%84%E6%B5%8B"><span class="toc-number">7.5.10.</span> <span class="toc-text">训练与预测</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%BB%83%E4%B9%A0-30"><span class="toc-number">7.5.11.</span> <span class="toc-text">练习</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%8E%B0%E4%BB%A3%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="toc-number">8.</span> <span class="toc-text">现代循环神经网络</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%97%A8%E6%8E%A7%E5%BE%AA%E7%8E%AF%E5%8D%95%E5%85%83%EF%BC%88GRU%EF%BC%89"><span class="toc-number">8.1.</span> <span class="toc-text">门控循环单元（GRU）</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%BB%83%E4%B9%A0-31"><span class="toc-number">8.1.1.</span> <span class="toc-text">练习</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%95%BF%E7%9F%AD%E6%9C%9F%E8%AE%B0%E5%BF%86%E7%BD%91%E7%BB%9C%EF%BC%88LSTM%EF%BC%89"><span class="toc-number">8.2.</span> <span class="toc-text">长短期记忆网络（LSTM）</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%BB%83%E4%B9%A0-32"><span class="toc-number">8.2.1.</span> <span class="toc-text">练习</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%B7%B1%E5%BA%A6%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="toc-number">8.3.</span> <span class="toc-text">深度循环神经网络</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%8F%8C%E5%90%91%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="toc-number">8.4.</span> <span class="toc-text">双向循环神经网络</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%BB%83%E4%B9%A0-33"><span class="toc-number">8.4.1.</span> <span class="toc-text">练习</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%9C%BA%E5%99%A8%E7%BF%BB%E8%AF%91%E4%B8%8E%E6%95%B0%E6%8D%AE%E9%9B%86"><span class="toc-number">8.5.</span> <span class="toc-text">机器翻译与数据集</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%BB%83%E4%B9%A0-34"><span class="toc-number">8.5.1.</span> <span class="toc-text">练习</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%BC%96%E7%A0%81%E5%99%A8-%E8%A7%A3%E7%A0%81%E5%99%A8%E7%BB%93%E6%9E%84"><span class="toc-number">8.6.</span> <span class="toc-text">编码器-解码器结构</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%BC%96%E7%A0%81%E5%99%A8"><span class="toc-number">8.6.1.</span> <span class="toc-text">编码器</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%90%88%E5%B9%B6%E7%BC%96%E7%A0%81%E5%99%A8%E5%92%8C%E8%A7%A3%E7%A0%81%E5%99%A8"><span class="toc-number">8.6.2.</span> <span class="toc-text">合并编码器和解码器</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%BB%83%E4%B9%A0-35"><span class="toc-number">8.6.3.</span> <span class="toc-text">练习</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%BA%8F%E5%88%97%E5%88%B0%E5%BA%8F%E5%88%97%E5%AD%A6%E4%B9%A0%EF%BC%88seq2seq%EF%BC%89"><span class="toc-number">8.7.</span> <span class="toc-text">序列到序列学习（seq2seq）</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%BB%83%E4%B9%A0-36"><span class="toc-number">8.7.1.</span> <span class="toc-text">练习</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%9D%9F%E6%90%9C%E7%B4%A2"><span class="toc-number">8.8.</span> <span class="toc-text">束搜索</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%B4%AA%E5%BF%83%E6%90%9C%E7%B4%A2"><span class="toc-number">8.8.1.</span> <span class="toc-text">贪心搜索</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%A9%B7%E4%B8%BE%E6%90%9C%E7%B4%A2"><span class="toc-number">8.8.2.</span> <span class="toc-text">穷举搜索</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%9D%9F%E6%90%9C%E7%B4%A2-1"><span class="toc-number">8.8.3.</span> <span class="toc-text">束搜索</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%BB%83%E4%B9%A0-37"><span class="toc-number">8.8.4.</span> <span class="toc-text">练习</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6"><span class="toc-number">9.</span> <span class="toc-text">注意力机制</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%BB%83%E4%B9%A0-38"><span class="toc-number">9.0.1.</span> <span class="toc-text">练习</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%B1%87%E8%81%9A%EF%BC%9ANadaraya-Watson%E6%A0%B8%E5%9B%9E%E5%BD%92"><span class="toc-number">9.1.</span> <span class="toc-text">注意力汇聚：Nadaraya-Watson核回归</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%94%9F%E6%88%90%E6%95%B0%E6%8D%AE%E9%9B%86"><span class="toc-number">9.1.1.</span> <span class="toc-text">生成数据集</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%B9%B3%E5%9D%87%E6%B1%87%E8%81%9A"><span class="toc-number">9.1.2.</span> <span class="toc-text">平均汇聚</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%9D%9E%E5%8F%82%E6%95%B0%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%B1%87%E8%81%9A"><span class="toc-number">9.1.3.</span> <span class="toc-text">非参数注意力汇聚</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%B8%A6%E5%8F%82%E6%95%B0%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%B1%87%E8%81%9A"><span class="toc-number">9.1.4.</span> <span class="toc-text">带参数注意力汇聚</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%89%B9%E9%87%8F%E7%9F%A9%E9%98%B5%E4%B9%98%E6%B3%95"><span class="toc-number">9.1.5.</span> <span class="toc-text">批量矩阵乘法</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%BB%83%E4%B9%A0-39"><span class="toc-number">9.1.6.</span> <span class="toc-text">练习</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%B3%A8%E6%84%8F%E5%8A%9B%E8%AF%84%E5%88%86%E5%87%BD%E6%95%B0"><span class="toc-number">9.2.</span> <span class="toc-text">注意力评分函数</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%8E%A9%E8%94%BDsoftmax%E6%93%8D%E4%BD%9C"><span class="toc-number">9.2.1.</span> <span class="toc-text">掩蔽softmax操作</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8A%A0%E6%80%A7%E6%B3%A8%E6%84%8F%E5%8A%9B"><span class="toc-number">9.2.2.</span> <span class="toc-text">加性注意力</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%BB%83%E4%B9%A0-40"><span class="toc-number">9.2.3.</span> <span class="toc-text">练习</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Bahdanau-%E6%B3%A8%E6%84%8F%E5%8A%9B"><span class="toc-number">9.3.</span> <span class="toc-text">Bahdanau 注意力</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%A8%A1%E5%9E%8B"><span class="toc-number">9.3.1.</span> <span class="toc-text">模型</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%AE%9A%E4%B9%89%E6%B3%A8%E6%84%8F%E5%8A%9B%E8%A7%A3%E7%A0%81%E5%99%A8"><span class="toc-number">9.3.2.</span> <span class="toc-text">定义注意力解码器</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%A4%9A%E5%A4%B4%E6%B3%A8%E6%84%8F%E5%8A%9B"><span class="toc-number">9.4.</span> <span class="toc-text">多头注意力</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%AE%9E%E7%8E%B0"><span class="toc-number">9.4.1.</span> <span class="toc-text">实现</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%BB%83%E4%B9%A0-41"><span class="toc-number">9.4.2.</span> <span class="toc-text">练习</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%87%AA%E6%B3%A8%E6%84%8F%E5%8A%9B%E4%B8%8E%E4%BD%8D%E7%BD%AE%E7%BC%96%E7%A0%81"><span class="toc-number">9.5.</span> <span class="toc-text">自注意力与位置编码</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BD%8D%E7%BD%AE%E7%BC%96%E7%A0%81"><span class="toc-number">9.5.1.</span> <span class="toc-text">位置编码</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%BB%83%E4%B9%A0-42"><span class="toc-number">9.5.2.</span> <span class="toc-text">练习</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Transformer"><span class="toc-number">9.6.</span> <span class="toc-text">Transformer</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%9F%BA%E4%BA%8E%E4%BD%8D%E7%BD%AE%E7%9A%84%E5%89%8D%E9%A6%88%E7%BD%91%E7%BB%9C"><span class="toc-number">9.6.1.</span> <span class="toc-text">基于位置的前馈网络</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%AE%8B%E5%B7%AE%E8%BF%9E%E6%8E%A5%E5%92%8C%E5%B1%82%E8%A7%84%E8%8C%83%E5%8C%96"><span class="toc-number">9.6.2.</span> <span class="toc-text">残差连接和层规范化</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%BC%96%E7%A0%81%E5%99%A8-1"><span class="toc-number">9.6.3.</span> <span class="toc-text">编码器</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%A7%A3%E7%A0%81%E5%99%A8"><span class="toc-number">9.6.4.</span> <span class="toc-text">解码器</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%BB%83%E4%B9%A0-43"><span class="toc-number">9.6.5.</span> <span class="toc-text">练习</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95"><span class="toc-number">10.</span> <span class="toc-text">优化算法</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BC%98%E5%8C%96%E5%92%8C%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0"><span class="toc-number">10.1.</span> <span class="toc-text">优化和深度学习</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BC%98%E5%8C%96%E7%9A%84%E7%9B%AE%E6%A0%87"><span class="toc-number">10.1.1.</span> <span class="toc-text">优化的目标</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%AD%E7%9A%84%E4%BC%98%E5%8C%96%E6%8C%91%E6%88%98"><span class="toc-number">10.1.2.</span> <span class="toc-text">深度学习中的优化挑战</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%9E%8D%E7%82%B9"><span class="toc-number">10.1.3.</span> <span class="toc-text">鞍点</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%BB%83%E4%B9%A0-44"><span class="toc-number">10.1.4.</span> <span class="toc-text">练习</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%87%B8%E6%80%A7"><span class="toc-number">10.2.</span> <span class="toc-text">凸性</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%87%B8%E9%9B%86"><span class="toc-number">10.2.1.</span> <span class="toc-text">凸集</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%87%B8%E5%87%BD%E6%95%B0"><span class="toc-number">10.2.2.</span> <span class="toc-text">凸函数</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%A9%B9%E6%A3%AE%E4%B8%8D%E7%AD%89%E5%BC%8F"><span class="toc-number">10.2.3.</span> <span class="toc-text">詹森不等式</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%80%A7%E8%B4%A8"><span class="toc-number">10.2.4.</span> <span class="toc-text">性质</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%8B%89%E6%A0%BC%E6%9C%97%E6%97%A5%E5%87%BD%E6%95%B0"><span class="toc-number">10.2.5.</span> <span class="toc-text">拉格朗日函数</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%BB%83%E4%B9%A0-45"><span class="toc-number">10.2.6.</span> <span class="toc-text">练习</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D"><span class="toc-number">10.3.</span> <span class="toc-text">梯度下降</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%9A%8F%E6%9C%BA%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D"><span class="toc-number">10.4.</span> <span class="toc-text">随机梯度下降</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%B0%8F%E6%89%B9%E9%87%8F%E9%9A%8F%E6%9C%BA%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D"><span class="toc-number">10.5.</span> <span class="toc-text">小批量随机梯度下降</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%8A%A8%E9%87%8F%E6%B3%95"><span class="toc-number">10.6.</span> <span class="toc-text">动量法</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BB%8E%E9%9B%B6%E5%BC%80%E5%A7%8B%E5%AE%9E%E7%8E%B0-1"><span class="toc-number">10.6.1.</span> <span class="toc-text">从零开始实现</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%AE%80%E4%BB%8B%E5%AE%9E%E7%8E%B0-1"><span class="toc-number">10.6.2.</span> <span class="toc-text">简介实现</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%BB%83%E4%B9%A0-46"><span class="toc-number">10.6.3.</span> <span class="toc-text">练习</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#AdaGrad%E7%AE%97%E6%B3%95"><span class="toc-number">10.7.</span> <span class="toc-text">AdaGrad算法</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%AE%80%E4%BB%8B%E5%AE%9E%E7%8E%B0-2"><span class="toc-number">10.7.1.</span> <span class="toc-text">简介实现</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%BB%83%E4%B9%A0-47"><span class="toc-number">10.7.2.</span> <span class="toc-text">练习</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#RMSProp%E7%AE%97%E6%B3%95"><span class="toc-number">10.8.</span> <span class="toc-text">RMSProp算法</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%AE%80%E4%BB%8B%E5%AE%9E%E7%8E%B0-3"><span class="toc-number">10.8.1.</span> <span class="toc-text">简介实现</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Adadelta"><span class="toc-number">10.9.</span> <span class="toc-text">Adadelta</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%AE%80%E4%BB%8B%E5%AE%9E%E7%8E%B0-4"><span class="toc-number">10.9.1.</span> <span class="toc-text">简介实现</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%BB%83%E4%B9%A0-48"><span class="toc-number">10.9.2.</span> <span class="toc-text">练习</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Adam%E7%AE%97%E6%B3%95"><span class="toc-number">10.10.</span> <span class="toc-text">Adam算法</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%BB%83%E4%B9%A0-49"><span class="toc-number">10.10.1.</span> <span class="toc-text">练习</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%AD%A6%E4%B9%A0%E7%8E%87%E8%B0%83%E5%BA%A6%E5%99%A8"><span class="toc-number">10.11.</span> <span class="toc-text">学习率调度器</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%BB%83%E4%B9%A0-50"><span class="toc-number">10.11.1.</span> <span class="toc-text">练习</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E8%AE%A1%E7%AE%97%E6%80%A7%E8%83%BD"><span class="toc-number">11.</span> <span class="toc-text">计算性能</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%AC%A6%E5%8F%B7%E5%BC%8F%E7%BC%96%E7%A8%8B"><span class="toc-number">11.0.1.</span> <span class="toc-text">符号式编程</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%BC%82%E6%AD%A5%E8%AE%A1%E7%AE%97"><span class="toc-number">11.1.</span> <span class="toc-text">异步计算</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%87%AA%E5%8A%A8%E5%B9%B6%E8%A1%8C"><span class="toc-number">11.1.1.</span> <span class="toc-text">自动并行</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%9F%BA%E4%BA%8EGPU%E7%9A%84%E5%B9%B6%E8%A1%8C%E8%AE%A1%E7%AE%97"><span class="toc-number">11.1.2.</span> <span class="toc-text">基于GPU的并行计算</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%BB%83%E4%B9%A0-51"><span class="toc-number">11.1.3.</span> <span class="toc-text">练习</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%A4%9AGPU%E8%AE%AD%E7%BB%83"><span class="toc-number">11.2.</span> <span class="toc-text">多GPU训练</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%8F%82%E6%95%B0%E6%9C%8D%E5%8A%A1%E5%99%A8"><span class="toc-number">11.3.</span> <span class="toc-text">参数服务器</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%BB%83%E4%B9%A0-52"><span class="toc-number">11.3.1.</span> <span class="toc-text">练习</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89"><span class="toc-number">12.</span> <span class="toc-text">计算机视觉</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%9B%BE%E5%83%8F%E5%A2%9E%E5%B9%BF"><span class="toc-number">12.1.</span> <span class="toc-text">图像增广</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%B8%B8%E7%94%A8%E7%9A%84%E6%96%B9%E6%B3%95"><span class="toc-number">12.1.1.</span> <span class="toc-text">常用的方法</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BD%BF%E7%94%A8%E5%9B%BE%E5%83%8F%E5%A2%9E%E5%B9%BF%E8%BF%9B%E8%A1%8C%E8%AE%AD%E7%BB%83"><span class="toc-number">12.1.2.</span> <span class="toc-text">使用图像增广进行训练</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%BB%83%E4%B9%A0-53"><span class="toc-number">12.1.3.</span> <span class="toc-text">练习</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%BE%AE%E8%B0%83"><span class="toc-number">12.2.</span> <span class="toc-text">微调</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E5%92%8C%E8%BE%B9%E7%95%8C%E6%A1%86"><span class="toc-number">12.3.</span> <span class="toc-text">目标检测和边界框</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%BE%B9%E7%95%8C%E6%A1%86"><span class="toc-number">12.3.1.</span> <span class="toc-text">边界框</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%94%9A%E6%A1%86"><span class="toc-number">12.4.</span> <span class="toc-text">锚框</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%94%9F%E6%88%90%E5%A4%9A%E4%B8%AA%E9%94%9A%E6%A1%86"><span class="toc-number">12.4.1.</span> <span class="toc-text">生成多个锚框</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BA%A4%E5%B9%B6%E6%AF%94%EF%BC%88IoU%EF%BC%89"><span class="toc-number">12.4.2.</span> <span class="toc-text">交并比（IoU）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%BB%83%E4%B9%A0-54"><span class="toc-number">12.4.3.</span> <span class="toc-text">练习</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%A4%9A%E5%B0%BA%E5%BA%A6%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B"><span class="toc-number">12.5.</span> <span class="toc-text">多尺度目标检测</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%BB%83%E4%B9%A0-55"><span class="toc-number">12.5.1.</span> <span class="toc-text">练习</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E6%95%B0%E6%8D%AE%E9%9B%86"><span class="toc-number">12.6.</span> <span class="toc-text">目标检测数据集</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%8D%95%E5%8F%91%E5%A4%9A%E6%A1%86%E6%A3%80%E6%B5%8B"><span class="toc-number">12.7.</span> <span class="toc-text">单发多框检测</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%8C%BA%E5%9F%9F%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="toc-number">12.8.</span> <span class="toc-text">区域卷积神经网络</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%BB%83%E4%B9%A0-56"><span class="toc-number">12.8.1.</span> <span class="toc-text">练习</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%AF%AD%E4%B9%89%E5%88%86%E5%89%B2%E5%92%8C%E6%95%B0%E6%8D%AE%E9%9B%86"><span class="toc-number">12.9.</span> <span class="toc-text">语义分割和数据集</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E9%9B%86"><span class="toc-number">12.9.1.</span> <span class="toc-text">数据集</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%BB%83%E4%B9%A0-57"><span class="toc-number">12.9.2.</span> <span class="toc-text">练习</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%BD%AC%E7%BD%AE%E5%8D%B7%E7%A7%AF"><span class="toc-number">12.10.</span> <span class="toc-text">转置卷积</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%BB%83%E4%B9%A0-58"><span class="toc-number">12.10.1.</span> <span class="toc-text">练习</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%85%A8%E5%8D%B7%E7%A7%AF%E7%BD%91%E7%BB%9C"><span class="toc-number">12.11.</span> <span class="toc-text">全卷积网络</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%A8%A1%E5%9E%8B-1"><span class="toc-number">12.11.1.</span> <span class="toc-text">模型</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%BB%83%E4%B9%A0-59"><span class="toc-number">12.11.2.</span> <span class="toc-text">练习</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%A3%8E%E6%A0%BC%E8%BF%81%E7%A7%BB"><span class="toc-number">12.12.</span> <span class="toc-text">风格迁移</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%BB%83%E4%B9%A0-60"><span class="toc-number">12.12.1.</span> <span class="toc-text">练习</span></a></li></ol></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/posts/1c7f6819.html" title="李沐动手学深度学习部分笔记及练习">李沐动手学深度学习部分笔记及练习</a><time datetime="2024-04-10T06:54:20.000Z" title="发表于 2024-04-10 14:54:20">2024-04-10</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/posts/11203c58.html" title="sam入门"><img src="https://pic.imgdb.cn/item/659a2491871b83018a4100da.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="sam入门"/></a><div class="content"><a class="title" href="/posts/11203c58.html" title="sam入门">sam入门</a><time datetime="2024-01-07T03:38:31.000Z" title="发表于 2024-01-07 11:38:31">2024-01-07</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/posts/bb54d268.html" title="IIM论文笔记"><img src="https://pic.imgdb.cn/item/659a2493871b83018a41074c.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="IIM论文笔记"/></a><div class="content"><a class="title" href="/posts/bb54d268.html" title="IIM论文笔记">IIM论文笔记</a><time datetime="2023-10-20T10:59:45.000Z" title="发表于 2023-10-20 18:59:45">2023-10-20</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/posts/464412ff.html" title="Norm_Softmax_VIT笔记"><img src="https://i.imgtg.com/2023/06/03/Oq6ZiN.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Norm_Softmax_VIT笔记"/></a><div class="content"><a class="title" href="/posts/464412ff.html" title="Norm_Softmax_VIT笔记">Norm_Softmax_VIT笔记</a><time datetime="2023-06-03T11:23:49.000Z" title="发表于 2023-06-03 19:23:49">2023-06-03</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/posts/c1ef5b44.html" title="斯坦福 cs231n笔记"><img src="https://pic.imgdb.cn/item/659a24fe871b83018a4273a3.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="斯坦福 cs231n笔记"/></a><div class="content"><a class="title" href="/posts/c1ef5b44.html" title="斯坦福 cs231n笔记">斯坦福 cs231n笔记</a><time datetime="2023-04-25T00:42:02.000Z" title="发表于 2023-04-25 08:42:02">2023-04-25</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2024 By whisper</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.umd.min.js"></script><div class="js-pjax"></div><script data-pjax src="//npm.elemecdn.com/pace-js@1.2.4/pace.min.js"></script><div class="aplayer no-destroy" data-id="7696294679" data-server="tencent" data-type="playlist"   data-order="list" data-fixed="true" data-preload="auto" data-autoplay="false" data-mutex="true" ></div><script id="click-heart" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/click-heart.min.js" async="async" mobile="false"></script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.js"></script><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/metingjs/dist/Meting.min.js"></script><script src="https://cdn.jsdelivr.net/npm/pjax/pjax.min.js"></script><script>let pjaxSelectors = ["head > title","#config-diff","#body-wrap","#rightside-config-hide","#rightside-config-show",".js-pjax"]

var pjax = new Pjax({
  elements: 'a:not([target="_blank"])',
  selectors: pjaxSelectors,
  cacheBust: false,
  analytics: false,
  scrollRestoration: false
})

document.addEventListener('pjax:send', function () {

  // removeEventListener scroll 
  window.tocScrollFn && window.removeEventListener('scroll', window.tocScrollFn)
  window.scrollCollect && window.removeEventListener('scroll', scrollCollect)

  document.getElementById('rightside').style.cssText = "opacity: ''; transform: ''"
  
  if (window.aplayers) {
    for (let i = 0; i < window.aplayers.length; i++) {
      if (!window.aplayers[i].options.fixed) {
        window.aplayers[i].destroy()
      }
    }
  }

  typeof typed === 'object' && typed.destroy()

  //reset readmode
  const $bodyClassList = document.body.classList
  $bodyClassList.contains('read-mode') && $bodyClassList.remove('read-mode')

  typeof disqusjs === 'object' && disqusjs.destroy()
})

document.addEventListener('pjax:complete', function () {
  window.refreshFn()

  document.querySelectorAll('script[data-pjax]').forEach(item => {
    const newScript = document.createElement('script')
    const content = item.text || item.textContent || item.innerHTML || ""
    Array.from(item.attributes).forEach(attr => newScript.setAttribute(attr.name, attr.value))
    newScript.appendChild(document.createTextNode(content))
    item.parentNode.replaceChild(newScript, item)
  })

  GLOBAL_CONFIG.islazyload && window.lazyLoadInstance.update()

  typeof chatBtnFn === 'function' && chatBtnFn()
  typeof panguInit === 'function' && panguInit()

  // google analytics
  typeof gtag === 'function' && gtag('config', '', {'page_path': window.location.pathname});

  // baidu analytics
  typeof _hmt === 'object' && _hmt.push(['_trackPageview',window.location.pathname]);

  typeof loadMeting === 'function' && document.getElementsByClassName('aplayer').length && loadMeting()

  // prismjs
  typeof Prism === 'object' && Prism.highlightAll()
})

document.addEventListener('pjax:error', (e) => {
  if (e.request.status === 404) {
    pjax.loadUrl('/404.html')
  }
})</script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><div class="app-refresh" id="app-refresh" style="position: fixed;top: -2.2rem;left: 0;right: 0;z-index: 99999;padding: 0 1rem;font-size: 15px;height: 2.2rem;transition: all 0.3s ease;"><div class="app-refresh-wrap" style=" display: flex;color: #fff;height: 100%;align-items: center;justify-content: center;"><label>✨ 有新文章啦！ 👉</label><a href="javascript:void(0)" onclick="location.reload()"><span style="color: #fff;text-decoration: underline;cursor: pointer;">🍗点击食用🍔</span></a></div></div><script>if ('serviceWorker' in navigator) {
if (navigator.serviceWorker.controller) {
navigator.serviceWorker.addEventListener('controllerchange', function() {
showNotification()
})
}
window.addEventListener('load', function() {
navigator.serviceWorker.register('/sw.js')
})
}
function showNotification() {
if (GLOBAL_CONFIG.Snackbar) {
var snackbarBg =
document.documentElement.getAttribute('data-theme') === 'light' ?
GLOBAL_CONFIG.Snackbar.bgLight :
GLOBAL_CONFIG.Snackbar.bgDark
var snackbarPos = GLOBAL_CONFIG.Snackbar.position
Snackbar.show({
text: '✨ 有新文章啦！ 👉',
backgroundColor: snackbarBg,
duration: 500000,
pos: snackbarPos,
actionText: '🍗点击食用🍔',
actionTextColor: '#fff',
onActionClick: function(e) {
location.reload()
},
})
} else {
var showBg =
document.documentElement.getAttribute('data-theme') === 'light' ?
'#3b70fc' :
'#1f1f1f'
var cssText = `top: 0; background: ${showBg};`
document.getElementById('app-refresh').style.cssText = cssText
}
}</script></div><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  数据库加载中</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div><hr/><div id="local-search-results"></div></div></div><div id="search-mask"></div><script src="/js/search/local-search.js"></script></div><!-- hexo injector body_end start --><script data-pjax>
  function butterfly_clock_anzhiyu_injector_config(){
    var parent_div_git = document.getElementsByClassName('sticky_layout')[0];
    var item_html = '<div class="card-widget card-clock"><div class="card-glass"><div class="card-background"><div class="card-content"><div id="hexo_electric_clock"><img class="entered loading" id="card-clock-loading" src="https://cdn.cbd.int/hexo-butterfly-clock-anzhiyu/lib/loading.gif" style="height: 120px; width: 100%;" data-ll-status="loading"/></div></div></div></div></div>';
    console.log('已挂载butterfly_clock_anzhiyu')
    if(parent_div_git) {
      parent_div_git.insertAdjacentHTML("afterbegin",item_html)
    }
  }
  var elist = 'null'.split(',');
  var cpage = location.pathname;
  var epage = 'all';
  var qweather_key = '4f60bb7b1e494d6090b14273201dbe54';
  var gaud_map_key = 'ca9deefeece92d0ae985ba0e1a1bbc80';
  var baidu_ak_key = 'undefined';
  var flag = 0;
  var clock_rectangle = '112.982279,28.19409';
  var clock_default_rectangle_enable = 'false';

  for (var i=0;i<elist.length;i++){
    if (cpage.includes(elist[i])){
      flag++;
    }
  }

  if ((epage ==='all')&&(flag == 0)){
    butterfly_clock_anzhiyu_injector_config();
  }
  else if (epage === cpage){
    butterfly_clock_anzhiyu_injector_config();
  }
  </script><script src="https://widget.qweather.net/simple/static/js/he-simple-common.js?v=2.0"></script><script data-pjax src="https://cdn.cbd.int/hexo-butterfly-clock-anzhiyu/lib/clock.min.js"></script><div class="js-pjax"><script async="async">var arr = document.getElementsByClassName('recent-post-item');
for(var i = 0;i<arr.length;i++){
    arr[i].classList.add('wow');
    arr[i].classList.add('animate__zoomIn');
    arr[i].setAttribute('data-wow-duration', '1.5s');
    arr[i].setAttribute('data-wow-delay', '200ms');
    arr[i].setAttribute('data-wow-offset', '30');
    arr[i].setAttribute('data-wow-iteration', '1');
  }</script><script async="async">var arr = document.getElementsByClassName('card-widget');
for(var i = 0;i<arr.length;i++){
    arr[i].classList.add('wow');
    arr[i].classList.add('animate__zoomIn');
    arr[i].setAttribute('data-wow-duration', '');
    arr[i].setAttribute('data-wow-delay', '200ms');
    arr[i].setAttribute('data-wow-offset', '');
    arr[i].setAttribute('data-wow-iteration', '');
  }</script><script async="async">var arr = document.getElementsByClassName('flink-list-card');
for(var i = 0;i<arr.length;i++){
    arr[i].classList.add('wow');
    arr[i].classList.add('animate__flipInY');
    arr[i].setAttribute('data-wow-duration', '3s');
    arr[i].setAttribute('data-wow-delay', '');
    arr[i].setAttribute('data-wow-offset', '');
    arr[i].setAttribute('data-wow-iteration', '');
  }</script><script async="async">var arr = document.getElementsByClassName('flink-list-card');
for(var i = 0;i<arr.length;i++){
    arr[i].classList.add('wow');
    arr[i].classList.add('animate__animated');
    arr[i].setAttribute('data-wow-duration', '3s');
    arr[i].setAttribute('data-wow-delay', '');
    arr[i].setAttribute('data-wow-offset', '');
    arr[i].setAttribute('data-wow-iteration', '');
  }</script><script async="async">var arr = document.getElementsByClassName('article-sort-item');
for(var i = 0;i<arr.length;i++){
    arr[i].classList.add('wow');
    arr[i].classList.add('animate__slideInRight');
    arr[i].setAttribute('data-wow-duration', '1.5s');
    arr[i].setAttribute('data-wow-delay', '');
    arr[i].setAttribute('data-wow-offset', '');
    arr[i].setAttribute('data-wow-iteration', '');
  }</script><script async="async">var arr = document.getElementsByClassName('site-card');
for(var i = 0;i<arr.length;i++){
    arr[i].classList.add('wow');
    arr[i].classList.add('animate__flipInY');
    arr[i].setAttribute('data-wow-duration', '3s');
    arr[i].setAttribute('data-wow-delay', '');
    arr[i].setAttribute('data-wow-offset', '');
    arr[i].setAttribute('data-wow-iteration', '');
  }</script><script async="async">var arr = document.getElementsByClassName('site-card');
for(var i = 0;i<arr.length;i++){
    arr[i].classList.add('wow');
    arr[i].classList.add('animate__animated');
    arr[i].setAttribute('data-wow-duration', '3s');
    arr[i].setAttribute('data-wow-delay', '');
    arr[i].setAttribute('data-wow-offset', '');
    arr[i].setAttribute('data-wow-iteration', '');
  }</script></div><script defer src="https://cdn.cbd.int/hexo-butterfly-wowjs/lib/wow.min.js"></script><script defer src="https://cdn.cbd.int/hexo-butterfly-wowjs/lib/wow_init.js"></script><script async src="/sourse/js/ali_font.js"></script><!-- hexo injector body_end end --><script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"tagMode":false,"debug":false,"model":{"jsonPath":"/live2dw/assets/koharu.model.json"},"display":{"position":"left","width":100,"height":200},"mobile":{"show":false},"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/"});</script></body></html>