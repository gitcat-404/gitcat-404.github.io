<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>Norm_Softmax_VIT笔记</title>
      <link href="/posts/464412ff.html"/>
      <url>/posts/464412ff.html</url>
      
        <content type="html"><![CDATA[<h2 id="一些术语"><a href="#一些术语" class="headerlink" title="一些术语"></a>一些术语</h2><h3 id="batch：批次，一批处理，"><a href="#batch：批次，一批处理，" class="headerlink" title="batch：批次，一批处理，"></a>batch：批次，一批处理，</h3><h3 id="batch-size：表示每个batch有多少样本"><a href="#batch-size：表示每个batch有多少样本" class="headerlink" title="batch_size：表示每个batch有多少样本"></a>batch_size：表示每个batch有多少样本</h3><h3 id="LR（learning-rate-：学习率"><a href="#LR（learning-rate-：学习率" class="headerlink" title="LR（learning rate)：学习率"></a>LR（learning rate)：学习率</h3><h3 id="patch：补丁"><a href="#patch：补丁" class="headerlink" title="patch：补丁"></a>patch：补丁</h3><h3 id="epoch：周期，阶段"><a href="#epoch：周期，阶段" class="headerlink" title="epoch：周期，阶段"></a>epoch：周期，阶段</h3><h3 id="criterion：评判准则（一般用于命名损失函数）"><a href="#criterion：评判准则（一般用于命名损失函数）" class="headerlink" title="criterion：评判准则（一般用于命名损失函数）"></a>criterion：评判准则（一般用于命名损失函数）</h3><h3 id="optimizer：优化器"><a href="#optimizer：优化器" class="headerlink" title="optimizer：优化器"></a>optimizer：优化器</h3><h3 id="BP：反向传播算法（Back-Propagation），"><a href="#BP：反向传播算法（Back-Propagation），" class="headerlink" title="BP：反向传播算法（Back Propagation），"></a>BP：反向传播算法（Back Propagation），</h3><p>通过计算误差的反向传播来更新网络的权重和偏置</p><h3 id="Embedding："><a href="#Embedding：" class="headerlink" title="Embedding："></a>Embedding：</h3><p>是指将高维的离散型数据（如词汇、用户ID等）转换为低维的连续型向量的过程，也可以指转换后的向量</p><h3 id="感受野（Receptive-Field-："><a href="#感受野（Receptive-Field-：" class="headerlink" title="感受野（Receptive Field)："></a>感受野（Receptive Field)：</h3><p>指在神经网络中，输出特征映射上的一个像素点对应在输入图像中的区域。感受野的大小取决于网络的架构和层数，它可以用来衡量网络对输入信息的感知范围和理解能力。</p><p>具体来说，感受野的大小在卷积神经网络(Convolutional Neural Network, CNN)中是递增的，随着网络层数的增加，感受野的大小也随之增加。在前面的卷积层中，每个像素点的感受野通常只是输入图像的一个小区域，但是在后面的卷积层中，每个像素点的感受野可以覆盖整个输入图像。这样，网络就可以学习到更全局的特征和上下文信息，以更好地理解输入图像并提高分类或检测的准确性。</p><p>感受野的大小与卷积核的大小、步幅和填充有关。通常，较大的卷积核、较小的步幅和合适的填充可以增加感受野的大小。此外，池化层(Pooling Layer)也可以增加感受野大小，它通过池化操作对输入特征映射进行缩小，从而扩大感受野范围。</p><p>在神经网络设计中，理解和控制感受野大小可以帮助我们更好地设计网络结构和优化算法，从而提高网络的性能和稳定性。</p><h3 id="MLP（Multilayer-Perceptron，多层感知器）"><a href="#MLP（Multilayer-Perceptron，多层感知器）" class="headerlink" title="MLP（Multilayer Perceptron，多层感知器）"></a>MLP（Multilayer Perceptron，多层感知器）</h3><p>是一种基于前馈神经网络（Feedforward Neural Network）的模型，通常用于处理分类和回归问题。它由多个节点（神经元）组成，每个节点与前一层的所有节点相连。每个神经元的输出值是由前一层的神经元输出值通过权重参数的线性组合和激活函数的非线性变换计算得到的。MLP通常包含一个或多个隐藏层，以及一个输出层。也成为全连接神经网络（Fully Connected Neural Network）</p><h2 id="BN-Details"><a href="#BN-Details" class="headerlink" title="BN Details"></a>BN Details</h2><ol><li>Batch Normalization（BN）（批量归一化）是一种常用的神经网络正则化技术，旨在解决深度神经网络训练过程中的梯度消失和梯度爆炸问题，并改善网络性能和收敛速度。</li><li>为什么？<br> 每一层参数更新，会导致输入分布差距</li><li>是什么？<br>小批量数据计算均值方差，处理归一</li><li>怎么做？<br>B={x1,x2,…xn}<br>计算出该小批量数据的均值与方差，归一化<script type="math/tex; mode=display">\widehat{x}_{i}=\frac{x_{i}-\mu _{B}}{\sqrt{\sigma _{B}^{2}+\varepsilon}}</script><script type="math/tex; mode=display">y_{i}=\gamma \widehat{x}_{i}+\beta</script>乘系数加偏置是为了输入不是集中在-1，1，在通过激活函数后非线性.<h4 id="优缺点："><a href="#优缺点：" class="headerlink" title="优缺点："></a>优缺点：</h4></li><li><p>BN的基本思想是将神经网络中每个隐藏层的输入标准化，以使其均值为0，方差为1。具体来说，BN在每个训练批次中计算出批次输入的均值和方差，然后将其应用于批次中的每个样本。标准化后的样本通过调整缩放因子和偏移因子来恢复其原始输入特征空间，这些因子通过学习得到。这样，即使网络的输入和参数分布发生变化，BN也可以保持网络的稳定性。</p></li><li><p>BN的主要优点是可以提高网络的收敛速度和泛化能力，并具有一定的正则化作用，可以减少过拟合的风险。此外，BN可以减少对学习率和其他超参数的依赖，提高网络的可训练性和可调节性。</p></li><li><p>BN的一些缺点包括增加了网络的计算复杂性和内存使用，特别是在大型网络和GPU计算上。此外，BN对小批量数据的表现可能很差，因为它需要计算均值和方差的无偏估计，必须依赖于有效的批量大小。</p></li><li><p>总之，BN是一种强大的神经网络正则化技术，可以改善网络性能和训练过程的稳定性，特别是在深度神经网络和大型数据集上。</p><h2 id="Layer-Norm"><a href="#Layer-Norm" class="headerlink" title="Layer Norm"></a>Layer Norm</h2><p>Layer Norm（层标准化）是深度学习中一种常用的归一化方法，用于减小模型中许多常见问题，如梯度消失（vanishing gradients）和梯度爆炸（exploding gradients）等。它是对Batch Norm的一种改进，解决了Batch Norm难以应用于序列模型和小批量数据的问题。</p></li></ol><p>Layer Norm对于每个样本的每个特征分别进行归一化，使得每个样本在每个特征上都具有相似的分布，从而提高了模型的泛化性能。在Layer Norm中，对于每个样本的第i个特征，其标准化后的值为：</p><script type="math/tex; mode=display">\widehat{x}_{norm}=\frac{x_{i}-\mu _{B}}{\sqrt{\sigma _{B}^{2}+\varepsilon}}</script><p>其中，均值和方差是在特征维度上计算的，epsilon是一个很小的常量，用于防止除以0</p><script type="math/tex; mode=display">LayerNorm(x) = \gamma * x_{norm} + \beta</script><h3 id="区别"><a href="#区别" class="headerlink" title="区别"></a>区别</h3><ul><li>Layer Norm与Batch Norm的区别在于，Layer Norm是对每一层的输出进行标准化处理，而Batch Norm是对每一个batch进行标准化处理。<h2 id="Softmax-Details"><a href="#Softmax-Details" class="headerlink" title="Softmax Details"></a>Softmax Details</h2>softmax是一种用于多分类问题的激活函数，主要用于将一个向量转换为概率分布。在深度学习中，softmax通常用于输出层，将神经元的输出转换为类别概率。softmax函数的实现原理如下：<ol><li>计算每个神经元的指数值：对于输入向量x中的每个元素xi，计算<script type="math/tex">exp(xi)</script></li><li>计算指数值的和：将每个指数值相加，得到:<script type="math/tex">sum = Σ(exp(xi))</script></li><li>计算每个神经元的概率值：对于每个xi，计算:<script type="math/tex">pi = exp(xi) / sum</script></li><li>输出概率分布：将所有pi组成的向量作为输出，即<script type="math/tex">softmax(x) = [p1, p2, ..., pn]</script><br>其中，n是神经元的数量。softmax函数的输出可以被视为一个概率分布，其中每个元素pi表示输入向量属于第i个类别的概率。<h2 id="Transformer-VIT"><a href="#Transformer-VIT" class="headerlink" title="Transformer/VIT"></a>Transformer/VIT</h2>VIT（Vision Transformer）是用于图像分类的基于Transformer的模型。其步骤如下：</li></ol></li></ul><p><img src="https://pic4.zhimg.com/v2-5afd38bd10b279f3a572b13cda399233_r.jpg" alt="1132"></p><ol><li><p>图像的处理：将输入图像分割成多个小图像（patch）。</p></li><li><p>patch的线性变换：对于每个patch，使用一个线性变换将其转换为一个向量(patch_embesdding)</p></li><li><p>位置编码：为每个patch添加位置编码，以表示它们在原始图像中的位置(posttion_embedding)（与patch_embedding的形状完全一样）</p></li><li><p>输入嵌入：将所有patch的向量和位置编码拼接(相加)在一起，形成一个输入嵌入（input embedding）矩阵。</p></li><li><p>Layer Norm</p></li><li><p>Transformer Encoder：对输入嵌入进行多层Transformer编码器的处理，以便进行全局的特征提取。这里使用Multi-Head Attention方法，具体步骤如下：</p><ul><li><p>特征线性变换：每个输入序列复制3份（Q，K，V），经过线性变换，即将输入序列转换为多个查询（query）、键（key）和值（value）向量。</p></li><li><p>计算注意力分数：对于每个查询向量，计算它与所有键向量的内积，得到注意力分数的一组向量。<script type="math/tex">Output_{p1}=Attention(p1,p2)*p2+Attention(p1,p3)*p3+...+Attention(p1,p64)*p64</script></p></li><li><p>归一化：对注意力分数进行softmax归一化，得到所有键的权重分布。</p></li><li><p>加权求值：将所有值向量按照权重分布加权求和，得到该查询向量的注意力表示:引入wq，wk，wv权重，得到新的Q，K，V</p></li><li><p>多头注意力（Multi-Head）：将上述步骤在多个子空间中分别进行，得到不同子空间的注意力表示。即：wq，wk，wv权重矩阵有H组，计算注意力分数的操作可以进行H次。</p></li><li><p>输出合并：将各子空间的注意力（H次Multi-Head运算的结果）表示拼接成一个向量。使用softmax进行处理。</p></li><li><p>线性变换：将合并后的向量再进行一次线性变换，得到最终输出。</p></li></ul></li><li><p>Norm</p></li><li><p>（全局池化）：对于输出矩阵中的所有特征向量，进行全局池化（如平均池化或最大池化）得到一个全局特征向量。</p></li><li><p>全连接层：将全局特征向量输入到一个全连接层，进行分类预测。为了实现每个像素之间的交互。<br>附：师兄笔记<br><img src="https://i.imgtg.com/2023/06/03/Oqj90I.jpg" alt=""><br><img src="https://i.imgtg.com/2023/06/03/Oqj2MD.jpg" alt=""></p></li></ol><p><link rel="stylesheet" href="https://cdn1.tianli0.top/gh/zhheo/Post-Abstract-AI@0.7/tianli_gpt.css"></p><script>let tianliGPT_postSelector = '#post #article-container';let tianliGPT_key = 'd90e7ff2968fd4a313cd';</script><script src="https://cdn1.tianli0.top/gh/zhheo/Post-Abstract-AI@0.7/tianli_gpt.js"></script>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>斯坦福 cs231n笔记</title>
      <link href="/posts/c1ef5b44.html"/>
      <url>/posts/c1ef5b44.html</url>
      
        <content type="html"><![CDATA[<p><img src="/images/2.jpg" alt=""></p><h1 id="图像分类"><a href="#图像分类" class="headerlink" title="图像分类"></a>图像分类</h1><h3 id="目标"><a href="#目标" class="headerlink" title="目标"></a>目标</h3><p>所谓图像分类问题，就是已有固定的分类标签集合，然后对于输入的图像，从分类标签集合中找出一个分类标签，最后把分类标签分配给该输入图像。虽然看起来挺简单的，但这可是计算机视觉领域的核心问题之一，并且有着各种各样的实际应用。计算机视觉领域中很多看似不同的问题（比如物体检测和分割），都可以被归结为图像分类问题。</p><h3 id="困难和挑战"><a href="#困难和挑战" class="headerlink" title="困难和挑战"></a>困难和挑战</h3><p>对于人来说，识别出一个像“猫”一样视觉概念是简单至极的，然而从计算机视觉算法的角度来看就值得深思了。以下为计算机视觉算法在图像识别方面遇到的一些困难，图像是以3维数组来表示的，数组中的元素是亮度值。</p><ul><li><strong>视角变化（Viewpoint variation</strong>）**：同一个物体，摄像机可以从多个角度来展现。</li><li><strong>大小变化（Scale variation</strong>）**：物体可视的大小通常是会变化的（不仅是在图片中，在真实世界中大小也是变化的）。</li><li><strong>形变（Deformation</strong>）：很多东西的形状并非一成不变，会有很大变化。</li><li><strong>遮挡（Occlusion</strong>）：目标物体可能被挡住。有时候只有物体的一小部分（可以小到几个像素）是可见的。</li><li><strong>光照条件（Illumination conditions</strong>）：在像素层面上，光照的影响非常大。</li><li><strong>背景干扰（Background clutter</strong>）：物体可能混入背景之中，使之难以被辨认。</li><li><strong>类内差异（Intra-class variation</strong>）：一类物体的个体之间的外形差异很大，比如椅子。这一类物体有许多不同的对象，每个都有自己的外形。</li></ul><p>面对以上所有变化及其组合，好的图像分类模型能够在维持分类结论稳定的同时，保持对类间差异足够敏感。</p><h3 id="最近邻分类器Nearest-Neighbour"><a href="#最近邻分类器Nearest-Neighbour" class="headerlink" title="最近邻分类器Nearest Neighbour"></a>最近邻分类器Nearest Neighbour</h3><p>那么具体如何比较两张图片呢？在本例中，就是比较32（长）x32（宽）x3（RGB）的像素块。最简单的方法就是逐个像素比较，最后将差异值全部加起来。换句话说，就是将两张图片先转化为两个向量<code>I1</code>和<code>I2</code>，然后计算他们的<strong>L1距离：</strong></p><script type="math/tex; mode=display">d_{1}(I_{1},I_{2})=\sum_{p}^{}|I_{1}^{p}-I_{2}^{p}|</script><p>以图片中的一个颜色通道为例来进行说明。两张图片使用L1距离来进行比较。逐个像素求差值，然后将所有差值加起来得到一个数值。如果两张图片一模一样，那么L1距离为0，但是如果两张图片很是不同，那L1值将会非常大。<br>代码操作：首先，将CIFAR-10的数据加载到内存中，并分成4个数组：训练数据和标签，测试数据和标签。在下面的代码中，<strong>Xtr</strong>（大小是50000x32x32x3）存有训练集中所有的图像，<strong>Ytr</strong>是对应的长度为50000的1维数组，存有图像对应的分类标签（从0到9）：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Xtr, Ytr, Xte, Yte = load_CIFAR10(<span class="string">&#x27;data/cifar10/&#x27;</span>) <span class="comment"># a magic function we provide</span></span><br><span class="line"><span class="comment"># flatten out all images to be one-dimensional</span></span><br><span class="line">Xtr_rows = Xtr.reshape(Xtr.shape[<span class="number">0</span>], <span class="number">32</span> * <span class="number">32</span> * <span class="number">3</span>) <span class="comment"># Xtr_rows becomes 50000 x 3072</span></span><br><span class="line">Xte_rows = Xte.reshape(Xte.shape[<span class="number">0</span>], <span class="number">32</span> * <span class="number">32</span> * <span class="number">3</span>) <span class="comment"># Xte_rows becomes 10000 x 3072</span></span><br></pre></td></tr></table></figure><br>训练一个分类器<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">nn = NearestNeighbor() <span class="comment"># create a Nearest Neighbor classifier class</span></span><br><span class="line">nn.train(Xtr_rows, Ytr) <span class="comment"># train the classifier on the training images and labels</span></span><br><span class="line">Yte_predict = nn.predict(Xte_rows) <span class="comment"># predict labels on the test images</span></span><br><span class="line"><span class="comment"># and now print the classification accuracy, which is the average number</span></span><br><span class="line"><span class="comment"># of examples that are correctly predicted (i.e. label matches)</span></span><br><span class="line"><span class="built_in">print</span> <span class="string">&#x27;accuracy: %f&#x27;</span> % ( np.mean(Yte_predict == Yte) )</span><br></pre></td></tr></table></figure><br>作为评价标准，我们常常使用<strong>准确率</strong>，它描述了我们预测正确的得分。请注意以后我们实现的所有分类器都需要有这个API：train(X, y)函数。该函数使用训练集的数据和标签来进行训练。从其内部来看，类应该实现一些关于标签和标签如何被预测的模型。这里还有个predict(X)函数，它的作用是预测输入的新数据的分类标签。现在还没介绍分类器的实现，下面就是使用L1距离的Nearest Neighbor分类器的实现套路：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">NearestNeighbor</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line">  <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">    <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">def</span> <span class="title function_">train</span>(<span class="params">self, X, y</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot; X is N x D where each row is an example. Y is 1-dimension of size N &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># the nearest neighbor classifier simply remembers all the training data</span></span><br><span class="line">    self.Xtr = X</span><br><span class="line">    self.ytr = y</span><br><span class="line"></span><br><span class="line">  <span class="keyword">def</span> <span class="title function_">predict</span>(<span class="params">self, X</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot; X is N x D where each row is an example we wish to predict label for &quot;&quot;&quot;</span></span><br><span class="line">    num_test = X.shape[<span class="number">0</span>]</span><br><span class="line">    <span class="comment"># lets make sure that the output type matches the input type</span></span><br><span class="line">    Ypred = np.zeros(num_test, dtype = self.ytr.dtype)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># loop over all test rows</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> xrange(num_test):</span><br><span class="line">      <span class="comment"># find the nearest training image to the i&#x27;th test image</span></span><br><span class="line">      <span class="comment"># using the L1 distance (sum of absolute value differences)</span></span><br><span class="line">      distances = np.<span class="built_in">sum</span>(np.<span class="built_in">abs</span>(self.Xtr - X[i,:]), axis = <span class="number">1</span>)</span><br><span class="line">      min_index = np.argmin(distances) <span class="comment"># get the index with smallest distance</span></span><br><span class="line">      Ypred[i] = self.ytr[min_index] <span class="comment"># predict the label of the nearest example</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> Ypred</span><br></pre></td></tr></table></figure><br>另一种计算向量间距离的方法，L2距离</p><script type="math/tex; mode=display">d_{1}(I_{1},I_{2})=\sqrt{\sum (I_{1}^{p}-I_{2}^{p})^{2}}</script><p>修改距离计算方法<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">distances = np.sqrt(np.<span class="built_in">sum</span>(np.square(self.Xtr - X[i,:]), axis = <span class="number">1</span>))</span><br></pre></td></tr></table></figure></p><h3 id="K-Nearest-Neighbour分类器"><a href="#K-Nearest-Neighbour分类器" class="headerlink" title="K-Nearest Neighbour分类器"></a>K-Nearest Neighbour分类器</h3><p>最近邻分类器只找最相近的那1个图片的标签，而k近邻是找最相似的k个图片的标签，然后让他们针对测试图片进行投票，最后把票数最高的标签作为对测试图片的预测。所以当k=1的时候，k-Nearest Neighbor分类器就是Nearest Neighbor分类器。从直观感受上就可以看到，更高的k值可以让分类的效果更平滑，使得分类器对于异常值更有抵抗力。<br>k值选取?(超参数)<br>验证集调优方法：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># assume we have Xtr_rows, Ytr, Xte_rows, Yte as before</span></span><br><span class="line"><span class="comment"># recall Xtr_rows is 50,000 x 3072 matrix</span></span><br><span class="line">Xval_rows = Xtr_rows[:<span class="number">1000</span>, :] <span class="comment"># take first 1000 for validation</span></span><br><span class="line">Yval = Ytr[:<span class="number">1000</span>]</span><br><span class="line">Xtr_rows = Xtr_rows[<span class="number">1000</span>:, :] <span class="comment"># keep last 49,000 for train</span></span><br><span class="line">Ytr = Ytr[<span class="number">1000</span>:]</span><br><span class="line"></span><br><span class="line"><span class="comment"># find hyperparameters that work best on the validation set</span></span><br><span class="line">validation_accuracies = []</span><br><span class="line"><span class="keyword">for</span> k <span class="keyword">in</span> [<span class="number">1</span>, <span class="number">3</span>, <span class="number">5</span>, <span class="number">10</span>, <span class="number">20</span>, <span class="number">50</span>, <span class="number">100</span>]:</span><br><span class="line"></span><br><span class="line">  <span class="comment"># use a particular value of k and evaluation on validation data</span></span><br><span class="line">  nn = NearestNeighbor()</span><br><span class="line">  nn.train(Xtr_rows, Ytr)</span><br><span class="line">  <span class="comment"># here we assume a modified NearestNeighbor class that can take a k as input</span></span><br><span class="line">  Yval_predict = nn.predict(Xval_rows, k = k)</span><br><span class="line">  acc = np.mean(Yval_predict == Yval)</span><br><span class="line">  <span class="built_in">print</span> <span class="string">&#x27;accuracy: %f&#x27;</span> % (acc,)</span><br><span class="line"></span><br><span class="line">  <span class="comment"># keep track of what works on the validation set</span></span><br><span class="line">  validation_accuracies.append((k, acc))</span><br></pre></td></tr></table></figure><br>程序结束后，可以作图分析出哪个k值表现最好，然后用这个k值来跑真正的测试集，并作出对算法的评价。（把训练集分成训练集和验证集。使用验证集来对所有超参数调优。最后只在测试集上跑一次并报告结果）<br><strong>交叉验证</strong>：有时候，训练集数量较小（因此验证集的数量更小），会使用一种被称为<strong>交叉验证</strong>的方法，这种方法更加复杂些。还是用刚才的例子，如果是交叉验证集，我们就不是取1000个图像，而是将训练集平均分成5份，其中4份用来训练，1份用来验证。然后我们循环着取其中4份来训练，其中1份来验证，最后取所有5次验证结果的平均值作为算法验证结果。</p><h1 id="线性分类"><a href="#线性分类" class="headerlink" title="线性分类"></a>线性分类</h1><h1 id="神经网络与反向传播"><a href="#神经网络与反向传播" class="headerlink" title="神经网络与反向传播"></a>神经网络与反向传播</h1><h1 id="卷积神经网络"><a href="#卷积神经网络" class="headerlink" title="卷积神经网络"></a>卷积神经网络</h1><h1 id="循环神经网络"><a href="#循环神经网络" class="headerlink" title="循环神经网络"></a>循环神经网络</h1>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>线性代数知识点（遗忘则补充）</title>
      <link href="/posts/376cf89b.html"/>
      <url>/posts/376cf89b.html</url>
      
        <content type="html"><![CDATA[<h2 id="第1章-行列式"><a href="#第1章-行列式" class="headerlink" title="第1章 行列式"></a>第1章 行列式</h2><h3 id="1-1-全排列和对换"><a href="#1-1-全排列和对换" class="headerlink" title="1.1 全排列和对换"></a>1.1 全排列和对换</h3><h3 id="1-2-n阶行列式"><a href="#1-2-n阶行列式" class="headerlink" title="1.2 n阶行列式"></a>1.2 n阶行列式</h3><h3 id="1-3-行列式的性质"><a href="#1-3-行列式的性质" class="headerlink" title="1.3 行列式的性质"></a>1.3 行列式的性质</h3><h3 id="1-4-行列式按行（列）展开"><a href="#1-4-行列式按行（列）展开" class="headerlink" title="1.4 行列式按行（列）展开"></a>1.4 行列式按行（列）展开</h3><h2 id="第2章-矩阵及其运算"><a href="#第2章-矩阵及其运算" class="headerlink" title="第2章 矩阵及其运算"></a>第2章 矩阵及其运算</h2><h3 id="2-1-线性方程组和矩阵"><a href="#2-1-线性方程组和矩阵" class="headerlink" title="2.1 线性方程组和矩阵"></a>2.1 线性方程组和矩阵</h3><h3 id="2-2-矩阵的运算"><a href="#2-2-矩阵的运算" class="headerlink" title="2.2 矩阵的运算"></a>2.2 矩阵的运算</h3><h3 id="2-3-逆矩阵"><a href="#2-3-逆矩阵" class="headerlink" title="2.3 逆矩阵"></a>2.3 逆矩阵</h3><h3 id="2-4-Cramer法则"><a href="#2-4-Cramer法则" class="headerlink" title="2.4 Cramer法则"></a>2.4 Cramer法则</h3><h2 id="第3章-矩阵的初等变换与线性方程组"><a href="#第3章-矩阵的初等变换与线性方程组" class="headerlink" title="第3章 矩阵的初等变换与线性方程组"></a>第3章 矩阵的初等变换与线性方程组</h2><h3 id="3-1-矩阵的初等变换"><a href="#3-1-矩阵的初等变换" class="headerlink" title="3.1 矩阵的初等变换"></a>3.1 矩阵的初等变换</h3><h3 id="3-2-矩阵的秩"><a href="#3-2-矩阵的秩" class="headerlink" title="3.2 矩阵的秩"></a>3.2 矩阵的秩</h3><h3 id="3-3-方程组的解"><a href="#3-3-方程组的解" class="headerlink" title="3.3 方程组的解"></a>3.3 方程组的解</h3><h2 id="第4章-向量组的线性相关性"><a href="#第4章-向量组的线性相关性" class="headerlink" title="第4章 向量组的线性相关性"></a>第4章 向量组的线性相关性</h2><h3 id="4-1-向量组及其线性组合"><a href="#4-1-向量组及其线性组合" class="headerlink" title="4.1 向量组及其线性组合"></a>4.1 向量组及其线性组合</h3><h3 id="4-2-向量组的线性相关性"><a href="#4-2-向量组的线性相关性" class="headerlink" title="4.2 向量组的线性相关性"></a>4.2 向量组的线性相关性</h3><h3 id="4-3-向量组的秩"><a href="#4-3-向量组的秩" class="headerlink" title="4.3 向量组的秩"></a>4.3 向量组的秩</h3><h3 id="4-4-线性方程组解的结构"><a href="#4-4-线性方程组解的结构" class="headerlink" title="4.4 线性方程组解的结构"></a>4.4 线性方程组解的结构</h3><h3 id="4-5-向量空间"><a href="#4-5-向量空间" class="headerlink" title="4.5 向量空间"></a>4.5 向量空间</h3><h2 id="第5章-相似矩阵及二次型"><a href="#第5章-相似矩阵及二次型" class="headerlink" title="第5章 相似矩阵及二次型"></a>第5章 相似矩阵及二次型</h2><h3 id="5-1-向量的内积、长度及正交性"><a href="#5-1-向量的内积、长度及正交性" class="headerlink" title="5.1 向量的内积、长度及正交性"></a>5.1 向量的内积、长度及正交性</h3><h3 id="5-2-方阵的特征值与特征向量"><a href="#5-2-方阵的特征值与特征向量" class="headerlink" title="5.2 方阵的特征值与特征向量"></a>5.2 方阵的特征值与特征向量</h3><h3 id="5-3-相似矩阵"><a href="#5-3-相似矩阵" class="headerlink" title="5.3 相似矩阵"></a>5.3 相似矩阵</h3><h3 id="5-4-对称矩阵的对角化"><a href="#5-4-对称矩阵的对角化" class="headerlink" title="5.4 对称矩阵的对角化"></a>5.4 对称矩阵的对角化</h3><h3 id="5-5-二次型及其标准型"><a href="#5-5-二次型及其标准型" class="headerlink" title="5.5 二次型及其标准型"></a>5.5 二次型及其标准型</h3><h3 id="5-6-正定二次型"><a href="#5-6-正定二次型" class="headerlink" title="5.6 正定二次型"></a>5.6 正定二次型</h3>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>机器学习笔记</title>
      <link href="/posts/5a8a6c8d.html"/>
      <url>/posts/5a8a6c8d.html</url>
      
        <content type="html"><![CDATA[<h1 id="统计学习方法"><a href="#统计学习方法" class="headerlink" title="统计学习方法"></a>统计学习方法</h1><p><em>赫尔伯特.西蒙：”如果一个系统能够通过执行某个过程改进它的性能，这就是学习。“</em></p><h3 id="1-1-统计学习-基本概念"><a href="#1-1-统计学习-基本概念" class="headerlink" title="1.1 统计学习 基本概念"></a>1.1 统计学习 基本概念</h3><ul><li>对象： 数据</li><li>目的： 对数据预测与分析</li><li>方法： 监督学习，无监督学习，强化学习</li><li>三要素： 模型，策略，算法</li></ul><h3 id="1-2-统计学习分类"><a href="#1-2-统计学习分类" class="headerlink" title="1.2 统计学习分类"></a>1.2 统计学习分类</h3><h4 id="1-2-1监督学习："><a href="#1-2-1监督学习：" class="headerlink" title="1.2.1监督学习："></a>1.2.1监督学习：</h4><p>学习输入到输出的统计规律。</p><ol><li>输入空间，输出空间，特征空间：输入与输出对成为样本</li><li>联合概率分布：假设输入X输出Y遵循联合分布概率P（X，Y），表示分布密度函数</li><li>假设空间：学习范围的确定</li><li>模型：用训练数据集学习一个模型，再用模型对测试样本集进行预测。由学习系统和预测系统完成。</li></ol><h4 id="1-2-2无监督学习："><a href="#1-2-2无监督学习：" class="headerlink" title="1.2.2无监督学习："></a>1.2.2无监督学习：</h4><p>从无标注数据中学习预测模型，学习数据中的统计规律或潜在结构。</p><ol><li>使用无标注数据学习或训练</li><li>可以用于对已有数据的分析，也可以对未来数据预测</li></ol><h4 id="1-2-3强化学习"><a href="#1-2-3强化学习" class="headerlink" title="1.2.3强化学习"></a>1.2.3强化学习</h4><p>智能系统在与环境的连续互动中学习最优行为策略的机器学习问题，本质为学习最优的序贯决策</p><ol><li>智能系统与环境互动：在每一步t，智能系统在环境中观测到一个状态st和一个奖励rt，采取一个动作at。环境根据智能体选择的动作，决定t+1的状态与奖励。目标是长期奖励的最大化。</li><li>马尔可夫决策过程</li></ol><h4 id="1-2-4半监督学习与主动学习"><a href="#1-2-4半监督学习与主动学习" class="headerlink" title="1.2.4半监督学习与主动学习"></a>1.2.4半监督学习与主动学习</h4><p>半监督学习，使用少量标注数据与大量未标注数据<br>主动学习，机器不断主动给出实例让教师进行标注，然后利用标注数据学习预测模型</p><h3 id="1-3-按模型分类"><a href="#1-3-按模型分类" class="headerlink" title="1.3 按模型分类"></a>1.3 按模型分类</h3><ol><li>概率模型与非概率模型：<ul><li>概率模型：决策树，朴素贝叶斯，隐马尔可夫，条件随机场，概率潜在语义分析，潜在迪利克雷分配，高斯混合模型</li><li>非概率模型：感知机，支持向量机，k近邻，adaboost，kmeans，潜在语义分析，神经网络<br>在监督学习中，概率模型是生成模型，非概率模型是判别模型<br>概率模型可用基本概率加法和乘法模型进行概率推理。</li></ul></li><li>线性模型与非线性模型：<ul><li>针对非概率模型，如果y=f（x）或z=g（x）是线性函数，则称模型为线性模型</li><li>否则为非线性模型</li></ul></li><li>参数化模型与非参数化模型：<ul><li>参数化模型假设模型参数维度固定，模型可用有限维数刻画</li><li>非参数化：维数不固定或无穷大，随训练数据量增大而增大</li></ul></li></ol><h3 id="1-4按算法分类"><a href="#1-4按算法分类" class="headerlink" title="1.4按算法分类"></a>1.4按算法分类</h3><ol><li>在线学习：每次接受一个样本进行预测，之后学习模型</li><li>批量学习：一次接受所有数据，学习模型之后进行预测</li></ol><h3 id="1-5三要素"><a href="#1-5三要素" class="headerlink" title="1.5三要素"></a>1.5三要素</h3><ol><li>模型：模型的假设空间包含所有可能的条件概率或决策函数</li><li>策略：在假设空间中寻找最优模型<ul><li>损失和风险函数：经验风险是模型关于训练样本集的平均损失记作Rexp（f）当样本容量趋于无穷时，经验风险趋于期望风险。</li><li>经验风险最小化和结构风险最小化</li><li>过拟合：为了减少训练误差而使得模型复杂度增加，导致测试误差增大（M次多项式拟合问题）</li></ul></li><li>算法：模型的具体计算方法</li></ol><h3 id="1-6正则化与交叉验证"><a href="#1-6正则化与交叉验证" class="headerlink" title="1.6正则化与交叉验证"></a>1.6正则化与交叉验证</h3><ol><li>正则化：在经验风险上加一个正则化项，模型越复杂，正则化数越大。</li><li>交叉验证：给定的数据切分，组合成为训练集与测试集，反复训练</li></ol><h3 id="1-7泛化能力"><a href="#1-7泛化能力" class="headerlink" title="1.7泛化能力"></a>1.7泛化能力</h3><ul><li>指由该方法学习到的模型对未知数据的预测能力</li><li>设学到的模型为f</li></ul><script type="math/tex; mode=display">R_{exp}(\widehat{f})=E_{p}[L(Y,\widehat{f}(X))]=\int_{\chi * \gamma }^{}L(y,\widehat{f}(x))P(x,y)d_{x}d_{y}</script><h3 id="1-8生成模型与判别模型（在监督学习中）"><a href="#1-8生成模型与判别模型（在监督学习中）" class="headerlink" title="1.8生成模型与判别模型（在监督学习中）"></a>1.8生成模型与判别模型（在监督学习中）</h3><ol><li>生成方法：由数据学习联合概率分布P（X，Y），求出P（Y|X）作为预测的模型：<script type="math/tex; mode=display">P(Y|X)=P(X,Y)/P(X)</script><ul><li>因为模型表示了给定输入X产生输出Y的生成关系</li><li>e.g.朴素贝叶斯，隐马尔可夫</li><li>特点：可以还原出联合分布概率P（X，Y），收敛速度更快</li></ul></li><li>判别方法：直接学习决策函数f（x）或条件概率分布P（X，Y）作为预测的模型。关心的是对给定的输入X，应预测什么样的输出Y<ul><li>特点：直接面对预测，直接学习P（Y|X）或决策函数f（X），可以对数据进行各种程度上的抽象、定义特征并使用特征，可以简化学习问题。</li></ul></li></ol><h3 id="1-9-监督学习应用"><a href="#1-9-监督学习应用" class="headerlink" title="1.9 监督学习应用"></a>1.9 监督学习应用</h3><ol><li>分类问题</li><li>标注问题<ul><li>学习一个模型，使它能够对观测序列给出标记序列作为预测（单词序列预测，词性标注）</li></ul></li><li>回归问题<ul><li>预测输入变量与输出变量之间的关系，特别是输入变量的值发生变化时，输出变量随之发生的变化。</li></ul></li></ol><h3 id="2-1感知机模型"><a href="#2-1感知机模型" class="headerlink" title="2.1感知机模型"></a>2.1感知机模型</h3><ul><li>感知机是二类分类的线性分类模型，输入为特征向量输出为类别，取+1，-1.将实例划分为正负两类分离超平面属于判别模型。<script type="math/tex; mode=display">f(x)=sign(\omega x+b)</script></li><li>$\omega$叫做权值向量，b叫做偏置<script type="math/tex; mode=display">sign(x)=\left\{\begin{matrix}+1 &x\geqslant 0  \\-1& x<0 \\\end{matrix}\right.</script></li><li>假设空间是特征空间中所有线性分类模型</li><li>训练数据集：实例的特征向量及类别（xi，yi）</li></ul><h3 id="2-2感知机学习策略"><a href="#2-2感知机学习策略" class="headerlink" title="2.2感知机学习策略"></a>2.2感知机学习策略</h3><ol><li>线性可分性：存在某个超平面S：$\omega x+b=0$能够将数据集的正负实例点完全正确划分到超平面的两侧，称数据集为线性可分数据集</li><li>感知机学习策略：目标是求得一个能够将训练集正实例点和负实例点完全正确分开的超平面。确定$\omega$和b，使损失最小化。<ul><li>损失函数：误分类点到超平面的总距离<script type="math/tex; mode=display">L(\omega ,b)=-\sum_{xi\in M}^{} y_{i}(\omega x_{i}+b)</script></li></ul></li></ol><h3 id="2-3感知机学习算法"><a href="#2-3感知机学习算法" class="headerlink" title="2.3感知机学习算法"></a>2.3感知机学习算法</h3><ul><li>随机梯度下降：学习率$\eta$(步长）（0-1），</li></ul><ol><li>任选取超平面w0，b0，</li><li>选取数据（xi，yi）</li><li>若<script type="math/tex">y_{i}(\omega x_{i}+b)\leqslant 0</script><script type="math/tex; mode=display">\omega<=\omega +\eta y_{i}x_{i}</script><script type="math/tex; mode=display">b<=b+\eta y_{i}</script></li><li>跳回（2），直到没有误分类点</li></ol><h3 id="2-4感知器学习算法的对偶形式"><a href="#2-4感知器学习算法的对偶形式" class="headerlink" title="2.4感知器学习算法的对偶形式"></a>2.4感知器学习算法的对偶形式</h3><ul><li>基本想法，将w和b表示为实例xi和标记yi的线性组合的形式，通过解其系数球的w和b。初始w和b为0，经过对误分类点的修改后，最后学习到的w和b可以表示为：(这里$\alpha$i=ni$\eta$)<script type="math/tex; mode=display">\omega=\sum_{i=1}^{N} \alpha _{i}x_{i}y_{i}</script><script type="math/tex; mode=display">b=\sum_{i=1}^{N} \alpha _{i}y_{i}</script><ol><li>a=0,b=0</li><li>在训练集中选取数据（xi，yi）</li><li>如果$y<em>{i}(\sum</em>{j=1}^{N}\alpha <em>{j}y</em>{j}x<em>{j}\cdot x</em>{i}+b)\leqslant 0$ :<script type="math/tex; mode=display">\alpha_{i}<=\alpha _{i}+\eta</script><script type="math/tex; mode=display">b<=b+\eta y_{i}</script></li><li>转到2直到没有误分类数据</li></ol></li></ul><h3 id="3-1k近邻算法"><a href="#3-1k近邻算法" class="headerlink" title="3.1k近邻算法"></a>3.1k近邻算法</h3><ul><li>给定一个训练数据集，对于新的训练实例，在训练数据集中找到与该实例最邻近的k个实例，这k个实例多数属于某个类，就把该输入实例分为这个类。</li></ul><h3 id="3-2k近邻模型"><a href="#3-2k近邻模型" class="headerlink" title="3.2k近邻模型"></a>3.2k近邻模型</h3><ol><li>模型：根据训练集，距离度量，k值，以及决策规则，将特征空间划分为一些子空间。</li><li>距离度量：点xi与xj的Lp距离：<script type="math/tex; mode=display">L_{p}(x_{i},x_{j})=(\sum_{l=1}^{n}\left|x_{i}^{(l)}- x_{j}^{(l)}\right|^{p})^{\frac{1}{p}}</script><ul><li>p=2,欧氏距离</li></ul></li></ol><script type="math/tex; mode=display">L_{2}(x_{i},x_{j})=(\sum_{l=1}^{n}\left|x_{i}^{(l)}- x_{j}^{(l)}\right|^{2})^{\frac{1}{2}}</script><ul><li>p=1,曼哈顿距离<script type="math/tex; mode=display">L_{1}(x_{i},x_{j})=\sum_{l=1}^{n}\left|x_{i}^{(l)}- x_{j}^{(l)}\right|</script></li><li>p=无穷<script type="math/tex; mode=display">L_{\infty }(x_{i},x_{j})=max\left|x_{i}^{(l)}- x_{j}^{(l)}\right|</script></li><li>不同距离度量确定的最近邻点是不同的</li></ul><h4 id="k值的选择"><a href="#k值的选择" class="headerlink" title="k值的选择"></a>k值的选择</h4><ul><li>k比较小，模型复杂容易过拟合</li><li>k大，学习误差小，模型简单</li></ul><h3 id="3-3k近邻的实现：kd树（未理解）"><a href="#3-3k近邻的实现：kd树（未理解）" class="headerlink" title="3.3k近邻的实现：kd树（未理解）"></a>3.3k近邻的实现：kd树（未理解）</h3><ul><li>kd树的构造：<ol><li>开始，构造根节点，由根节点生成深度为1的左右子节点。。。将落在切分超平面上的实例点保存在根节点</li><li>重复：对深度为j的结点选择x(l)为切分的坐标轴，l=j(mod k)+1,以该节点对应的区域中所有实例x(l)坐标的中位数为切分点。切分通过切分点并与坐标轴x(l)垂直的超平面实现。将落在切分超平面上的实例点保存在该节点。</li><li>直到两个子区域没有实例时停止。</li></ol></li><li>kd树搜索：</li></ul><h3 id="4-1朴素贝叶斯"><a href="#4-1朴素贝叶斯" class="headerlink" title="4.1朴素贝叶斯"></a>4.1朴素贝叶斯</h3><ol><li>基本方法：x为N维向量，y为类标记（class label），P（X，Y）为x和y的联合概率分布，训练数据集<script type="math/tex">T=\begin{Bmatrix}(x_{1},y_{1}),(x_{2},y_{2}),...,(x_{n},y_{n})\end{Bmatrix}</script><br>由P（X，Y）独立同分布产生。<ul><li>通过训练数据集学习联合分布概率P（X，Y）。先学习先验概率分布及条件概率分布：<script type="math/tex">P(Y=c_{k}),k=1,2,...,K</script></li><li>由于条件概率分布P（X=x，Y=ck）有指数量级的参数，不宜计算。朴素贝叶斯对条件概率分布作了条件独立性的假设：<script type="math/tex; mode=display">P(X=x|Y=c_{k})=P(X^{(1)}=x^{(1)},...,X^{(n)}=x^{(n)}|Y=c_{k})</script><script type="math/tex; mode=display">=\prod_{j=1}^{n}P(X^{(j)}  =x^{(j)}|Y=c_{k})</script></li><li>由学习到生成数据的机制，属于生成模型。条件独立假设等于说用于分类的特征在类确定条件下都是条件独立的。这一假设使朴素贝叶斯变得简单，有时牺牲准确率。</li><li>后验概率计算根据贝叶斯定理：<script type="math/tex; mode=display">P(Y=c_{k}|X=x)=\frac{P(X=x|Y=c_{k})P(Y=c_{k})}{\sum_{k}^{}P(X=x|Y=c_{k})P(Y=c_{k})}</script>将以上假设带入得到朴素贝叶斯分类基本公式：<script type="math/tex; mode=display">P(Y=c_{k}|X=x)=\frac{P(X=x|Y=c_{k})\prod_{j=1}^{n}P(X^{(j)}  =x^{(j)}|Y=c_{k})}{\sum_{k}^{}\prod_{j=1}^{n}P(X^{(j)}  =x^{(j)}|Y=c_{k})P(Y=c_{k})}</script></li><li>朴素贝叶斯分类器可表示为<script type="math/tex; mode=display">y=f(x)=argmaxP(Y=c_{k}|X=x)</script><script type="math/tex; mode=display">=\prod_{j=1}^{n}P(X^{(j)}  =x^{(j)}|Y=c_{k})</script>2.后验概率最大化的含义：为了使期望风险函数（条件期望）最小化，推导出后验概率最大化。</li></ul></li></ol><h3 id="4-2朴素贝叶斯的参数估计"><a href="#4-2朴素贝叶斯的参数估计" class="headerlink" title="4.2朴素贝叶斯的参数估计"></a>4.2朴素贝叶斯的参数估计</h3><ol><li>极大似然估计：<br><em>极大似然估计，通俗理解来说，就是利用已知的样本结果信息，反推最具有可能（最大概率）导致这些样本结果出现的模型参数值！换句话说，极大似然估计提供了一种给定观察数据来评估模型参数的方法，即：“模型已定，参数未知”。</em></li><li>朴素贝叶斯算法：<ul><li>计算先验概率及条件概率<script type="math/tex; mode=display">P(Y=c_{k})=\frac{\sum_{i=1}^{N}I(y_{i}=c_{k})}{N},k=1,2,...K</script><script type="math/tex; mode=display">P(X^{(j)}=a_{jl}|Y=c_{k})=\frac{\sum_{i=1}^{N}I(x_{i}^{(j)}=a_{jl},y_{i}=c_{k})}{\sum_{i=1}^{N}I(y_{i}=c_{k})}</script>j=1,2,…n;l=1,2,…Sj;k=1,2…K</li><li>对于给定的实例<script type="math/tex">x=(x^{(1)},x^{(2)},...x^{(n)})</script><script type="math/tex; mode=display">P(Y=c_{k})\prod_{j=1}^{n}P(X^{(j)}=x^{(j)}|Y=c_{k})</script></li><li>通过计算上式在$P(Y=c_{k})$为何值时取得最大，得出实例x的类。</li></ul></li><li>贝叶斯估计：用极大似然估计可能会出现所求概率值为0的情况。会影响后验概率的计算结果，使分类产生偏差。<script type="math/tex">P(X^{(j)}=a_{jl}|Y=c_{k})=\frac{\sum_{i=1}^{N}I(x_{i}^{(j)}=a_{jl},y_{i}=c_{k})+\lambda }{\sum_{i=1}^{N}I(y_{i}=c_{k})+\lambda S_{j}}</script>在随机变量各个取值的频数上赋予一个正数$\lambda$,常取1.成为拉普拉斯平滑。Sj为x所有可能总数量。同样，先验概率的贝叶斯估计是：<script type="math/tex">P(Y=c_{k})=\frac{\sum_{i=1}^{N}I(y_{i}=c_{k})+\lambda }{N+K\lambda },k=1,2,...K</script>K为所有y可能取值的总数量</li></ol><h3 id="5支持向量机"><a href="#5支持向量机" class="headerlink" title="5支持向量机"></a>5支持向量机</h3><p><em>支持向量机是一种二类分类模型。它的基本模型是定义在特征空间上的间隔最大的线性分类器，间隔最大使它有别于感知机。支持向量机的学习策略是间隔最大化，可形式化为一个求解凸二次规划的问题，等价于正则化的合页损失函数的最小化问题。</em></p><h3 id="5-1线性可分支持向量机与硬间隔最大化"><a href="#5-1线性可分支持向量机与硬间隔最大化" class="headerlink" title="5.1线性可分支持向量机与硬间隔最大化"></a>5.1线性可分支持向量机与硬间隔最大化</h3><ol><li>假设输入空间与特征空间为两个不同的空间。线性可分支持向量机假设这两个空间的元素一一对应，将输入空间中的输入映射为特征空间中的特征向量。假定给定一个特征空间上的训练数据集：<script type="math/tex">T=\begin{Bmatrix}(x_{1},y_{1}),(x_{2},y_{2}),...,(x_{n},y_{n})\end{Bmatrix}</script><br>其中：<script type="math/tex">x_{i}\in \chi =R^{n},y_{i}\in \gamma =\begin{Bmatrix}+1,-1\end{Bmatrix},i=1,2,...N</script><br>yi=1,称xi为正例；yi=-1，称xi为负例。假设训练数据是线性可分的，学习的目标是在特征空间中找到一个分离超平面，能将实例分到不同的类。分离超平面对应方程wx+b=0，由法向量w和截距决定。法向量指向的一侧为正类，一侧为负类。<br>当训练数据集线性可分时，由于存在无穷个分离超平面。感知机利用误分类最小的策略，，解有无穷多个；线性可分支持向量机利用间隔最大化求解最优分离超平面，这时，解是唯一的。<br>超平面：<script type="math/tex">\omega^{*} x+b^{*}=0</script><br>相应的分类决策函数：<script type="math/tex; mode=display">f(x)=sign(\omega^{*} x+b^{*})</script></li><li><p>函数间隔和几何间隔<br>一般来说，一个点距离超平面的远近可以表示分类预测的确信程度。在超平面$\omega x+b=0$<br>确定的情况下，用$\left| \omega x+b\right|$来相对表示x距离超平面的远近。$\omega x+b=0$的符号与标记y的符号是否一致来表示分类是否正确。<br>对于某一样本点的函数间隔：</p><script type="math/tex; mode=display">\widehat{\gamma _{i}}=y_{i}(\omega x_{i}+b)</script><ul><li>定义：超平面 （w，b）关于训练集T的函数间隔为超平面（w，b）关于T中所有样本点（xi，yi）的函数间隔的最小值：<script type="math/tex; mode=display">\widehat{\gamma }=min\widehat{\gamma _{i}}</script>函数间隔可以表示分类预测的正确性和确信度。但如果等比例改变w和b，超平面没有改变，但是函数间隔变化。因此，我们要对分离超平面的法向量w加以约束，如规范化||w||=1，使得间隔是确定的，这时间隔为几何间隔：<script type="math/tex; mode=display">\widehat{\gamma _{i}}=y_{i}(\frac{\omega}{\begin{Vmatrix}\omega \end{Vmatrix}} x_{i}+\frac{b}{\begin{Vmatrix}\omega \end{Vmatrix}})</script>同理，定义超平面（w，b）关于训练数据集T的几何间隔为各样本几何间隔最小值。<script type="math/tex; mode=display">\widehat{\gamma }=min\widehat{\gamma _{i}}</script></li></ul></li><li>硬间隔最大化：<ul><li>最大间隔分离超平面：<script type="math/tex; mode=display">\underset{\omega ,b}{min} \frac{1}{2}\begin{Vmatrix}\omega \end{Vmatrix}^{2}</script><script type="math/tex; mode=display">s.t.\ \  y_{i}(\omega x_{i}+b)-1\geqslant 0,i=1,2,...N</script>凸二次规划问题</li></ul></li><li>支持向量与间隔边界：在线性可分发情况下，训练样本数据集中的样本点中与分离超平面距离最近的样本点的实例成为支持向量.支持向量是使下式成立的点。<script type="math/tex">y_{i}(\omega x_{i}+b)-1= 0</script>对yi=+1的正例点，支持向量在超平面<script type="math/tex">H1:\omega x+b=1</script>对于yi=-1的负例点，支持向量在超平面<script type="math/tex">H2:\omega x+b=-1</script>在H1和H2上的点就是支持向量。</li></ol><h3 id="5-2软间隔最大化"><a href="#5-2软间隔最大化" class="headerlink" title="5.2软间隔最大化"></a>5.2软间隔最大化</h3><h3 id="6神经网络"><a href="#6神经网络" class="headerlink" title="6神经网络"></a>6神经网络</h3><ul><li>简介（以下由chatgpt生成）<br>神经网络算法是一类基于生物神经系统思维方式和人工智能技术的计算模型，被广泛应用于机器学习、图像识别、自然语言处理、语音识别、控制系统等领域。常用的神经网络算法包括以下几种：</li></ul><ol><li><p>前馈神经网络（Feedforward Neural Networks）：是最基本、最常用的神经网络算法，其特点是信息传递是单向的，不具有反馈和循环。可以通过调节网络的层数、神经元数量和权重等参数进行训练和优化。</p></li><li><p>卷积神经网络（Convolutional Neural Networks）：主要用于图像识别和计算机视觉等领域，其结构由多层卷积层、池化层和全连接层构成，可以有效地提取图像特征。</p></li><li><p>循环神经网络（Recurrent Neural Networks）：具有时间依赖性，可以处理序列数据、自然语言处理、语音识别等任务。其具有循环结构，可以记住过去的状态信息，并且通过反馈机制将当前状态与前面的状态相连。</p></li><li><p>长短期记忆网络（Long Short-Term Memory Networks）：是一种特殊的循环神经网络，能够有效地解决长时间依赖问题，主要应用于自然语言处理、语音识别、机器翻译等领域。</p></li><li><p>自编码器（Autoencoders）：是一种无监督学习方法，可以用于数据压缩、特征提取、图像去噪等任务，其原理是通过学习数据的低维表示来重构输入数据。</p></li></ol><p>神经网络算法的优点是能够进行大规模并行计算、处理非线性问题、具有自适应性和学习能力等，但也存在训练难度大、容易陷入局部最优解等问题。<br>当人们谈论神经网络时，通常指的是深度神经网络（Deep Neural Networks，DNNs），它们是一种人工神经网络（Artificial Neural Networks，ANNs）的变种。在深度学习中，神经网络是最流行的算法之一，用于解决各种任务，包括图像识别、自然语言处理和语音识别。</p><p>神经网络算法的基本原理是通过学习数据的规律来构建一个函数，将输入映射到输出。这个函数由神经网络的各个层组成，每个层都由多个神经元（或称为节点）组成。每个神经元接收到来自前一层的输入，并通过一些权重和偏置来计算输出。</p><p>神经网络的训练过程通常使用反向传播算法（Backpropagation）来优化神经元的权重和偏置。这个算法通过计算损失函数的梯度来更新神经元的权重和偏置，以使模型的预测更加准确。损失函数通常是一个衡量模型预测值与真实值之间差距的函数。</p><p>底层逻辑方面，神经网络是一种前馈网络，即数据从输入层开始流向输出层，中间没有反馈或循环。神经网络中每个神经元的输出是通过对它们的输入进行一系列数学运算来计算得出的。这些运算通常包括对输入进行加权求和，然后使用激活函数来转换成神经元的输出。</p><p>在深度神经网络中，通常有多个隐藏层，每个隐藏层都有多个神经元。每个隐藏层可以看作是对输入的一种抽象表示，它将输入中的一些特征提取出来并传递给下一层，最终得到输出。通过增加隐藏层和神经元的数量，神经网络可以学习更加复杂的模式和规律。</p><h3 id="7最速下降法"><a href="#7最速下降法" class="headerlink" title="7最速下降法"></a>7最速下降法</h3><ul><li>输入：目标函数f（x），梯度函数<script type="math/tex">g(x)=\triangledown f(x)</script>,精度$\varepsilon$.</li><li>输出：f（x）的极小值点x*</li></ul><ol><li>取初始值$x^{(0)}$,置k=0</li><li>计算$f(x^{(k)})$</li><li>计算梯度$g<em>{k}=g(x^{(k)})$,当$\begin{Vmatrix}g</em>{k}\end{Vmatrix}&lt;\varepsilon$时停止迭代，令x^{*}=x^{(k)}，否则令$p<em>{k}=-g(x^{(k)})$,求$\lambda</em>{k}$,,使：<script type="math/tex; mode=display">f(x^{(k)}+\lambda _{k}p_{k})=\underset{\lambda \geqslant 0}{min}f(x^{(k)}+\lambda p_{k})</script></li><li>置$x^{(k+1)}=x^{(k)}+\lambda <em>{k}p</em>{k}$,计算 $f(x^{(k+1)})$<br>当 $\begin{Vmatrix}f(x^{(k+1)}-f(x)^{(k)})\end{Vmatrix}&lt;\varepsilon$ 或： $\begin{Vmatrix}x^{(k+1)}-x^{(k)}\end{Vmatrix}&lt;\varepsilon$ 时，停止迭代，令 $x^{*}=x^{(k+1)}$</li><li>否则，令k=k+1，转到3.<br>计算量小，存储量小，必收敛<br>局部最优，收敛速度慢，大多数条件下线性收敛</li></ol><h3 id="8牛顿法"><a href="#8牛顿法" class="headerlink" title="8牛顿法"></a>8牛顿法</h3><ul><li>优点：收敛速度快，大多数情况下超线性收敛</li><li>缺点：二阶梯度正定时才下降，计算量大，当牛顿法无法产生下降方向时，用最速下降法代替</li></ul>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>一份（不太）简短的个人简介（完善中）</title>
      <link href="/posts/d63b390f.html"/>
      <url>/posts/d63b390f.html</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Hello World</title>
      <link href="/posts/4a17b156.html"/>
      <url>/posts/4a17b156.html</url>
      
        <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>]]></content>
      
      
      
    </entry>
    
    
  
  
    
    
    <entry>
      <title></title>
      <link href="/css/custom.css"/>
      <url>/css/custom.css</url>
      
        <content type="html"><![CDATA[/* @font-face {  font-family: Candyhome;  src: url(https://npm.elemecdn.com/anzhiyu-blog@1.1.6/fonts/Candyhome.ttf);  font-display: swap;  font-weight: lighter;} */@font-face {    font-family: ZhuZiAYuanJWD;    src: url(https://npm.elemecdn.com/anzhiyu-blog@1.1.6/fonts/ZhuZiAWan.woff2);    font-display: swap;    font-weight: lighter;  }    div#menus {    font-family: "ZhuZiAYuanJWD";  }  h1#site-title {    font-family: ZhuZiAYuanJWD;    font-size: 3em !important;  }  a.article-title,  a.blog-slider__title,  a.categoryBar-list-link,  h1.post-title {    font-family: ZhuZiAYuanJWD;  }    .iconfont {    font-family: "iconfont" !important;    font-size: 3em;    /* 可以定义图标大小 */    font-style: normal;    -webkit-font-smoothing: antialiased;    -moz-osx-font-smoothing: grayscale;  }    /* 时间轴生肖icon */  svg.icon {    /* 这里定义svg.icon，避免和Butterfly自带的note标签冲突 */    width: 1em;    height: 1em;    /* width和height定义图标的默认宽度和高度*/    vertical-align: -0.15em;    fill: currentColor;    overflow: hidden;  }    .icon-zhongbiao::before {    color: #f7c768;  }    /* bilibli番剧插件 */  #article-container .bangumi-tab.bangumi-active {    background: var(--anzhiyu-theme);    color: var(--anzhiyu-ahoverbg);    border-radius: 10px;  }  a.bangumi-tab:hover {    text-decoration: none !important;  }  .bangumi-button:hover {    background: var(--anzhiyu-theme) !important;    border-radius: 10px !important;    color: var(--anzhiyu-ahoverbg) !important;  }  a.bangumi-button.bangumi-nextpage:hover {    text-decoration: none !important;  }  .bangumi-button {    padding: 5px 10px !important;  }    a.bangumi-tab {    padding: 5px 10px !important;  }  svg.icon.faa-tada {    font-size: 1.1em;  }  .bangumi-info-item {    border-right: 1px solid #f2b94b;  }  .bangumi-info-item span {    color: #f2b94b;  }  .bangumi-info-item em {    color: #f2b94b;  }    /* 解决artitalk的图标问题 */  #uploadSource > svg {    width: 1.19em;    height: 1.5em;  }    /*top-img黑色透明玻璃效果移除，不建议加，除非你执着于完全一图流或者背景图对比色明显 */  #page-header:not(.not-top-img):before {    background-color: transparent !important;  }    /* 首页文章卡片 */  #recent-posts > .recent-post-item {    background: rgba(255, 255, 255, 0.9);  }    /* 首页侧栏卡片 */  #aside-content .card-widget {    background: rgba(255, 255, 255, 0.9);  }    /* 文章页面正文背景 */  div#post {    background: rgba(255, 255, 255, 0.9);  }    /* 分页页面 */  div#page {    background: rgba(255, 255, 255, 0.9);  }    /* 归档页面 */  div#archive {    background: rgba(255, 255, 255, 0.9);  }    /* 标签页面 */  div#tag {    background: rgba(255, 255, 255, 0.9);  }    /* 分类页面 */  div#category {    background: rgba(255, 255, 255, 0.9);  }    /*夜间模式伪类遮罩层透明*/  [data-theme="dark"] #recent-posts > .recent-post-item {    background: #121212;  }    [data-theme="dark"] .card-widget {    background: #121212 !important;  }    [data-theme="dark"] div#post {    background: #121212 !important;  }    [data-theme="dark"] div#tag {    background: #121212 !important;  }    [data-theme="dark"] div#archive {    background: #121212 !important;  }    [data-theme="dark"] div#page {    background: #121212 !important;  }    [data-theme="dark"] div#category {    background: #121212 !important;  }    [data-theme="dark"] div#category {    background: transparent !important;  }  /* 页脚透明 */  #footer {    background: transparent !important;  }    /* 头图透明 */  #page-header {    background: transparent !important;  }    #rightside > div > button {    border-radius: 5px;  }    /* 滚动条 */    ::-webkit-scrollbar {    width: 10px;    height: 10px;  }    ::-webkit-scrollbar-thumb {    background-color: #3b70fc;    border-radius: 2em;  }    ::-webkit-scrollbar-corner {    background-color: transparent;  }    ::-moz-selection {    color: #fff;    background-color: #3b70fc;  }    /* 音乐播放器 */    /* .aplayer .aplayer-lrc {    display: none !important;  } */    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {    left: -66px !important;    transition: all 0.3s;    /* 默认情况下缩进左侧66px，只留一点箭头部分 */  }    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {    left: 0 !important;    transition: all 0.3s;    /* 鼠标悬停是左侧缩进归零，完全显示按钮 */  }    .aplayer.aplayer-fixed {    z-index: 999999 !important;  }    /* 评论框  */  .vwrap {    box-shadow: 2px 2px 5px #bbb;    background: rgba(255, 255, 255, 0.3);    border-radius: 8px;    padding: 30px;    margin: 30px 0px 30px 0px;  }    /* 设置评论框 */    .vcard {    box-shadow: 2px 2px 5px #bbb;    background: rgba(255, 255, 255, 0.3);    border-radius: 8px;    padding: 30px;    margin: 30px 0px 0px 0px;  }    /* 鼠标图标 */  body {    cursor: url("/img/x1.cur"), auto;  }  a,  [type="button"]:not(:disabled),  [type="reset"]:not(:disabled),  [type="submit"]:not(:disabled),  button:not(:disabled) {    cursor: url("/img/x2.cur"), auto !important;  }  /* md网站下划线 */  #article-container a:hover {    text-decoration: none !important;  }    #article-container #hpp_talk p img {    display: inline;  }    /* 404页面 */  #error-wrap {    position: absolute;    top: 40%;    right: 0;    left: 0;    margin: 0 auto;    padding: 0 1rem;    max-width: 1000px;    transform: translate(0, -50%);  }    #error-wrap .error-content {    display: flex;    flex-direction: row;    justify-content: center;    align-items: center;    margin: 0 1rem;    height: 18rem;    border-radius: 8px;    background: var(--card-bg);    box-shadow: var(--card-box-shadow);    transition: all 0.3s;  }    #error-wrap .error-content .error-img {    box-flex: 1;    flex: 1;    height: 100%;    border-top-left-radius: 8px;    border-bottom-left-radius: 8px;    background-color: #3b70fc;    background-position: center;    background-size: cover;  }    #error-wrap .error-content .error-info {    box-flex: 1;    flex: 1;    padding: 0.5rem;    text-align: center;    font-size: 14px;    font-family: Titillium Web, "PingFang SC", "Hiragino Sans GB", "Microsoft JhengHei", "Microsoft YaHei", sans-serif;  }  #error-wrap .error-content .error-info .error_title {    margin-top: -4rem;    font-size: 9em;  }  #error-wrap .error-content .error-info .error_subtitle {    margin-top: -3.5rem;    word-break: break-word;    font-size: 1.6em;  }  #error-wrap .error-content .error-info a {    display: inline-block;    margin-top: 0.5rem;    padding: 0.3rem 1.5rem;    background: var(--btn-bg);    color: var(--btn-color);  }    #body-wrap.error .aside-list {    display: flex;    flex-direction: row;    flex-wrap: nowrap;    bottom: 0px;    position: absolute;    padding: 1rem;    width: 100%;    overflow: scroll;  }    #body-wrap.error .aside-list .aside-list-group {    display: flex;    flex-direction: row;    flex-wrap: nowrap;    max-width: 1200px;    margin: 0 auto;  }    #body-wrap.error .aside-list .aside-list-item {    padding: 0.5rem;  }    #body-wrap.error .aside-list .aside-list-item img {    width: 100%;    object-fit: cover;    border-radius: 12px;  }    #body-wrap.error .aside-list .aside-list-item .thumbnail {    overflow: hidden;    width: 230px;    height: 143px;    background: var(--anzhiyu-card-bg);    display: flex;  }    #body-wrap.error .aside-list .aside-list-item .content .title {    -webkit-line-clamp: 2;    overflow: hidden;    display: -webkit-box;    -webkit-box-orient: vertical;    line-height: 1.5;    justify-content: center;    align-items: flex-end;    align-content: center;    padding-top: 0.5rem;    color: white;  }    #body-wrap.error .aside-list .aside-list-item .content time {    display: none;  }    /* 代码框主题 */  #article-container figure.highlight {    border-radius: 10px;  }]]></content>
      
    </entry>
    
    
    
    <entry>
      <title></title>
      <link href="/manifest.json"/>
      <url>/manifest.json</url>
      
        <content type="html"><![CDATA[{"name":"whisper`Blog","short_name":"whisper","theme_color":"#3b70fc","background_color":"#3b70fc","display":"standalone","scope":"/","start_url":"/","icons":[{"src":"/img/siteicon/16.png","sizes":"16x16","type":"image/png"},{"src":"/img/siteicon/32.png","sizes":"32x32","type":"image/png"},{"src":"/img/siteicon/48.png","sizes":"48x48","type":"image/png"},{"src":"/img/siteicon/64.png","sizes":"64x64","type":"image/png"},{"src":"/img/siteicon/128.png","sizes":"128x128","type":"image/png"},{"src":"/img/siteicon/144.png","sizes":"144x144","type":"image/png"},{"src":"/img/siteicon/512.png","sizes":"512x512","type":"image/png"}],"splash_pages":null}]]></content>
      
    </entry>
    
    
    
    <entry>
      <title></title>
      <link href="/js/ali_font.js"/>
      <url>/js/ali_font.js</url>
      
        <content type="html"><![CDATA[!(function (c) {    var l,      h,      a,      t,      i,      v =        '<svg><symbol id="icon-dragon_chen" viewBox="0 0 1024 1024"><path d="M512 512m-296.421053 0a296.421053 296.421053 0 1 0 592.842106 0 296.421053 296.421053 0 1 0-592.842106 0Z" fill="#D6B196" ></path><path d="M970.105263 512c0 224.983579-163.166316 412.186947-377.263158 450.533053v-54.460632C777.135158 870.507789 916.210526 707.206737 916.210526 512c0-222.881684-181.328842-404.210526-404.210526-404.210526S107.789474 289.118316 107.789474 512s181.328842 404.210526 404.210526 404.210526c9.081263 0 18.000842-0.754526 26.947368-1.374315v53.894736c-8.973474 0.538947-17.866105 1.374316-26.947368 1.374316-252.604632 0-458.105263-205.500632-458.105263-458.105263S259.395368 53.894737 512 53.894737s458.105263 205.500632 458.105263 458.105263z m-498.122105 265.620211L431.157895 754.526316V485.052632h-66.074948c-14.470737 110.645895-44.355368 197.066105-102.696421 260.742736l-39.747368-36.432842C306.526316 617.876211 323.368421 462.901895 323.368421 242.526316V215.578947h377.263158v53.894737H377.182316c-0.404211 58.260211-2.209684 112.128-6.359579 161.684211H700.631579v53.894737h-122.152421a481.172211 481.172211 0 0 0 76.826947 119.70021l66.479158-39.855158 27.728842 46.214737-54.460631 32.687158c29.507368 24.953263 63.757474 45.675789 102.80421 58.098526l-16.303158 51.361684c-134.224842-42.711579-222.773895-167.073684-261.551158-268.207157H485.052632v221.857684l68.985263-41.391158 27.728842 46.214737-109.783579 65.886316zM646.736842 377.263158h-215.578947v-53.894737h215.578947v53.894737z" fill="#231F20" ></path></symbol><symbol id="icon-dog_xu" viewBox="0 0 1024 1024"><path d="M512 512m-296.421053 0a296.421053 296.421053 0 1 0 592.842106 0 296.421053 296.421053 0 1 0-592.842106 0Z" fill="#D6B196" ></path><path d="M970.105263 512c0 224.983579-163.166316 412.186947-377.263158 450.533053v-54.460632C777.135158 870.507789 916.210526 707.206737 916.210526 512c0-222.881684-181.328842-404.210526-404.210526-404.210526S107.789474 289.118316 107.789474 512s181.328842 404.210526 404.210526 404.210526c9.081263 0 18.000842-0.754526 26.947368-1.374315v53.894736c-8.973474 0.538947-17.866105 1.374316-26.947368 1.374316-252.604632 0-458.105263-205.500632-458.105263-458.105263S259.395368 53.894737 512 53.894737s458.105263 205.500632 458.105263 458.105263z m-375.592421 150.393263c33.684211 44.544 75.210105 74.698105 124.739369 90.812632l11.425684 3.718737 10.401684-6.009264C781.204211 727.740632 808.421053 622.565053 808.421053 592.842105h-53.894737c0 22.069895-19.132632 80.869053-33.711158 103.504842-34.816-14.605474-64.538947-39.262316-89.249684-74.13221 48.316632-55.269053 92.079158-117.328842 120.535579-179.900632l-49.044211-22.285473c-23.767579 52.250947-59.742316 104.717474-100.055579 152.656842-24.010105-50.930526-41.148632-115.927579-51.658105-195.395369H700.631579v-53.894737h-155.189895A1848.050526 1848.050526 0 0 1 538.947368 161.684211h-53.894736c0 58.206316 2.155789 112.074105 6.494315 161.68421H323.368421v26.947368c0 216.549053-13.177263 263.545263-100.702316 359.046737l39.747369 36.432842c63.326316-69.093053 92.806737-118.272 105.714526-206.848H485.052632v-53.894736h-111.319579a1742.147368 1742.147368 0 0 0 3.449263-107.789474h120.158316c12.611368 98.250105 35.031579 177.475368 67.395368 238.187789-61.978947 65.536-128.053895 117.975579-173.298526 142.282106l25.519158 47.481263c47.589053-25.573053 114.095158-77.446737 177.55621-142.821053z m125.170526-411.971368l-80.842105-80.842106-38.103579 38.103579 80.842105 80.842106 38.103579-38.103579z" fill="#231F20" ></path></symbol><symbol id="icon-dog" viewBox="0 0 1024 1024"><path d="M894.814316 904.434526l83.240421-183.134315-13.824-13.204211c-0.485053-0.458105-45.648842-47.589053-47.939369-185.263158-0.134737-7.922526-0.134737-33.953684-0.134736-55.996631-30.693053 15.306105-70.090105 19.887158-106.09179 19.887157-92.752842 0-163.624421-23.983158-210.647579-71.275789a192.512 192.512 0 0 1-27.944421-36.513684H377.263158v377.263158c342.662737 0 403.105684 51.092211 494.592 128.377263 7.922526 6.682947 15.521684 13.312 22.959158 19.86021z" fill="#85C3DE" ></path><path d="M326.063158 282.947368c0 34.250105-13.231158 44.463158-29.642105 44.463158s-29.642105-10.213053-29.642106-44.463158c0-34.223158 13.231158-44.463158 29.642106-44.463157s29.642105 10.24 29.642105 44.463157zM269.473684 430.295579v311.646316L190.275368 916.210526h59.203369L323.368421 753.637053V377.263158h-26.947368c-119.403789 0-172.732632-53.382737-185.505685-107.789474h35.624421c51.092211 0 68.581053-15.764211 120.535579-62.544842 12.773053-11.506526 28.079158-25.276632 47.023158-41.741474l18.351158-15.952842-69.658947-99.139368-44.085895 30.989474 41.768421 59.472842c-11.183158 9.862737-20.884211 18.593684-29.480421 26.327579C180.736 212.156632 176.235789 215.578947 146.539789 215.578947H53.894737v26.947369c0 88.710737 66.910316 178.149053 215.578947 187.769263z m216.710737-161.414737c2.290526 71.733895 28.698947 136.326737 75.048421 182.918737C618.711579 509.628632 702.437053 538.947368 810.091789 538.947368c18.593684 0 36.190316-1.158737 52.628211-3.449263 3.745684 111.265684 33.630316 170.334316 51.496421 196.015158l-38.507789 84.722526C782.174316 742.049684 688.774737 700.631579 377.263158 700.631579v53.894737c34.277053 0 65.697684 0.512 94.639158 1.509052L374.595368 970.105263h59.203369l96.013474-211.240421c66.182737 4.338526 117.005474 11.829895 157.911578 22.016L626.229895 916.210526h59.176421l54.16421-119.134315c47.616 18.405053 79.737263 42.091789 113.125053 69.739789L805.753263 970.105263h59.203369l113.071157-248.778105-13.824-13.204211c-0.485053-0.458105-45.648842-47.589053-47.939368-185.263158C985.168842 498.553263 1024 447.811368 1024 377.263158c0-95.205053-66.506105-161.684211-161.684211-161.684211v53.894737c65.482105 0 107.789474 42.307368 107.789474 107.789474 0 89.088-87.013053 107.789474-160.013474 107.789474-92.752842 0-163.624421-23.983158-210.647578-71.27579-30.315789-30.504421-45.891368-65.832421-53.35579-98.735158 11.210105 6.952421 22.932211 13.338947 35.274105 19.186527l23.04-48.720843c-92.106105-43.654737-148.992-128.646737-219.243789-243.981473l-46.026105 28.05221c49.448421 81.246316 92.968421 148.506947 147.051789 199.302737z" fill="#231F20" ></path></symbol><symbol id="icon-goat" viewBox="0 0 1024 1024"><path d="M548.378947 646.736842a952.32 952.32 0 0 1 140.90779-161.68421H107.789474c0 107.600842 0 107.600842-63.649685 169.283368l-13.069473 12.665263L66.721684 754.526316h417.172211c20.345263-41.472 43.654737-77.446737 64.485052-107.789474z" fill="#F7C768" ></path><path d="M608.256 144.734316C555.762526 115.577263 506.098526 107.789474 485.052632 107.789474V53.894737c32.579368 0 91.270737 11.452632 149.369263 43.735579 75.290947 41.822316 130.694737 94.531368 171.385263 150.878316C755.873684 288.013474 697.101474 323.368421 646.736842 323.368421h-107.789474v-53.894737h107.789474c20.506947 0 48.424421-11.210105 80.437895-31.285895a471.04 471.04 0 0 0-118.918737-93.453473zM832.673684 342.231579c-16.384 0-29.642105 10.24-29.642105 44.463158 0 34.250105 13.231158 44.463158 29.642105 44.463158s29.642105-10.213053 29.642105-44.463158c0-34.223158-13.231158-44.463158-29.642105-44.463158zM1024 619.789474C1024 347.109053 901.066105 122.448842 686.753684 3.395368l-26.165895 47.104C914.324211 191.461053 964.688842 440.400842 969.647158 592.842105h-84.506947c-17.92-35.624421-45.352421-69.12-87.013053-101.995789l-16.788211-13.285053-16.734315 13.392842c-66.128842 52.897684-134.629053 127.083789-187.311158 209.677474H102.965895l-8.272842-20.318316C159.043368 617.013895 161.684211 603.109053 161.684211 485.052632v-53.894737h485.052631v-53.894737H161.684211c0-80.384 14.309053-110.026105 66.586947-137.916632l-25.384421-47.535158C123.365053 234.226526 107.789474 291.920842 107.789474 377.263158v107.789474c0 107.600842 0 107.600842-63.649685 169.283368l-13.069473 12.665263L110.618947 862.315789h58.206316l-43.897263-107.789473h103.477895l43.897263 107.789473h58.206316l-43.897263-107.789473h259.47621C508.981895 824.939789 485.052632 899.152842 485.052632 970.105263h53.894736c0-68.688842 27.270737-144.060632 68.958316-215.578947H687.157895c7.410526 0 13.473684 6.063158 13.473684 13.473684V862.315789h53.894737v-94.315789c0-37.160421-30.208-67.368421-67.368421-67.368421h-44.65179c40.771368-58.017684 89.438316-111.427368 138.913684-153.626947C841.512421 600.037053 862.315789 655.225263 862.315789 754.526316h53.894737c0-38.912-2.748632-74.482526-11.102315-107.789474H1024v-26.947368z" fill="#231F20" ></path></symbol><symbol id="icon-goat_wei" viewBox="0 0 1024 1024"><path d="M512 512m-296.421053 0a296.421053 296.421053 0 1 0 592.842106 0 296.421053 296.421053 0 1 0-592.842106 0Z" fill="#D6B196" ></path><path d="M970.105263 512c0 224.983579-163.166316 412.186947-377.263158 450.533053v-54.460632C777.135158 870.507789 916.210526 707.206737 916.210526 512c0-222.881684-181.328842-404.210526-404.210526-404.210526S107.789474 289.118316 107.789474 512s181.328842 404.210526 404.210526 404.210526c9.081263 0 18.000842-0.754526 26.947368-1.374315v53.894736c-8.973474 0.538947-17.866105 1.374316-26.947368 1.374316-252.604632 0-458.105263-205.500632-458.105263-458.105263S259.395368 53.894737 512 53.894737s458.105263 205.500632 458.105263 458.105263z m-431.157895 50.202947c52.304842 70.925474 136.973474 152.144842 232.528843 190.383158l19.994947-50.041263c-109.271579-43.708632-202.805895-152.629895-238.780632-217.49221H808.421053v-53.894737H538.947368v-53.894737h215.578948v-53.894737h-215.578948V161.684211h-53.894736v161.68421h-215.578948v53.894737h215.578948v53.894737H215.578947v53.894737h255.757474c-35.974737 64.862316-129.536 173.783579-238.807579 217.49221l20.021895 50.041263c95.528421-38.238316 180.197053-119.484632 232.501895-190.383158V808.421053h53.894736v-246.218106z" fill="#231F20" ></path></symbol><symbol id="icon-dragon" viewBox="0 0 1024 1024"><path d="M366.376421 344.441263l152.980211-152.98021c43.142737-43.142737 141.204211-9.216 270.201263 115.738947-15.225263 9.835789-25.114947 15.818105-44.13979 32.256s-38.076632 35.489684-59.418947 56.832c-4.203789 4.203789-51.173053 53.221053-78.740211 82.027789-10.805895-12.126316-22.743579-24.171789-34.654315-36.082526L493.136842 362.792421l-54.218105 54.218105-72.542316-72.569263zM862.315789 512c0 46.834526-45.352421 80.842105-107.789473 80.842105-108.948211 0-189.359158-28.806737-267.129263-56.697263C414.100211 509.871158 344.872421 485.052632 258.182737 485.052632 80.788211 485.052632 0 588.126316 0 683.897263h53.894737C73.216 659.779368 135.302737 646.736842 177.340632 646.736842c77.338947 0 223.124211 23.282526 291.893894 47.912421C547.462737 722.701474 615.989895 754.526316 734.315789 754.526316 862.315789 754.526316 916.210526 670.315789 916.210526 512h-53.894737z" fill="#FF8787" ></path><path d="M552.421053 1024c-69.766737 0-113.825684-13.958737-156.402527-27.459368-54.487579-17.273263-110.807579-35.004632-232.421052-26.516211l-3.826527-53.733053c131.718737-9.458526 195.934316 10.967579 252.52379 28.887579 42.226526 13.365895 78.686316 24.926316 140.126316 24.926316 92.752842 0 148.210526-57.936842 148.210526-113.960421 0-16.949895-5.524211-101.618526-114.634105-101.618526-64.970105 0-112.747789 23.336421-163.328 48.02021C365.325474 830.571789 300.301474 862.315789 204.288 862.315789 85.908211 862.315789 0 787.294316 0 683.897263 0 588.126316 80.788211 485.052632 258.182737 485.052632c86.689684 0 155.917474 24.818526 229.214316 51.09221 45.810526 16.410947 92.564211 33.172211 145.488842 44.166737 9.000421-7.033263 13.850947-16.276211 13.850947-26.758737 0-37.187368-37.672421-74.859789-74.13221-111.265684l-3.287579-3.287579 38.103579-38.103579 3.260631 3.287579C652.853895 446.275368 700.631579 494.026105 700.631579 553.552842c0 12.719158-2.802526 24.926316-7.976421 36.109474A594.997895 594.997895 0 0 0 754.526316 592.842105c62.437053 0 107.789474-34.007579 107.789473-80.842105 0-58.853053-52.870737-110.268632-108.840421-164.702316l-8.057263-7.841684c-19.024842 16.437895-38.076632 35.489684-59.418947 56.832l-38.103579-38.103579c74.805895-74.832842 134.898526-134.898526 268.314947-141.931789V55.619368c-63.407158 7.787789-120.993684 39.424-121.667368 39.801264l-15.818105 8.811789-14.120421-11.344842C731.701895 66.452211 709.712842 53.894737 673.684211 53.894737c-41.418105 0-74.347789 25.869474-109.190737 53.301895-26.624 20.911158-54.137263 42.549895-86.851369 53.194105L469.342316 161.684211h-69.093053l-105.525895 105.525894-38.103579-38.130526L324.015158 161.684211H161.684211V107.789474h303.104c22.231579-8.272842 43.708632-25.168842 66.398315-42.981053C569.829053 34.438737 613.618526 0 673.684211 0c48.909474 0 81.408 17.946947 110.888421 40.097684C813.702737 26.300632 877.729684 0 943.157895 0h26.947368v323.368421h-53.894737v-53.167158c-54.164211 3.098947-92.914526 15.845053-127.002947 36.675369l1.832421 1.778526C852.587789 368.505263 916.210526 430.376421 916.210526 512c0 60.928-43.708632 109.945263-107.789473 127.622737V700.631579h53.894736v-53.894737h53.894737v53.894737h53.894737v53.894737h-53.894737v53.894737h-53.894737v-53.894737h-53.894736c-29.722947 0-53.894737-24.171789-53.894737-53.894737v-53.894737c-118.325895 0-207.063579-31.797895-285.318737-59.877053C400.437895 562.229895 335.494737 538.947368 258.182737 538.947368 117.059368 538.947368 53.894737 611.732211 53.894737 683.897263 53.894737 757.221053 115.738947 808.421053 204.288 808.421053c11.910737 0 23.228632-0.538947 34.034526-1.536C248.454737 796.321684 269.473684 770.640842 269.473684 739.166316c0-33.118316-43.088842-70.979368-58.152421-81.596632l30.935579-44.139789c8.299789 5.793684 81.111579 58.664421 81.111579 125.736421 0 19.429053-4.527158 37.052632-10.994526 52.304842 30.773895-10.051368 58.314105-23.498105 86.662737-37.349053C452.877474 727.848421 508.577684 700.631579 585.997474 700.631579 702.410105 700.631579 754.526316 778.725053 754.526316 856.144842 754.526316 938.657684 678.912 1024 552.421053 1024z m-21.180632-623.104L493.136842 362.792421l137.889684-137.889684 38.103579 38.103579-137.889684 137.889684z m-126.760421-18.351158l-38.103579-38.103579 152.980211-152.98021 38.103579 38.103579-152.980211 152.98021z m282.004211-218.624c15.494737-9.754947 43.331368-31.447579 43.331368-31.447579-25.734737-27.809684-49.556211-33.333895-67.368421-29.07621-19.240421 4.608-37.753263 24.602947-37.753263 24.602947s42.253474 22.447158 61.790316 35.920842z" fill="#231F20" ></path></symbol><symbol id="icon-horse" viewBox="0 0 1024 1024"><path d="M776.003368 646.736842c16.599579-99.947789 43.439158-181.086316 83.213474-256.538947l6.817684-12.934737H269.473684c-36.756211 0-53.894737 54.945684-53.894737 92.05221 0 46.753684 6.656 77.527579 70.278737 176.074106l84.533895 128.269473L498.876632 646.736842h277.126736z" fill="#FFAF6E" ></path><path d="M1024 0v404.210526c0 33.333895 0 134.736842-92.079158 134.736842h-13.824l-78.362947-109.056c-22.743579 49.906526-40.340211 103.046737-53.490527 162.950737h115.092211C937.310316 592.842105 970.105263 625.637053 970.105263 661.638737c0 60.631579-69.389474 154.300632-77.312 164.75621l-43.008-32.471579C875.466105 759.861895 916.210526 693.813895 916.210526 661.638737c0-5.982316-8.919579-14.901895-14.901894-14.901895h-125.332211C761.128421 736.121263 754.526316 840.569263 754.526316 970.105263h-53.894737c0-283.971368 31.097263-453.605053 110.888421-605.049263l20.318316-38.534737 112.801684 156.995369c14.443789-4.419368 25.465263-20.938105 25.465263-79.306106V0h53.894737z m-161.684211 161.684211h53.894737V0h-53.894737v80.842105c-17.381053-14.955789-38.184421-26.947368-80.842105-26.947368h-134.736842v53.894737h134.736842c37.672421 0 80.842105 40.906105 80.842105 53.894737z m-107.789473 0h-215.578948v53.894736h161.684211l53.894737-53.894736zM300.894316 766.544842L400.680421 916.210526h64.754526l-95.043368-142.551579L498.876632 646.736842h167.855157a1212.631579 1212.631579 0 0 1 9.431579-53.894737h-199.383579l-175.885473 173.702737z m109.97221-184.400842l-37.861052-38.319158-132.419369 130.802526C173.729684 571.095579 161.684211 529.812211 161.684211 469.315368 161.684211 398.578526 199.464421 323.368421 269.473684 323.368421h323.368421l53.894737-53.894737H269.473684c-6.709895 0-13.258105 0.565895-19.698526 1.482105C234.927158 249.451789 204.638316 215.578947 160.633263 215.578947 65.967158 215.578947 0 349.291789 0 469.315368c0 70.170947 16.141474 136.650105 49.232842 202.671158L6.197895 723.833263l41.472 34.41179 66.128842-79.737264-8.704-16.033684C83.105684 622.133895 53.894737 558.214737 53.894737 469.315368 53.894737 368.451368 106.765474 269.473684 160.633263 269.473684c13.231158 0 25.815579 9.889684 35.43579 20.533895C142.874947 321.967158 107.789474 388.500211 107.789474 469.315368c0 78.201263 19.698526 130.937263 93.642105 243.981474l-55.296 54.622316L280.899368 970.105263h64.754527l-130.048-195.072 195.260631-192.889263z" fill="#231F20" ></path></symbol><symbol id="icon-monkey_shen" viewBox="0 0 1024 1024"><path d="M512 512m-296.421053 0a296.421053 296.421053 0 1 0 592.842106 0 296.421053 296.421053 0 1 0-592.842106 0Z" fill="#BBC4C9" ></path><path d="M970.105263 512c0 224.983579-163.166316 412.186947-377.263158 450.533053v-54.460632C777.135158 870.507789 916.210526 707.206737 916.210526 512c0-222.881684-181.328842-404.210526-404.210526-404.210526S107.789474 289.118316 107.789474 512s181.328842 404.210526 404.210526 404.210526c9.081263 0 18.000842-0.754526 26.947368-1.374315v53.894736c-8.973474 0.538947-17.866105 1.374316-26.947368 1.374316-252.604632 0-458.105263-205.500632-458.105263-458.105263S259.395368 53.894737 512 53.894737s458.105263 205.500632 458.105263 458.105263z m-431.157895 134.736842h161.684211v53.894737h53.894737V269.473684h-215.578948V161.684211h-53.894736v107.789473h-215.578948v431.157895h53.894737v-53.894737h161.684211v215.578947h53.894736v-215.578947z m0-161.68421h161.684211v107.789473h-161.684211v-107.789473z m-215.578947 0h161.684211v107.789473h-161.684211v-107.789473z m215.578947-161.684211h161.684211v107.789474h-161.684211v-107.789474z m-215.578947 0h161.684211v107.789474h-161.684211v-107.789474z" fill="#231F20" ></path></symbol><symbol id="icon-ox_chou" viewBox="0 0 1024 1024"><path d="M512 512m-296.421053 0a296.421053 296.421053 0 1 0 592.842106 0 296.421053 296.421053 0 1 0-592.842106 0Z" fill="#D6B196" ></path><path d="M970.105263 512c0 224.983579-163.166316 412.186947-377.263158 450.533053v-54.460632C777.135158 870.507789 916.210526 707.206737 916.210526 512c0-222.881684-181.328842-404.210526-404.210526-404.210526S107.789474 289.118316 107.789474 512s181.328842 404.210526 404.210526 404.210526c9.081263 0 18.000842-0.754526 26.947368-1.374315v53.894736c-8.973474 0.538947-17.866105 1.374316-26.947368 1.374316-252.604632 0-458.105263-205.500632-458.105263-458.105263S259.395368 53.894737 512 53.894737s458.105263 205.500632 458.105263 458.105263z m-161.68421 188.631579h-159.555369c13.985684-172.813474 43.115789-357.429895 70.817684-385.158737L700.631579 269.473684H323.368421v53.894737h107.169684c-1.940211 45.756632-8.192 103.962947-15.76421 161.684211H323.368421v53.894736h83.968c-9.862737 68.446316-20.264421 130.128842-25.734737 161.684211H215.578947v53.894737h592.842106v-53.894737z m-346.543158-161.684211h149.800421a3313.717895 3313.717895 0 0 0-16.842105 161.684211h-158.477474c6.036211-35.247158 16.114526-95.636211 25.519158-161.684211z m22.608842-215.578947h171.735579c-15.198316 41.121684-27.405474 100.594526-36.890948 161.684211h-150.123789c7.383579-57.505684 13.419789-115.361684 15.279158-161.684211z" fill="#231F20" ></path></symbol><symbol id="icon-monkey" viewBox="0 0 1024 1024"><path d="M757.733053 485.052632H565.894737a80.842105 80.842105 0 0 0-80.842105 80.842105v215.578947c0 40.96 43.546947 99.678316 77.446736 139.210105C596.426105 960.215579 603.055158 970.105263 603.055158 970.105263H754.526316s15.144421-18.674526 45.891368-58.071579S862.315789 809.984 862.315789 717.608421c0-89.573053-47.993263-166.346105-104.582736-232.555789z" fill="#C3D686" ></path><path d="M538.947368 1024h-53.894736c0-32.794947 25.869474-87.417263 77.446736-103.316211C528.599579 881.152 485.052632 822.433684 485.052632 781.473684c0-44.570947 36.271158-80.842105 80.842105-80.842105h80.842105v53.894737h-80.842105a26.947368 26.947368 0 0 0-26.947369 26.947368c0 19.725474 36.675368 77.473684 92.133053 134.736842h88.602947c20.210526-14.147368 88.737684-71.464421 88.737685-198.602105 0-108.382316-93.237895-202.967579-168.151579-278.986105-49.502316-50.202947-88.576-89.842526-98.735158-128.61979-11.749053-44.732632-21.584842-112.586105-26.327579-148.318315H377.263158c-45.136842 0-89.519158 8.434526-121.802105 53.894736H431.157895v53.894737c-97.28 0-107.789474 113.071158-107.789474 161.684211v53.894737h53.894737v161.68421h-53.894737v-107.789474h-26.947368c-170.253474 0-188.631579-94.234947-188.631579-134.736842 0-31.043368 35.220211-72.326737 55.727158-93.722947 2.694737-14.686316 5.847579-28.348632 9.431579-41.013895H161.684211V215.578947h31.528421C239.642947 120.993684 317.224421 107.789474 377.263158 107.789474h185.640421l2.802526 23.794526c0.134737 1.050947 12.719158 106.657684 27.944421 164.756211 6.494316 24.872421 44.624842 63.514947 84.965053 104.448C760.481684 483.813053 862.315789 587.129263 862.315789 717.608421c0 92.375579-31.124211 155.028211-61.898105 194.425263C904.919579 892.146526 970.105263 803.004632 970.105263 673.684211c0-91.405474-42.819368-154.381474-84.237474-215.255579C847.791158 402.458947 808.421053 344.576 808.421053 269.473684c0-119.349895 87.093895-161.684211 161.68421-161.68421v53.894737c-32.417684 0-107.789474 10.509474-107.789474 107.789473 0 58.502737 31.555368 104.933053 68.096 158.639158C974.282105 492.597895 1024 565.679158 1024 673.684211c0 177.286737-108.301474 296.421053-269.473684 296.421052h-161.684211c-37.672421 0-53.894737 40.906105-53.894737 53.894737zM229.214316 269.473684a384.808421 384.808421 0 0 0-14.012632 58.341053l-1.401263 8.488421-6.090105 6.117053c-22.878316 22.932211-44.813474 52.601263-46.026105 62.275368 0 56.805053 53.76 75.264 107.789473 79.386947V431.157895c0-58.691368 13.473684-119.619368 46.511158-161.684211h-86.770526zM323.368421 1024h-53.894737c0-32.794947 25.869474-87.417263 77.446737-103.316211C313.020632 881.152 269.473684 822.433684 269.473684 781.473684c0-44.570947 36.271158-80.842105 80.842105-80.842105h45.16379A188.847158 188.847158 0 0 1 565.894737 592.842105h134.736842v53.894737h-134.736842c-74.293895 0-134.736842 60.442947-134.736842 134.736842v26.516211l-53.894737 0.377263V781.473684c0-9.162105 0.646737-18.135579 1.913263-26.947368H350.315789c-14.848 0-26.947368 12.072421-26.947368 26.947368 0 19.725474 36.675368 77.473684 92.133053 134.736842H431.157895v53.894737h-53.894737c-37.672421 0-53.894737 40.906105-53.894737 53.894737z" fill="#231F20" ></path></symbol><symbol id="icon-horse_wu" viewBox="0 0 1024 1024"><path d="M512 512m-296.421053 0a296.421053 296.421053 0 1 0 592.842106 0 296.421053 296.421053 0 1 0-592.842106 0Z" fill="#FF8787" ></path><path d="M970.105263 512c0 224.983579-163.166316 412.186947-377.263158 450.533053v-54.460632C777.135158 870.507789 916.210526 707.206737 916.210526 512c0-222.881684-181.328842-404.210526-404.210526-404.210526S107.789474 289.118316 107.789474 512s181.328842 404.210526 404.210526 404.210526c9.081263 0 18.000842-0.754526 26.947368-1.374315v53.894736c-8.973474 0.538947-17.866105 1.374316-26.947368 1.374316-252.604632 0-458.105263-205.500632-458.105263-458.105263S259.395368 53.894737 512 53.894737s458.105263 205.500632 458.105263 458.105263z m-431.157895 26.947368h269.473685v-53.894736H538.947368v-161.684211h161.684211v-53.894737H411.001263c12.045474-33.28 20.156632-69.793684 20.156632-107.789473h-53.894737c0 121.963789-105.364211 233.391158-106.415158 234.496l38.858105 37.349052c2.883368-3.018105 43.816421-46.133895 77.392842-110.160842H485.052632v161.684211H215.578947v53.894736h269.473685v323.368421h53.894736V538.947368z" fill="#231F20" ></path></symbol><symbol id="icon-ox" viewBox="0 0 1025 1024"><path d="M540.294737 754.526316h215.578947c20.210526 0 35.112421 1.374316 53.894737 4.581052 91.863579 15.656421 145.354105 67.691789 161.684211 86.069895V916.210526h53.894736V635.580632l-7.895579-7.895579c-9.269895-9.269895-36.513684-49.232842-44.032-196.527158H540.294737a161.684211 161.684211 0 0 0-161.684211 161.68421v131.098948c43.304421 20.210526 97.28 30.585263 161.684211 30.585263z" fill="#FFAF6E" ></path><path d="M1025.347368 635.580632V916.210526h-53.894736v-71.033263c-16.330105-18.405053-69.820632-70.413474-161.684211-86.069895V916.210526h-53.894737v-161.68421h-107.789473v215.578947h-53.894737V700.631579h161.68421c100.998737 0 172.570947 38.669474 215.578948 71.868632v-115.738948c-33.684211-43.627789-51.712-137.458526-53.706106-279.498105H701.978947c-76.934737 0-127.218526-26.219789-175.804631-51.550316a1556.048842 1556.048842 0 0 0-26.839579-13.743158c-26.839579 26.004211-66.209684 44.921263-115.738948 55.511579 24.441263 22.986105 60.874105 52.116211 106.469053 72.838737l-22.312421 49.044211c-76.584421-34.816-129.589895-88.926316-150.824421-113.125053-10.644211 0.619789-21.477053 1.024-32.687158 1.024a473.734737 473.734737 0 0 1-123.365053-15.952842l-93.022315 186.314105 68.581052 53.86779C167.882105 579.557053 237.891368 538.947368 324.715789 538.947368v53.894737c-95.986526 0-170.361263 62.490947-171.088842 63.137684l-16.78821 14.282106-136.838737-107.358316 109.729684-219.809684C46.430316 314.448842 1.347368 267.371789 1.347368 199.868632 1.347368 89.815579 121.586526 53.894737 163.031579 53.894737v53.894737c-14.120421 0-107.789474 17.165474-107.789474 92.079158C55.242105 290.465684 192.188632 323.368421 284.240842 323.368421c67.907368 0 122.421895-12.988632 157.696-35.624421-42.711579-14.336-95.097263-23.120842-169.337263-18.324211l-3.503158-53.786947c95.878737-6.117053 160.148211 8.515368 211.429053 28.833684C484.244211 235.439158 486.4 225.818947 486.4 215.578947c0-48.855579-57.829053-76.288-58.394947-76.557473l22.393263-49.017263C454.063158 91.648 540.294737 131.826526 540.294737 215.578947c0 18.566737-3.422316 35.84-9.997474 51.631158 7.060211 3.584 13.985684 7.168 20.776421 10.698106C597.854316 302.322526 638.248421 323.368421 701.978947 323.368421h269.473685v26.947368c0 214.689684 35.220211 266.590316 45.999157 277.369264l7.895579 7.895579z m-729.384421 25.141894l-98.789052 118.541474 86.797473 137.835789 45.594948-28.725894-65.913263-104.690527 37.052631-44.43621C358.642526 785.192421 439.080421 808.421053 540.294737 808.421053v-53.894737c-99.893895 0-175.077053-24.549053-223.474526-72.946527l-20.857264-20.857263z" fill="#231F20" ></path></symbol><symbol id="icon-rabbit_mao" viewBox="0 0 1024 1024"><path d="M512 512m-296.421053 0a296.421053 296.421053 0 1 0 592.842106 0 296.421053 296.421053 0 1 0-592.842106 0Z" fill="#7DD47F" ></path><path d="M970.105263 512c0 224.983579-163.166316 412.186947-377.263158 450.533053v-54.460632C777.135158 870.507789 916.210526 707.206737 916.210526 512c0-222.881684-181.328842-404.210526-404.210526-404.210526S107.789474 289.118316 107.789474 512s181.328842 404.210526 404.210526 404.210526c9.081263 0 18.000842-0.754526 26.947368-1.374315v53.894736c-8.973474 0.538947-17.866105 1.374316-26.947368 1.374316-252.604632 0-458.105263-205.500632-458.105263-458.105263S259.395368 53.894737 512 53.894737s458.105263 205.500632 458.105263 458.105263z m-377.263158-188.631579h107.789474v323.368421c-20.48 0-39.936-11.264-40.016842-11.317895l-27.728842 46.214737c3.206737 1.940211 32.660211 18.997895 67.745684 18.997895 30.746947 0 53.894737-23.147789 53.894737-53.894737V269.473684h-215.578948v538.947369h53.894737V323.368421z m-107.789473 242.526316v-242.526316h-53.894737v196.904421l-107.789474 40.421053v-243.927579l169.094737-48.316632-14.821053-51.819789L269.473684 276.102737v304.801684l-36.405895 13.662316 18.917053 50.472421 178.741895-67.018105c-5.039158 69.928421-55.269053 106.981053-165.133474 122.933894l7.733895 53.328842C325.712842 746.657684 485.052632 723.536842 485.052632 565.894737z" fill="#231F20" ></path></symbol><symbol id="icon-rabbit" viewBox="0 0 1024 1024"><path d="M680.96 488.744421a1666.667789 1666.667789 0 0 0-54.433684-23.95621c-16.006737 12.234105-33.899789 20.264421-60.631579 20.264421h-80.842105c-36.810105 0-83.644632 30.396632-104.394106 67.772631-42.819368 77.123368-53.409684 117.813895-11.021473 201.701053C397.096421 808.879158 431.157895 876.409263 431.157895 970.105263h338.539789l68.338527-138.859789c20.129684-40.96 24.252632-73.701053 24.252631-110.349474 0.026947-57.397895-25.061053-159.717053-181.328842-232.151579z" fill="#FFBDD8" ></path><path d="M862.315789 720.896c0 36.621474-4.122947 69.389474-24.252631 110.349474L769.697684 970.105263H485.052632v-53.894737h48.370526C507.877053 880.074105 485.052632 833.509053 485.052632 781.473684c0-59.418947 24.171789-113.313684 63.218526-152.360421l38.103579 38.103579A161.091368 161.091368 0 0 0 538.947368 781.473684c0 54.784 35.381895 104.043789 63.514948 134.736842h133.712842l53.490526-108.759579c15.710316-31.851789 18.755368-55.834947 18.755369-86.554947 0-80.976842-63.434105-150.096842-178.607158-195.503158-17.542737 8.138105-38.292211 13.554526-63.919158 13.554526h-80.842105c-13.958737 0-43.924211 15.979789-57.290106 40.016843l-47.104-26.165895C401.408 515.449263 448.242526 485.052632 485.052632 485.052632h80.842105c37.268211 0 57.478737-15.440842 79.090526-36.45979C625.367579 336.195368 549.753263 269.473684 485.052632 269.473684h-107.789474a21.288421 21.288421 0 0 0-5.955369 2.021053A683.762526 683.762526 0 0 0 302.187789 194.021053c-35.84-34.223158-61.763368-58.933895-94.908631-79.440842A42.442105 42.442105 0 0 0 185.478737 107.789474a22.824421 22.824421 0 0 0-17.381053 7.194947c-10.913684 11.425684-6.063158 28.240842 1.428211 39.181474 21.989053 32.121263 47.912421 56.858947 83.752421 91.109052 20.614737 19.671579 49.259789 43.169684 77.392842 63.08379C281.007158 367.400421 215.578947 484.432842 215.578947 592.842105c0 74.482526 24.791579 124.065684 51.065264 176.586106C294.534737 825.209263 323.368421 882.903579 323.368421 970.105263h-53.894737c0-74.482526-24.791579-124.065684-51.065263-176.586105C190.517895 737.738105 161.684211 680.043789 161.684211 592.842105c0-90.866526 42.226526-197.685895 93.453473-274.485894a803.759158 803.759158 0 0 1-39.046737-34.115369C177.852632 247.754105 150.231579 221.399579 125.035789 184.616421c-24.441263-35.759158-22.797474-78.686316 4.069053-106.819368 26.300632-27.567158 70.898526-31.043368 106.522947-9.000421 37.941895 23.444211 65.562947 49.798737 103.774316 86.258526 9.970526 9.512421 33.037474 32.309895 56.93979 60.550737h68.634947c-27.621053-37.780211-60.416-72.730947-88.522105-99.543579-28.833684-27.540211-54.730105-52.116211-84.533895-74.024421L326.305684 0.296421c31.232 23.228632 57.802105 48.532211 87.309474 76.719158 53.840842 51.388632 94.450526 100.594526 121.74821 146.83621 82.836211 26.650947 150.042947 116.870737 165.025685 230.750316l1.724631 13.177263-9.404631 9.404632c-3.772632 3.772632-7.706947 7.653053-11.802948 11.587368C837.227789 561.178947 862.315789 663.498105 862.315789 720.896zM309.463579 754.526316c3.934316 8.057263 7.895579 16.087579 11.991579 24.144842C348.887579 832.970105 377.263158 889.128421 377.263158 970.105263h53.894737c0-93.696-34.061474-161.226105-61.520842-215.578947h-60.173474z m597.90821 53.894737c-3.422316 9.404632-7.814737 19.806316-13.770105 31.959579L829.790316 970.105263h60.065684l52.143158-105.957052c10.778947-21.935158 17.515789-40.016842 21.90821-55.727158h-56.535579zM514.694737 390.736842c0-34.223158-13.231158-44.463158-29.642105-44.463158s-29.642105 10.24-29.642106 44.463158c0 34.250105 13.231158 44.463158 29.642106 44.463158s29.642105-10.213053 29.642105-44.463158z" fill="#231F20" ></path></symbol><symbol id="icon-rat_zi" viewBox="0 0 1024 1024"><path d="M512 512m-296.421053 0a296.421053 296.421053 0 1 0 592.842106 0 296.421053 296.421053 0 1 0-592.842106 0Z" fill="#85C3DE" ></path><path d="M970.105263 512c0 224.983579-163.166316 412.186947-377.263158 450.533053v-54.460632C777.135158 870.507789 916.210526 707.206737 916.210526 512c0-222.881684-181.328842-404.210526-404.210526-404.210526S107.789474 289.118316 107.789474 512s181.328842 404.210526 404.210526 404.210526c9.081263 0 18.000842-0.754526 26.947368-1.374315v53.894736c-8.973474 0.538947-17.866105 1.374316-26.947368 1.374316-252.604632 0-458.105263-205.500632-458.105263-458.105263S259.395368 53.894737 512 53.894737s458.105263 205.500632 458.105263 458.105263z m-431.157895 188.631579v-215.578947h269.473685v-53.894737H538.947368v-39.585684c26.543158-18.081684 94.585263-65.050947 177.852632-127.488L700.631579 215.578947H323.368421v53.894737h295.316211a4221.008842 4221.008842 0 0 1-121.640421 85.369263l-11.991579 8.003369V431.157895H242.526316v53.894737h242.526316v215.578947c0 48.343579-13.850947 53.894737-134.736843 53.894737v53.894737c105.391158 0 188.631579 0 188.631579-107.789474z" fill="#231F20" ></path></symbol><symbol id="icon-rat" viewBox="0 0 1024 1024"><path d="M727.659789 431.157895c-132.581053 0-220.348632 47.454316-285.803789 154.354526-19.779368 32.309895-15.845053 76.503579-9.404632 96.579368 3.260632 10.159158 7.760842 18.647579 12.422737 25.546106C464.761263 737.010526 499.927579 754.526316 538.947368 754.526316h66.829474c1.158737 17.893053-1.967158 34.762105-15.144421 53.975579-12.692211 18.539789-37.807158 40.151579-56.32 54.810947 25.249684-0.673684 52.709053-0.997053 83.240421-0.997053C877.487158 862.315789 970.105263 711.922526 970.105263 571.176421 936.421053 512 882.364632 431.157895 727.659789 431.157895z" fill="#85C3DE" ></path><path d="M210.432 1012.897684l-43.573895-31.690105c106.954105-147.051789 185.317053-171.196632 423.828211-172.705684 21.396211-31.258947 16.249263-56.266105 9.377684-89.70779-3.557053-17.138526-7.221895-34.842947-7.221895-54.433684 0-68.958316 25.330526-104.636632 63.407158-136.973474l34.896842 41.040842c-29.453474 25.061053-44.409263 46.780632-44.409263 95.932632 0 14.093474 2.937263 28.402526 6.063158 43.546947 5.901474 28.510316 12.8 62.032842-1.131789 99.462737 166.373053-10.24 264.542316-96.902737 264.542315-236.193684C916.210526 418.330947 827.580632 323.368421 684.921263 323.368421c-83.644632 0-153.303579 29.696-174.187789 39.612632a224.875789 224.875789 0 0 1-20.533895 31.339789l-41.741474-34.115368 20.884211 17.057684-20.911158-16.976842C448.781474 359.828211 485.052632 314.287158 485.052632 262.736842c0-34.816-8.946526-60.766316-26.570106-77.069474-17.515789-16.249263-44.786526-24.602947-81.219368-24.953263V323.368421h-53.894737V109.783579l24.872421-1.913263c64.700632-4.931368 114.095158 7.895579 146.863158 38.238316C524.207158 173.056 538.947368 212.291368 538.947368 262.736842c0 11.102316-1.131789 21.908211-3.072 32.202105 37.268211-12.584421 89.842526-25.465263 149.045895-25.465263C858.165895 269.473684 970.105263 387.907368 970.105263 571.176421 970.105263 711.922526 877.487158 862.315789 617.552842 862.315789c-258.667789 0-311.942737 19.698526-407.120842 150.581895z m19.105684-256.835368c-12.045474 0-24.387368-0.565895-37.025684-1.64379l-22.096842-1.859368-2.425263-22.016C167.747368 728.144842 161.684211 672.444632 161.684211 631.026526c0-103.585684 21.450105-178.903579 53.894736-259.045052V107.789474h53.894737v274.782315l-2.021052 4.904422C235.439158 465.758316 215.578947 533.800421 215.578947 631.026526c0 22.878316 2.101895 51.442526 3.826527 70.979369 99.678316 2.802526 172.813474-35.408842 222.450526-116.493474l48.020211 24.090947c-11.237053 28.133053-11.371789 51.577263-0.377264 67.853474 9.701053 14.282105 28.645053 23.174737 49.448421 23.174737v53.894737c-39.019789 0-74.186105-17.515789-94.073263-46.888421a100.244211 100.244211 0 0 1-12.422737-25.546106c-53.221053 49.178947-121.128421 73.943579-202.913684 73.970527zM379.957895 525.473684c0-34.223158-13.231158-44.463158-29.642106-44.463158s-29.642105 10.24-29.642105 44.463158c0 34.250105 13.231158 44.463158 29.642105 44.463158s29.642105-10.213053 29.642106-44.463158z" fill="#231F20" ></path></symbol><symbol id="icon-rooster_you" viewBox="0 0 1024 1024"><path d="M512 512m-296.421053 0a296.421053 296.421053 0 1 0 592.842106 0 296.421053 296.421053 0 1 0-592.842106 0Z" fill="#BBC4C9" ></path><path d="M970.105263 512c0 224.983579-163.166316 412.186947-377.263158 450.533053v-54.460632C777.135158 870.507789 916.210526 707.206737 916.210526 512c0-222.881684-181.328842-404.210526-404.210526-404.210526S107.789474 289.118316 107.789474 512s181.328842 404.210526 404.210526 404.210526c9.081263 0 18.000842-0.754526 26.947368-1.374315v53.894736c-8.973474 0.538947-17.866105 1.374316-26.947368 1.374316-252.604632 0-458.105263-205.500632-458.105263-458.105263S259.395368 53.894737 512 53.894737s458.105263 205.500632 458.105263 458.105263z m-215.578947-188.631579h-161.684211v-26.947368h161.684211V242.526316H269.473684v53.894737h161.684211v26.947368h-161.684211v485.052632h53.894737v-53.894737h377.263158v53.894737h53.894737V323.368421zM323.368421 646.736842h377.263158v53.894737H323.368421v-53.894737z m0-269.473684h107.789474c0 103.316211-72.784842 107.654737-81.084632 107.789474L350.315789 538.947368c46.592 0 134.736842-33.792 134.736843-161.68421h53.894736v107.789474c0 29.722947 24.171789 53.894737 53.894737 53.894736h107.789474v53.894737H323.368421v-215.578947z m377.263158 0v107.789474h-107.789474v-107.789474h107.789474z m-215.578947-80.842105h53.894736v26.947368h-53.894736v-26.947368z" fill="#231F20" ></path></symbol><symbol id="icon-rooster" viewBox="0 0 1024 1024"><path d="M891.688421 506.421895C877.244632 455.033263 862.315789 401.893053 862.315789 323.368421V116.224l-323.368421 195.745684V323.368421c0 78.524632 14.928842 131.664842 29.372632 183.053474 12.611368 44.894316 24.522105 87.282526 24.522105 140.314947 0 101.618526-77.931789 176.693895-168.286316 203.991579l5.416422 11.587368h215.578947c24.333474 0 43.385263-0.242526 58.556631-2.128842C811.52 846.821053 916.210526 764.550737 916.210526 646.736842c0-53.032421-11.910737-95.420632-24.522105-140.314947z" fill="#FF8787" ></path><path d="M673.684211 354.357895c-16.384 0-29.642105-10.213053-29.642106-44.463158 0-34.223158 13.231158-44.463158 29.642106-44.463158s29.642105 10.24 29.642105 44.463158c0 34.250105-13.258105 44.463158-29.642105 44.463158zM540.106105 970.105263l-50.58021-107.789474h156.05221l50.607158 107.789474h59.553684l-51.60421-109.918316C811.52 846.821053 916.210526 764.550737 916.210526 646.736842c0-53.032421-11.910737-95.420632-24.522105-140.314947C877.244632 455.033263 862.315789 401.893053 862.315789 323.368421V107.789474c0-59.445895-48.343579-107.789474-107.789473-107.789474a107.924211 107.924211 0 0 0-107.789474 106.172632 100.890947 100.890947 0 0 0-24.117895-3.314527 88.710737 88.710737 0 0 0-88.602947 88.602948c0 20.668632 5.227789 39.720421 10.671158 53.921684l-99.489684 59.688421 93.749894 14.470737V377.263158c0 14.416842-5.901474 21.692632-33.360842 49.152l-11.129263 11.129263C398.228211 326.521263 324.985263 269.473684 215.740632 269.473684 96.768 269.473684 0 366.241684 0 485.214316V646.736842h53.894737v-161.522526A162.007579 162.007579 0 0 1 215.740632 323.368421c82.081684 0 140.422737 36.244211 240.64 152.252632l-38.615579 38.615579C367.804632 461.285053 323.098947 431.157895 259.584 431.157895A151.983158 151.983158 0 0 0 107.789474 582.952421V754.526316h53.894737v-171.573895A98.007579 98.007579 0 0 1 259.584 485.052632c46.322526 0 79.629474 20.911158 137.027368 86.016l18.970948 21.530947 128.080842-128.080842C572.200421 435.981474 592.842105 415.366737 592.842105 377.263158v-97.926737l23.309474-14.120421-13.662316-23.04c-0.161684-0.242526-14.578526-24.899368-14.578526-50.688 0-19.132632 15.575579-34.708211 34.70821-34.708211 5.093053 0 26.785684 3.179789 39.558737 18.647579l26.327579 46.026106 39.774316-24.090948-20.372211-49.367579C704.754526 140.449684 700.631579 117.517474 700.631579 107.789474c0-29.722947 24.171789-53.894737 53.894737-53.894737s53.894737 24.171789 53.894737 53.894737v215.578947c0 85.935158 16.680421 145.300211 31.366736 197.632C851.887158 564.008421 862.315789 601.141895 862.315789 646.736842c0 95.285895-99.408842 161.684211-188.631578 161.684211h-209.461895l-68.419369-145.704421C375.242105 618.954105 338.108632 592.842105 296.448 592.842105A80.976842 80.976842 0 0 0 215.578947 673.711158V862.315789h53.894737v-188.604631c0-14.874947 12.099368-26.974316 26.974316-26.974316 20.533895 0 38.965895 14.147368 50.553263 38.858105L480.579368 970.105263h59.526737z" fill="#231F20" ></path></symbol><symbol id="icon-snake_si" viewBox="0 0 1024 1024"><path d="M512 512m-296.421053 0a296.421053 296.421053 0 1 0 592.842106 0 296.421053 296.421053 0 1 0-592.842106 0Z" fill="#FF8787" ></path><path d="M970.105263 512c0 224.983579-163.166316 412.186947-377.263158 450.533053v-54.460632C777.135158 870.507789 916.210526 707.206737 916.210526 512c0-222.881684-181.328842-404.210526-404.210526-404.210526S107.789474 289.118316 107.789474 512s181.328842 404.210526 404.210526 404.210526c9.081263 0 18.000842-0.754526 26.947368-1.374315v53.894736c-8.973474 0.538947-17.866105 1.374316-26.947368 1.374316-252.604632 0-458.105263-205.500632-458.105263-458.105263S259.395368 53.894737 512 53.894737s458.105263 205.500632 458.105263 458.105263z m-242.041263 180.762947l-52.116211-13.797052C657.219368 749.864421 651.425684 754.526316 619.789474 754.526316h-242.526316V485.052632h269.473684v53.894736h53.894737V215.578947H323.368421v538.947369c0 29.722947 24.171789 53.894737 53.894737 53.894737h242.526316c77.689263 0 91.189895-51.065263 108.274526-115.658106zM377.263158 269.473684h269.473684v161.684211H377.263158v-161.684211z" fill="#231F20" ></path></symbol><symbol id="icon-tiger_yin" viewBox="0 0 1024 1024"><path d="M512 512m-296.421053 0a296.421053 296.421053 0 1 0 592.842106 0 296.421053 296.421053 0 1 0-592.842106 0Z" fill="#7DD47F" ></path><path d="M970.105263 512c0 224.983579-163.166316 412.186947-377.263158 450.533053v-54.460632C777.135158 870.507789 916.210526 707.206737 916.210526 512c0-222.881684-181.328842-404.210526-404.210526-404.210526S107.789474 289.118316 107.789474 512s181.328842 404.210526 404.210526 404.210526c9.081263 0 18.000842-0.754526 26.947368-1.374315v53.894736c-8.973474 0.538947-17.866105 1.374316-26.947368 1.374316-252.604632 0-458.105263-205.500632-458.105263-458.105263S259.395368 53.894737 512 53.894737s458.105263 205.500632 458.105263 458.105263z m-257.42821 299.250526l-107.789474-53.894737-24.117895 48.208843 107.789474 53.894736 24.117895-48.208842z m-269.473685-5.658947l-24.117894-48.208842-107.789474 53.894737 24.117895 48.208842 107.789473-53.894737zM700.631579 431.157895h-161.684211v-53.894737h107.789474v-53.894737H377.263158v53.894737h107.789474v53.894737h-161.684211v323.368421h53.894737v-53.894737h269.473684v53.894737h53.894737V431.157895z m-161.684211 161.68421h107.789474v53.894737h-107.789474v-53.894737z m-161.68421 0h107.789474v53.894737h-107.789474v-53.894737z m161.68421-107.789473h107.789474v53.894736h-107.789474v-53.894736z m-161.68421 0h107.789474v53.894736h-107.789474v-53.894736zM754.526316 215.578947h-223.097263l-20.803369-62.410105-51.119158 17.057684L474.624 215.578947H269.473684v107.789474h53.894737v-53.894737h377.263158v53.894737h53.894737V215.578947z" fill="#231F20" ></path></symbol><symbol id="icon-snake" viewBox="0 0 1024 1024"><path d="M107.789474 790.474105c0-72.434526 67.880421-91.513263 121.451789-91.513263 74.401684 0 153.815579 34.438737 237.891369 70.925474 50.580211 21.935158 104.609684 45.325474 162.250105 63.083789-52.412632 44.786526-118.784 74.347789-195.152842 83.078737-143.171368 16.357053-326.440421 7.006316-326.440421-125.574737zM377.263158 215.578947c-15.575579 0-30.288842 3.449263-43.654737 9.377685A250.691368 250.691368 0 0 0 323.368421 296.421053c0 115.550316 76.422737 169.391158 137.83579 212.614736 8.138105 5.712842 16.141474 11.371789 23.848421 17.057685V323.368421a107.789474 107.789474 0 0 0-107.789474-107.789474z" fill="#C3D686" ></path><path d="M671.528421 788.857263c44.328421 11.964632 89.626947 19.563789 136.892632 19.56379 89.168842 0 161.684211-60.442947 161.68421-134.736842s-72.515368-134.736842-161.68421-134.736843c-19.078737 0-37.025684 1.509053-54.218106 4.015158-0.754526-101.402947-38.211368-172.355368-79.413894-219.648L673.684211 323.368421a1749.962105 1749.962105 0 0 1-79.036632-1.751579c45.702737 35.866947 108.705684 107.870316 105.984 232.367158 0 0.431158-0.080842 0.808421-0.10779 1.239579-34.923789 10.994526-66.155789 26.731789-95.097263 45.190737a163.085474 163.085474 0 0 0-15.845052-42.388211c-21.557895-39.639579-60.065684-66.775579-97.360842-93.022316C433.098105 423.343158 377.263158 384 377.263158 296.421053c0-130.290526 108.274526-188.631579 215.578947-188.631579 64.134737 0 132.715789 12.045474 214.366316 37.807158C802.330947 180.250947 780.099368 209.381053 700.631579 214.635789V161.684211h-53.894737v53.679157c-63.272421-1.024-104.528842-5.200842-104.986947-5.254736l-5.578106 53.598315C538.408421 263.949474 592.357053 269.473684 673.684211 269.473684c125.170526 0 188.631579-48.128 188.631578-143.063579V106.981053l-18.432-6.144C747.789474 68.823579 668.025263 53.894737 592.842105 53.894737c-158.666105 0-269.473684 99.732211-269.473684 242.526316 0 115.550316 76.422737 169.391158 137.83579 212.614736 33.684211 23.713684 65.509053 46.106947 81.003789 74.698106 9.539368 17.542737 13.285053 33.414737 12.341895 47.750737 21.153684 9.108211 42.118737 17.839158 62.949052 25.977263C671.151158 620.193684 729.977263 592.842105 808.421053 592.842105c59.445895 0 107.789474 36.271158 107.789473 80.842106s-48.343579 80.842105-107.789473 80.842105c-105.472 0-203.237053-42.388211-297.768421-83.429053-94.800842-41.094737-184.346947-79.952842-281.411369-79.952842C122.718316 591.171368 53.894737 644.715789 53.894737 727.578947c0 79.063579 67.098947 136.434526 159.555368 136.434527 142.174316 0 230.426947-66.883368 306.79579-129.886316 31.420632 13.419789 62.787368 26.058105 94.450526 37.133474-47.077053 49.637053-110.969263 82.566737-186.610526 91.270736l5.066105 53.625264c93.453474-7.006316 143.144421 9.350737 195.718737 26.543157 46.457263 15.225263 94.127158 30.854737 169.822316 30.854737 19.994947 0 41.957053-1.077895 66.344421-3.557052l-5.416421-53.625263c-105.283368 10.778947-158.100211-6.548211-213.935158-24.872422-22.150737-7.275789-44.624842-14.632421-70.305684-20.345263a334.848 334.848 0 0 0 96.14821-82.297263z m-458.078316 21.261474C162.573474 810.118737 107.789474 784.276211 107.789474 727.578947c0-60.847158 62.733474-82.539789 121.451789-82.539789 77.850947 0 154.731789 30.288842 235.250526 64.943158-66.263579 52.924632-139.722105 100.136421-251.041684 100.136421z" fill="#231F20" ></path></symbol><symbol id="icon-tiger" viewBox="0 0 1024 1024"><path d="M431.157895 162.250105V134.736842c0-41.552842-39.289263-80.842105-80.842106-80.842105-28.833684 0-57.128421 4.661895-58.314105 4.850526L269.473684 62.490947v83.887158C144.788211 223.824842 89.222737 346.839579 66.991158 431.157895h266.051368c240.747789 0 415.851789 107.789474 415.85179 269.473684-14.848-25.114947-43.924211-53.894737-88.68379-53.894737-67.988211 0-121.263158 71.033263-121.263158 161.684211 0 66.802526 30.477474 119.888842 60.712421 156.16 12.638316 15.171368 36.055579 37.726316 59.014737 58.88 5.066105 0.107789 9.781895 0.538947 15.009685 0.538947 219.297684 0 350.315789-191.811368 350.315789-377.263158C1024 327.545263 679.855158 172.813474 431.157895 162.250105z" fill="#F7C768" ></path><path d="M673.684211 1024c-114.768842 0-188.820211-33.333895-254.167579-62.787368-53.625263-24.144842-99.974737-45.002105-161.28-45.002106-40.448 0-83.590737 23.255579-103.639579 45.16379l-39.747369-36.432842C142.497684 894.787368 199.168 862.315789 258.236632 862.315789c68.392421 0 119.861895 21.288421 172.921263 45.056V673.684211c0-35.166316-17.542737-64.107789-30.639158-80.815158-15.198316 9.835789-32.067368 18.890105-50.741895 26.947368l-21.342316-49.475368C469.800421 509.413053 485.052632 377.317053 485.052632 323.368421V221.642105A597.827368 597.827368 0 0 0 404.210526 215.578947h-26.947368V134.736842c0-12.099368-14.848-26.947368-26.947369-26.947368-9.377684 0-18.836211 0.592842-26.947368 1.347368V269.473684h-53.894737V211.671579c-136.030316 102.912-158.450526 266.886737-161.306947 295.882105 9.135158 9.108211 38.992842 25.061053 71.976421 38.669474l38.103579-59.365053 12.449684-1.589894C321.212632 473.653895 377.263158 392.192 377.263158 323.368421h53.894737c0 88.333474-68.796632 192.242526-180.870737 213.342316l-48.397474 75.398737-20.291368-7.437474C53.894737 557.756632 53.894737 523.317895 53.894737 512c0-50.041263 37.025684-254.733474 215.578947-365.621895V62.490947l22.528-3.745684C293.187368 58.556632 321.482105 53.894737 350.315789 53.894737c41.552842 0 80.842105 39.289263 80.842106 80.842105v27.513263c248.697263 10.563368 592.842105 165.295158 592.842105 484.486737 0 185.451789-131.018105 377.263158-350.315789 377.263158z m-13.473685-323.368421c-36.513684 0-67.368421 49.367579-67.368421 107.789474 0 85.746526 68.096 145.084632 89.465263 161.549473 91.540211-2.533053 164.378947-45.487158 213.827369-107.654737H700.631579v-53.894736h230.238316c8.919579-17.273263 16.357053-35.354947 22.285473-53.894737h-239.885473l-6.467369-17.650527C706.290526 735.582316 692.439579 700.631579 660.210526 700.631579zM485.052632 931.112421c33.926737 14.066526 70.521263 26.597053 114.607157 33.468632C569.424842 928.309895 538.947368 875.223579 538.947368 808.421053c0-90.650947 53.274947-161.684211 121.263158-161.684211 44.759579 0 73.835789 28.779789 88.68379 53.894737h217.007158c2.775579-17.866105 4.203789-35.920842 4.203789-53.894737 0-38.938947-5.658947-74.752-15.925895-107.627789l-126.706526 126.679579-38.103579-38.103579L932.001684 485.052632a367.939368 367.939368 0 0 0-57.775158-81.596632l-154.543158 154.543158-38.103579-38.103579 153.573053-153.573053a537.869474 537.869474 0 0 0-82.593684-56.751158l-140.665263 140.638316-38.103579-38.103579 128.134737-128.134737A794.731789 794.731789 0 0 0 538.947368 231.046737V323.368421c0 50.149053-11.102316 156.698947-95.932631 236.328421 18.378105 23.417263 42.037895 63.407158 42.037895 113.987369v257.42821zM215.578947 431.157895v-53.894737c39.774316 0 53.894737-29.022316 53.894737-53.894737h53.894737c0 53.571368-37.025684 107.789474-107.789474 107.789474z" fill="#231F20" ></path></symbol><symbol id="icon-boar" viewBox="0 0 1024 1024"><path d="M732.079158 377.263158c-107.789474 0-186.421895 31.393684-281.869474 126.841263L180.331789 773.982316C257.724632 807.909053 348.725895 808.421053 485.052632 808.421053h96.013473c55.834947-34.411789 133.551158-53.894737 227.354948-53.894737h121.344L970.105263 680.555789V572.631579c0-94.315789-130.236632-195.368421-238.026105-195.368421z" fill="#FFBDD8" ></path><path d="M808.421053 700.631579v53.894737c-196.446316 0-323.368421 84.641684-323.368421 215.578947h-53.894737c0-163.705263 148.075789-269.473684 377.263158-269.473684z m-323.368421 107.789474v-53.894737c-158.342737 0-245.598316 0-319.649685-49.367579L158.612211 700.631579H80.842105c-21.692632 0-26.624-14.821053-26.947368-26.947368v-82.620632c84.156632-11.183158 161.684211-74.913684 161.68421-186.853053V215.578947H161.684211v161.684211H134.736842c-66.964211 0-134.736842 37.025684-134.736842 107.789474h53.894737c0-42.630737 52.870737-53.894737 80.842105-53.894737h24.629895C147.132632 504.912842 85.153684 538.947368 26.947368 538.947368H0v134.736843c0 32.498526 21.530947 80.842105 80.842105 80.842105h61.682527c32.687158 20.506947 67.125895 33.145263 105.957052 41.013895A232.879158 232.879158 0 0 0 215.578947 916.210526h53.894737c0-41.930105 14.012632-80.303158 39.424-112.505263C358.885053 808.151579 415.959579 808.421053 485.052632 808.421053z m-72.946527-342.420211L323.368421 554.738526V431.157895h-53.894737v253.682526l180.736-180.736-38.103579-38.103579zM323.368421 161.684211h-53.894737v190.032842a769.536 769.536 0 0 1 53.894737-49.098106V161.684211z m323.368421-53.894737c-72.623158 0-146.809263 23.336421-215.578947 58.637473V107.789474h-53.894737v154.138947C458.832842 205.392842 555.331368 161.684211 646.736842 161.684211c148.587789 0 269.473684 120.885895 269.473684 269.473684v235.654737L809.579789 862.315789h61.359158L970.105263 680.555789V431.157895c0-178.310737-145.057684-323.368421-323.368421-323.368421z" fill="#231F20" ></path></symbol><symbol id="icon-boar_hai" viewBox="0 0 1024 1024"><path d="M512 512m-296.421053 0a296.421053 296.421053 0 1 0 592.842106 0 296.421053 296.421053 0 1 0-592.842106 0Z" fill="#85C3DE" ></path><path d="M309.975579 804.756211l-27.136-46.592c103.073684-60.011789 183.026526-132.473263 241.475368-219.24379H350.315789l-13.473684-50.283789c58.88-33.980632 99.435789-117.571368 118.703158-165.295158H242.526316v-53.894737h538.947368v53.894737h-268.18021c-12.395789 34.088421-42.469053 106.603789-90.435369 161.68421h134.009263a680.555789 680.555789 0 0 0 46.349474-107.708631l51.092211 17.057684c-58.421895 175.265684-171.034947 309.490526-344.333474 410.381474z m192.350316-2.937264L467.806316 760.454737c88.414316-73.728 154.516211-158.773895 202.105263-259.907369l48.801684 22.959158a797.372632 797.372632 0 0 1-82.351158 137.781895c32.741053 15.009684 83.456 44.867368 137.647158 101.591579l-38.938947 37.268211c-57.236211-59.877053-109.325474-85.557895-133.766737-95.178106a850.997895 850.997895 0 0 1-98.977684 96.848842z m48.613052-536.872421l-80.842105-53.894737 29.884632-44.840421 80.842105 53.894737-29.884632 44.840421zM512 53.894737C259.395368 53.894737 53.894737 259.395368 53.894737 512s205.500632 458.105263 458.105263 458.105263c9.081263 0 17.973895-0.835368 26.947368-1.374316v-53.894736c-8.946526 0.619789-17.866105 1.374316-26.947368 1.374315-222.881684 0-404.210526-181.328842-404.210526-404.210526S289.118316 107.789474 512 107.789474s404.210526 181.328842 404.210526 404.210526c0 195.206737-139.075368 358.507789-323.368421 396.045474v54.460631c214.096842-38.346105 377.263158-225.549474 377.263158-450.533052C970.105263 259.395368 764.604632 53.894737 512 53.894737z" fill="#231F20" ></path></symbol><symbol id="icon-bilibili1" viewBox="0 0 1129 1024"><path d="M234.909 9.656a80.468 80.468 0 0 1 68.398 0 167.374 167.374 0 0 1 41.843 30.578l160.937 140.82h115.07l160.936-140.82a168.983 168.983 0 0 1 41.843-30.578A80.468 80.468 0 0 1 930.96 76.445a80.468 80.468 0 0 1-17.703 53.914 449.818 449.818 0 0 1-35.406 32.187 232.553 232.553 0 0 1-22.531 18.508h100.585a170.593 170.593 0 0 1 118.289 53.109 171.397 171.397 0 0 1 53.914 118.288v462.693a325.897 325.897 0 0 1-4.024 70.007 178.64 178.64 0 0 1-80.468 112.656 173.007 173.007 0 0 1-92.539 25.75H212.377a341.186 341.186 0 0 1-72.421-4.024A177.835 177.835 0 0 1 28.91 939.065a172.202 172.202 0 0 1-27.36-92.539V388.662a360.498 360.498 0 0 1 0-66.789A177.03 177.03 0 0 1 162.487 178.64h105.414c-16.899-12.07-31.383-26.555-46.672-39.43a80.468 80.468 0 0 1-25.75-65.984 80.468 80.468 0 0 1 39.43-63.57M216.4 321.873a80.468 80.468 0 0 0-63.57 57.937 108.632 108.632 0 0 0 0 30.578v380.615a80.468 80.468 0 0 0 55.523 80.469 106.218 106.218 0 0 0 34.601 5.632h654.208a80.468 80.468 0 0 0 76.444-47.476 112.656 112.656 0 0 0 8.047-53.109v-354.06a135.187 135.187 0 0 0 0-38.625 80.468 80.468 0 0 0-52.304-54.719 129.554 129.554 0 0 0-49.89-7.242H254.22a268.764 268.764 0 0 0-37.82 0z m0 0" fill="#20B0E3" ></path><path d="M348.369 447.404a80.468 80.468 0 0 1 55.523 18.507 80.468 80.468 0 0 1 28.164 59.547v80.468a80.468 80.468 0 0 1-16.094 51.5 80.468 80.468 0 0 1-131.968-9.656 104.609 104.609 0 0 1-10.46-54.719v-80.468a80.468 80.468 0 0 1 70.007-67.593z m416.02 0a80.468 80.468 0 0 1 86.102 75.64v80.468a94.148 94.148 0 0 1-12.07 53.11 80.468 80.468 0 0 1-132.773 0 95.757 95.757 0 0 1-12.875-57.133V519.02a80.468 80.468 0 0 1 70.007-70.812z m0 0" fill="#20B0E3" ></path></symbol><symbol id="icon-yinle" viewBox="0 0 1024 1024"><path d="M512.2976 0a531.2 531.2 0 0 0-512 548.48V960h128V548.48a398.72 398.72 0 0 1 384-411.52 398.72 398.72 0 0 1 384 411.52V960h128V548.48A531.2 531.2 0 0 0 512.2976 0z" fill="#5c8add" ></path><path d="M64.2976 576l256 0 0 448-256 0 0-448Z" fill="#5c8add" ></path><path d="M704.2976 576l256 0 0 448-256 0 0-448Z" fill="#5c8add" ></path></symbol><symbol id="icon-icon-test-copy" viewBox="0 0 1024 1024"><path d="M512 512m-229.517241 0a229.517241 229.517241 0 1 0 459.034482 0 229.517241 229.517241 0 1 0-459.034482 0Z" fill="#5c8add" ></path><path d="M512 1024A512 512 0 1 1 1024 512 512 512 0 0 1 512 1024z m0-141.241379A370.758621 370.758621 0 1 0 141.241379 512 370.758621 370.758621 0 0 0 512 882.758621z" fill="#5c8add" ></path></symbol><symbol id="icon-V" viewBox="0 0 1024 1024"><path d="M1012.47774251 492.58192592L544.94137566 87.22962963a49.96686561 49.96686561 0 0 0-65.88275132 0L11.63784127 492.6975097c-21.03624691 18.26223633-23.3479224 49.93219048-5.08568606 70.96843739 18.03106878 21.03624691 49.93219048 23.3479224 70.96843738 5.08568607L512 191.83294532l434.71057495 376.91868784c9.47786949 8.20644797 21.26741446 12.25188008 32.82579189 12.13629629 14.10122046 0 27.97127337-5.77918871 38.02706173-17.33756613 18.14665256-20.92066314 15.95056084-52.70620106-5.08568606-70.9684374z" fill="#5c8add" ></path><path d="M109.30613051 567.59579541V896.89396825c0 42.53482892 34.90629982 77.44112875 77.44112875 77.44112875h220.76500882V666.30433862c0-25.54401411 20.92066314-46.46467725 46.46467724-46.46467724h116.16169313c25.54401411 0 46.46467725 20.92066314 46.46467725 46.46467724V974.335097h220.76500882c42.53482892 0 77.44112875-34.90629982 77.44112874-77.44112875l0.11558377-329.29817284L512 218.18604586 109.30613051 567.59579541zM848.00203175 197.49655027h-63.91782716c-12.82979894 0-23.23233862 10.40253968-23.23233863 23.23233862v24.27259259l110.49808818 95.70336508V220.72888889h-0.11558377c0-12.82979894-10.40253968-23.23233862-23.23233862-23.23233862zM905.44716754 83.18419754s-34.90629982 56.86721693-89.11508994 100.32671603c152.68616579 13.98563668 127.83565432-133.26809171 127.83565432-133.2680917-134.07717813-10.28695591-132.92134039 102.29164021-131.072 127.83565432 20.92066314-20.92066314 49.70102293-62.64640564 92.35143562-94.89427865zM798.53217637 174.61096297c-19.64924162-16.52847972-40.56990476-43.45949912-51.203612-53.97762258 0 0 32.94137566 20.57391182 56.40488184 49.3542716 2.42725926-18.37782011 6.47269135-93.3916896-93.16052205-85.3008254 0 0-13.98563668 104.71889947 87.95925221 89.92417638z" fill="#5c8add" ></path></symbol><symbol id="icon-zhifeiji" viewBox="0 0 1167 1024"><path d="M41.201759 463.52493L1110.665064 30.117647c10.32605-4.159104 21.942857 0.860504 26.101961 11.043137 1.434174 3.728852 1.864426 7.744538 1.003921 11.616807L949.033691 978.823529c-2.151261 10.89972-12.764146 17.927171-23.663865 15.632493-2.72493-0.573669-5.306443-1.721008-7.601121-3.298599L634.80624 789.79944l-163.065546 133.951821c-16.492997 13.62465-40.87395 11.186555-54.498599-5.306443-3.011765-3.728852-5.306443-7.887955-6.884034-12.477311l-102.973669-313.080112-265.178712-91.787115c-10.469468-3.585434-16.062745-15.058824-12.333893-25.528291 1.864426-5.44986 6.023529-9.895798 11.329972-12.047059z" fill="#FCFDFC" ></path><path d="M929.385512 1023.569748c-3.155182 0-6.453782-0.286835-9.752381-1.003922-6.740616-1.434174-12.907563-4.015686-18.50084-8.031372L635.953579 825.940616l-146.142297 120.040336c-13.911485 11.473389-31.408403 16.779832-49.335574 15.058824-17.927171-1.721008-34.133333-10.32605-45.463305-24.237535-5.306443-6.453782-9.322129-13.768067-11.903642-21.79944l-98.527731-299.598879-251.697479-87.19776c-12.333894-4.302521-22.229692-13.05098-27.966386-24.811204s-6.453782-24.954622-2.151261-37.288515c4.589356-13.337815 14.771989-23.9507 27.82297-29.257143L1099.908761 3.585434c24.954622-10.039216 53.351261 2.007843 63.533894 26.819048 3.585434 8.891877 4.445938 18.644258 2.581513 28.109804L977.143495 984.560224c-4.732773 23.090196-25.098039 39.009524-47.757983 39.009524z m-294.579272-233.770308l282.962465 201.357983c2.294678 1.577591 4.87619 2.72493 7.601121 3.298599 10.89972 2.151261 21.512605-4.87619 23.663865-15.632493L1137.914364 52.777591c0.860504-3.872269 0.430252-7.887955-1.003922-11.616807-4.159104-10.32605-15.919328-15.202241-26.101961-11.043137L41.201759 463.52493c-5.306443 2.151261-9.465546 6.597199-11.47339 12.047059-1.721008 5.019608-1.434174 10.469468 0.860505 15.345658 2.294678 4.87619 6.453782 8.461625 11.473389 10.182633l265.178711 91.787115L410.214644 905.967507c1.434174 4.589356 3.872269 8.748459 6.884033 12.477311 6.597199 8.031373 15.919328 12.907563 26.101961 13.911485 10.32605 1.003922 20.365266-2.007843 28.396639-8.605042l163.208963-133.951821z" fill="#4A4A4A" ></path><path d="M307.097557 592.743978l105.698599 316.091876c6.310364 18.787675 26.532213 28.970308 45.319888 22.659944 4.159104-1.434174 7.887955-3.442017 11.186555-6.166946l164.786555-133.951821-165.360224-118.892997c297.017367-287.982073 447.462185-433.980952 451.191036-437.853222 0.573669-0.573669 2.581513-3.442017 0.430252-7.027451-1.290756-1.577591-3.298599-3.298599-7.027451-2.15126-202.218487 120.327171-404.293557 242.805602-606.22521 367.291877z" fill="#CAE0EE" ></path><path d="M446.786072 934.794398c-5.736695 0-11.329972-1.290756-16.636414-3.872269-8.891877-4.445938-15.632493-12.047059-18.787675-21.512605L305.376549 592.313725l1.003921-0.573669C507.308201 467.684034 711.391114 344.058263 912.60568 224.161345l0.286835-0.143418c3.585434-1.147339 6.310364-0.286835 8.605042 2.581513l0.143417 0.143417c2.438095 4.015686 0.573669 7.457703-0.573669 8.74846-3.872269 4.015686-155.177591 150.87507-450.043698 436.705882l165.503642 119.036414-166.220728 135.09916c-3.442017 2.868347-7.457703 5.019608-11.760225 6.453782-3.728852 1.290756-7.744538 2.007843-11.760224 2.007843z m-137.967507-341.333334l105.268348 314.944538c2.868347 8.748459 9.035294 15.77591 17.210084 19.935014 8.17479 4.159104 17.496919 4.732773 26.245378 1.864426 3.872269-1.290756 7.60112-3.298599 10.756302-5.880112l163.352381-132.804482L466.434252 672.627451l1.290756-1.147339C763.308201 384.932213 915.043775 237.642577 918.772627 233.626891c0 0 2.007843-2.294678 0.286835-5.306443-1.003922-1.290756-2.438095-2.438095-5.306443-1.577591-200.784314 119.610084-404.293557 242.94902-604.934454 366.718207z" fill="#CAE0EE" ></path><path d="M460.840974 924.898599l7.457703-253.561904 165.933894 119.896918-168.658824 135.959664c-1.290756 1.003922-3.011765 0.860504-4.015686-0.430252-0.430252-0.430252-0.717087-1.147339-0.717087-1.864426z" fill="#94C3E2" ></path><path d="M463.709322 929.344538c-1.290756 0-2.438095-0.573669-3.2986-1.577591-0.573669-0.860504-1.003922-1.864426-1.003921-2.868348l7.60112-256.286834 169.519328 122.621848-1.434174 1.147339-168.658823 135.959664c-0.860504 0.717087-1.721008 1.003922-2.72493 1.003922z m6.023529-255.282913l-7.457703 250.836974c0 0.286835 0.143417 0.717087 0.286835 1.003922 0.430252 0.573669 1.434174 0.717087 2.007843 0.286835l167.22465-134.812325-162.061625-117.315406z" fill="#94C3E2" ></path></symbol><symbol id="icon-lianjie" viewBox="0 0 1079 1024"><path d="M695.355535 432.666896c-0.553495-1.10699-0.885592-2.186305-1.383737-3.265619-0.193723-0.193723-0.193723-0.359772-0.359771-0.719543-12.508983-26.318678-39.436506-43.366319-69.325226-41.013966-39.076734 3.265619-68.439634 39.021384-65.312388 79.841627 0.857917 10.516401 3.653066 20.147211 7.998 28.83708 19.78744 46.659613 11.097571 103.448181-25.377737 141.750022l-191.094085 199.950001a118.088119 118.088119 0 0 1-171.998513 0c-47.434506-49.537786-47.434506-130.098956 0-179.636742l71.234782-74.389703-0.52582-0.553494a75.911814 75.911814 0 0 0 24.326097-61.880721c-3.127246-40.820243-37.3609-71.51153-76.437634-68.24591a69.463599 69.463599 0 0 0-46.908685 23.966325l-0.166049-0.193723-72.618519 75.856464c-103.226783 107.793115-103.226783 282.36538 0 390.158495 103.171433 107.793115 270.299193 107.793115 373.498301 0l191.619904-200.1714c80.256748-83.992838 97.636485-208.307773 52.83108-310.289193z" fill="#5c8add" ></path><path d="M1002.047012 80.865592c-103.226783-107.82079-270.382217-107.82079-373.581325 0l-191.619905 200.199075c-80.284423 83.854464-97.66416 208.197074-52.997128 310.233843 0.52582 1.079315 0.857917 2.15863 1.383737 3.26562 0.166048 0.166048 0.166048 0.359772 0.332097 0.719543 12.536658 26.291004 39.46418 43.366319 69.3529 41.013966 39.076734-3.265619 68.439634-39.021384 65.312388-79.869302a78.679288 78.679288 0 0 0-7.998-28.864755c-19.78744-46.631938-11.097571-103.448181 25.377737-141.750022l191.287808-199.839302a118.088119 118.088119 0 0 1 172.026188 0c47.434506 49.537786 47.434506 130.126631 0 179.692091l-71.234782 74.417378 0.52582 0.553495a75.939489 75.939489 0 0 0-24.353772 61.88072c3.15492 40.847917 37.3609 71.51153 76.465309 68.245911a69.463599 69.463599 0 0 0 46.908685-23.938651l0.166049 0.166048 72.646194-75.856464c103.03306-107.82079 103.03306-282.642127 0-390.269194z" fill="#5c8add" ></path></symbol><symbol id="icon-liaotian" viewBox="0 0 1171 1024"><path d="M1068.71699 0.243751H102.193768C46.228437 0.243751 0.500666 45.045267 0.500666 99.74309v696.251622c0 54.697824 45.727771 99.450589 101.693102 99.450589h329.113198l120.851966 114.465677a48.652788 48.652788 0 0 0 66.641644 0l120.851966-114.465677h329.064448c55.965331 0 101.741852-44.752765 101.741852-99.450589V99.74309C1170.458842 45.045267 1124.682321 0.243751 1068.71699 0.243751z m-439.776354 596.849784h-370.989696c-27.933915 0-50.846551-22.425133-50.846551-49.774045 0-27.348912 22.912636-49.725294 50.846551-49.725294h370.989696c27.933915 0 50.846551 22.376382 50.846551 49.725294 0 27.348912-22.912636 49.774045-50.846551 49.774045z m287.18795-211.381252H254.782171a50.456549 50.456549 0 0 1-50.846551-49.725294c0-27.397662 22.912636-49.774045 50.846551-49.774045h661.346415c27.933915 0 50.846551 22.376382 50.846551 49.774045 0 27.348912-22.912636 49.725294-50.846551 49.725294z" fill="#5C8ADD" ></path></symbol><symbol id="icon-xinfeng" viewBox="0 0 1400 1024"><path d="M1301.63733163 214.78520234a207.81921797 207.81921797 0 0 1 7.02423018 52.42036465v489.73590176a205.10753818 205.10753818 0 0 1-205.05853125 205.05853125H283.05853124A205.15654424 205.15654424 0 0 1 77.99999999 756.79444971V267.20556699a201.36672685 201.36672685 0 0 1 7.02423106-52.42036465L586.24393329 562.1905874c69.44187217 51.96297217 146.36536612 49.13694404 214.1736961 0zM1103.60303056 62.0000167H283.05853124A204.50312753 204.50312753 0 0 0 106.37462518 163.41030547l489.71956641 335.75823018c62.43397646 50.77048623 127.85733457 50.31309463 194.62019765 0L1280.28693749 163.41030547A204.68281729 204.68281729 0 0 0 1103.60303056 62.0000167z m0 0" fill="#5c8add" ></path></symbol><symbol id="icon-QQ1" viewBox="0 0 1024 1024"><path d="M0 512a512 512 0 1 0 1024 0A512 512 0 1 0 0 512z" fill="#18ACFC" ></path><path d="M500.113 228.39c118.396-1.518 178.924 61.004 201 156 3.497 15.048 0.15 34.807 0 50 27.143 5.682 33.087 60.106 10 75v1h1c8.26 14.33 19.04 28.125 26 44 7.332 16.723 9.306 35.16 14 55 4.024 17.01-2.287 51.505-10 57-0.771 0.683-2.231 1.312-3 2-14.601-3.016-30.377-16.865-38-27-3.065-4.074-5.275-9.672-10-12-0.395 21.568-12.503 41.15-22 55-3.514 5.123-14.073 13.217-14 18 3.691 2.836 8.305 2.956 13 5 10.513 4.577 25.449 13.168 32 22 2.334 3.146 5.548 7.555 7 11 16.193 38.414-36.527 48.314-63 54-27.185 5.839-77.818-10.224-92-19-8.749-5.414-16.863-18.573-29-19-3.666 2.389-14.438 1.132-20 1-16.829 32.804-101.913 47.868-148 31-14.061-5.146-43.398-17.695-38-40 4.437-18.327 19.947-29.224 35-37 5.759-2.975 18.915-4.419 22-10-13.141-8.988-24.521-28.659-31-44-3.412-8.077-4.193-25.775-9-32-7.789 12.245-32.097 36.91-52 33-3.071-4.553-7.213-9.097-9-15-4.792-15.835-1.81-40.379 2-54 8.117-29.02 16.965-50.623 32-72 4.672-6.643 11.425-12.135 16-19-8.945-9.733-6.951-37.536-1-49 4.002-7.709 9.701-7.413 10-20-1.92-3.022-0.071-8.604-1-13-4.383-20.75 3.273-47.552 9-63 19.8-53.421 53.712-90.466 105-112 11.986-5.033 25.833-7.783 39-11 5.322-1.3 11.969 0.518 16-2z" fill="#FFFFFF" ></path></symbol><symbol id="icon-rss" viewBox="0 0 1024 1024"><path d="M749.61196492 908.06119793C749.61196492 560.41848146 463.58151854 274.36328126 115.93880207 274.36328126V115.93880207c434.50388795 0 792.12239584 357.61850789 792.12239586 792.12239586zM224.55858562 690.72261555a108.91682943 108.91682943 0 0 1 108.69404499 108.74355267C333.25263061 859.29616292 284.24005737 908.06119793 224.31104736 908.06119793 164.48105265 908.06119793 115.96355592 859.41993206 115.96355592 799.46616822s48.69077351-108.71879883 108.61978351-108.74355267zM641.01693522 908.06119793h-153.96879069c0-203.60020956-167.50913289-371.13409627-371.10934246-371.13409629v-153.96879068c288.03550619 0 525.07813313 237.11688843 525.07813315 525.10288697z" fill="#FFA500" ></path></symbol><symbol id="icon-youxiang" viewBox="0 0 1024 1024"><path d="M583.60666667 972h-68.08c-8.43333333 0-15.33333333-6.9-15.33333334-15.33333333V609.52c0-8.43333333 6.9-15.33333333 15.33333334-15.33333333h68.08c8.43333333 0 15.33333333 6.9 15.33333333 15.33333333V956.66666667c0 8.43333333-6.9 15.33333333-15.33333333 15.33333333z" fill="#629FF9" ></path><path d="M294.42 167c-113.62 0-205.77333333 92-205.77333333 205.31333333v336.72h411.39333333V372.31333333c0.15333333-113.31333333-92-205.31333333-205.62-205.31333333z" fill="#2166CC" ></path><path d="M519.97333333 627H216.98666667c-25.45333333 0-46-20.54666667-46-46V393.78c0-25.45333333 20.54666667-46 46-46h302.98666666c25.45333333 0 46 20.54666667 46 46V581c0 25.45333333-20.54666667 46-46 46z" fill="#D2E4FF" ></path><path d="M565.97333333 397a49.22 49.22 0 0 0-49.37333333-49.22H220.36c-27.29333333 0-49.37333333 22.08-49.37333333 49.22v10.27333333l179.4 94.60666667c11.34666667 5.98 24.84 5.98 36.18666666 0l179.4-94.60666667v-10.27333333z" fill="#FFFFFF" ></path><path d="M730.5 167h-427.8v0.46c109.78666667 4.29333333 197.49333333 94.3 197.49333333 205.00666667v336.72h411.39333334c27.29333333 0 49.37333333-22.08 49.37333333-49.22V397c0-126.96-103.19333333-230-230.46-230z" fill="#4E8DF6" ></path><path d="M845.80666667 52H681.12666667c-9.04666667 0-16.40666667 7.36-16.40666667 16.40666667v336.72a24.67133333 24.67133333 0 1 0 49.37333333 0V134.18666667h131.71333334c9.04666667 0 16.40666667-7.36 16.40666666-16.40666667V68.40666667c0-9.04666667-7.36-16.40666667-16.40666666-16.40666667z" fill="#2166CC" ></path><path d="M896.25333333 659.81333333h-35.11333333c-8.43333333 0-15.33333333-6.9-15.33333333-15.33333333v-35.11333333c0-8.43333333 6.9-15.33333333 15.33333333-15.33333334h35.11333333c8.43333333 0 15.33333333 6.9 15.33333334 15.33333334v35.11333333c0 8.58666667-6.9 15.33333333-15.33333334 15.33333333z" fill="#FFFFFF" ></path><path d="M88.8 709.18666667l-24.22666667 131.40666666c-9.66 54.43333333 26.83333333 98.59333333 81.26666667 98.59333334h213.9c54.58666667 0 106.56666667-44.16 116.22666667-98.59333334l23.15333333-131.40666666H88.8z" fill="#2974CE" ></path></symbol><symbol id="icon-gitHub" viewBox="0 0 1049 1024"><path d="M523.6581816 52C262.83923907 52 52 262.8401375 52 523.6581816c0 208.49703047 135.09433812 384.97758117 322.50789391 447.44906532 23.42658172 4.68531653 32.01647887-10.15136894 32.01647796-22.64584583 0-10.93210574-0.78163433-48.41463703-0.78163433-87.45953855-131.18885996 28.11189824-158.5200223-56.22379738-158.52002231-56.22379739-21.08437312-54.66232469-52.3201152-68.71827336-52.3201152-68.71827335-42.94858371-28.89353348 3.12384382-28.89353348 3.12384384-28.89353348 47.63479867 3.12384382 72.62285398 48.41643391 72.62285398 48.4164339 42.16784782 71.84121875 110.10538527 51.53758242 137.43654672 39.04400399 3.90457972-30.45500618 16.3990566-51.5393793 29.67427028-63.25222094-104.64023039-10.93300418-214.74561566-51.53848086-214.74561657-232.70524742 0-51.53848086 18.74126609-93.70632867 48.4164339-126.50444187-4.68621496-11.71284164-21.08527156-60.12837711 4.6844181-124.94207075 0 0 39.82563922-12.49447688 129.62738726 48.41463704 37.48253129-10.15136894 78.08980484-15.61742227 117.91454562-15.61742137s80.43201433 5.46605242 117.91454473 15.61742137c89.80264648-60.90911391 129.62828571-48.41463703 129.62828571-48.41463704 25.76879122 64.81369363 9.37063305 113.22922911 4.68531651 124.94207075 30.45410773 32.79721477 48.41463703 74.96506258 48.41463703 126.50444187 0 181.16676656-110.10538527 220.99150644-215.52545401 232.70524742 17.1797934 14.83668547 32.01647887 42.94858371 32.01647886 87.45953946 0 63.25222094-0.78163433 114.009965-0.78163523 129.62738636 0 12.49447688 8.59079468 27.33116234 32.01737731 22.64584583 187.41265734-62.4705866 322.50699547-238.95203574 322.50699546-447.44996375C995.31636231 262.8401375 783.69369203 52 523.6581816 52z" fill="#663399" ></path><path d="M230.82365863 729.03136735c-0.7807359 2.34310703-4.68531653 3.12384382-7.80916035 1.56237113s-5.46605242-4.68531653-3.90368129-7.02842356c0.7807359-2.34220859 4.68531653-3.12384382 7.80826192-1.56147269s4.68531653 4.68531653 3.90457972 7.02752512z m18.7412661 21.08437312c-2.34220859 2.34220859-7.02752512 0.78163433-9.37063305-2.34310703-3.12294539-3.12294539-3.90457972-7.80826192-1.5614727-10.15136894 2.34220859-2.34220859 6.24678922-0.7807359 9.37063305 2.34310702 3.12384382 3.90457972 3.90457972 8.58899782 1.5614727 10.15136895zM268.30618992 777.44690281c-3.12294539 2.34220859-7.80826192 0-10.15136895-3.90457972-3.12384382-3.90457972-3.12384382-9.37063305 0-10.93210574 3.12384382-2.34310703 7.80916035 0 10.15226739 3.90457972 3.12294539 3.90368129 3.12294539 8.58899782 0 10.93210574z m25.76968965 26.55042555c-2.34220859 3.12294539-7.80916035 2.34220859-12.49447688-1.56237113-3.90457972-3.90368129-5.46605242-9.37063305-2.34220859-11.71284164 2.34220859-3.12384382 7.80826192-2.34310703 12.49447687 1.56147269 3.90368129 3.12384382 4.68531653 8.58989625 2.3422086 11.71374008z m35.1403227 14.83668637c-0.78163433 3.90457972-6.24768766 5.46605242-11.71374008 3.90457972-5.46605242-1.5614727-8.58899782-6.24768766-7.80916036-9.37063305 0.78163433-3.90457972 6.24768766-5.46605242 11.71374009-3.90457972 5.46605242 1.5614727 8.58899782 5.46605242 7.80916035 9.37063305z m38.26416562 3.12384382c0 3.90457972-4.68621496 7.02752512-10.15226738 7.02752512-5.46605242 0-10.15226738-3.12294539-10.15226739-7.02752512s4.68621496-7.02842356 10.15226739-7.02842445c5.46605242 0 10.15226738 3.12384382 10.15226738 7.02842445z m35.92016106-6.24768766c0.78163433 3.90457972-3.12384382 7.80916035-8.58899872 8.58989625-5.46695086 0.78163433-10.15226738-1.5614727-10.93390172-5.46605241-0.77983747-3.90457972 3.12384382-7.80916035 8.5907947-8.58899872 5.46605242-0.78163433 10.15136894 1.56057426 10.93210574 5.46515488z m0 0" fill="#663399" ></path></symbol><symbol id="icon-bilibili" viewBox="0 0 1024 1024"><path d="M832.61667555 181.33447111h-164.32545185l74.45617778-74.45617778c12.84020148-12.84020148 12.84020148-30.8140563 0-43.65425778-12.84020148-12.84020148-30.8140563-12.84020148-43.65425778 0L573.2882963 189.04101925H450.04420741L324.2272237 63.23617185c-10.26730667-12.84020148-25.68040297-15.40096-41.08136295-7.70654815-2.57289482 0-2.57289482 2.57289482-5.13365334 5.13365333-12.84020148 12.84020148-12.84020148 30.8140563 0 43.65425779l77.02907259 77.02907259h-164.32545185c-89.86927408 0-164.32545185 74.45617778-164.32545185 164.32545184v408.24073483c0 87.29637925 74.45617778 161.75255703 164.32545185 161.75255703h25.68040296c0 30.8140563 25.68040297 53.92156445 53.92156444 53.92156444s53.92156445-25.68040297 53.92156445-53.92156444H704.23893333c2.57289482 30.8140563 28.24116148 53.92156445 59.05521778 51.34866964 28.24116148-2.57289482 48.78791111-23.10750815 51.34866964-51.34866964h20.53461333c89.86927408 0 164.32545185-74.45617778 164.32545184-164.32545186V343.09916445c-2.56075852-89.86927408-77.02907259-161.76469333-166.88621037-161.76469334z m-5.13365333 634.19429926H200.99527111c-33.37481482 0-59.05521778-28.24116148-61.61597629-61.61597629l-2.57289482-415.94728297c0-33.37481482 28.24116148-61.6159763 61.6159763-61.61597629h626.48775111c33.37481482 0 59.05521778 28.24116148 61.61597629 61.61597629l2.57289482 415.94728297c-2.57289482 35.93557333-28.24116148 61.6159763-61.6159763 61.61597629z" fill="#ff7299" ></path><path d="M403.82919111 417.55534222l15.40096 77.0290726-205.40681481 38.50846815-15.40096-77.0290726 205.40681481-38.50846815z m197.70026667 77.0290726l15.40096-77.0290726 205.40681481 38.50846815-15.40096 77.0290726-205.40681481-38.50846815z m41.08136297 161.75255703c0 2.57289482 0 7.70654815-2.57289483 10.26730667-12.84020148 28.24116148-41.08136297 46.2150163-74.45617777 48.78791111-20.53461333 0-41.08136297-10.26730667-53.92156445-25.68040296-15.40096 15.40096-33.37481482 25.68040297-53.92156445 25.68040296-30.8140563-2.57289482-59.05521778-20.53461333-74.45617777-48.78791111 0-2.57289482-2.57289482-5.13365333-2.57289481-10.26730667 0-10.26730667 7.70654815-17.97385482 17.97385481-20.53461333h2.57289482c7.70654815 0 12.84020148 2.57289482 15.40096 10.26730666 0 0 20.53461333 28.24116148 38.50846815 28.24116149 35.94770963 0 35.94770963-30.8140563 56.48232296-53.92156445 23.10750815 25.68040297 23.10750815 53.92156445 56.48232296 53.92156445 23.10750815 0 38.50846815-28.24116148 38.50846815-28.24116149 2.57289482-5.13365333 10.26730667-10.26730667 15.40096-10.26730666 10.26730667-2.57289482 17.97385482 5.13365333 20.53461333 15.40096v5.13365333h0.0364089z" fill="#ff7299" ></path></symbol></svg>',      o = (o = document.getElementsByTagName("script"))[o.length - 1].getAttribute("data-injectcss"),      p = function (c, l) {        l.parentNode.insertBefore(c, l);      };    if (o && !c.__iconfont__svg__cssinject__) {      c.__iconfont__svg__cssinject__ = !0;      try {        document.write(          "<style>.svgfont {display: inline-block;width: 1em;height: 1em;fill: currentColor;vertical-align: -0.1em;font-size:16px;}</style>"        );      } catch (c) {        console && console.log(c);      }    }    function d() {      i || ((i = !0), a());    }    function m() {      try {        t.documentElement.doScroll("left");      } catch (c) {        return void setTimeout(m, 50);      }      d();    }    (l = function () {      var c,        l = document.createElement("div");      (l.innerHTML = v),        (v = null),        (l = l.getElementsByTagName("svg")[0]) &&          (l.setAttribute("aria-hidden", "true"),          (l.style.position = "absolute"),          (l.style.width = 0),          (l.style.height = 0),          (l.style.overflow = "hidden"),          (l = l),          (c = document.body).firstChild ? p(l, c.firstChild) : c.appendChild(l));    }),      document.addEventListener        ? ~["complete", "loaded", "interactive"].indexOf(document.readyState)          ? setTimeout(l, 0)          : ((h = function () {              document.removeEventListener("DOMContentLoaded", h, !1), l();            }),            document.addEventListener("DOMContentLoaded", h, !1))        : document.attachEvent &&          ((a = l),          (t = c.document),          (i = !1),          m(),          (t.onreadystatechange = function () {            "complete" == t.readyState && ((t.onreadystatechange = null), d());          }));  })(window);]]></content>
      
    </entry>
    
    
  
</search>
